{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Distributed Training with Reduction Server\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to optimize large distributed training jobs with the Vertex Training Reduction Server. The [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) from the [TensorFlow Model Garden](https://github.com/tensorflow/models) is used to preprocess the data and create the model and training loop. `MultiWorkerMirroredStrategy` from the `tf.distribute` module is used to distribute model training across multiple machines. \n",
    "\n",
    "For more information about using Reduction Server to optimize distributed training refer to the [Optimizing Distributed Training with Vertex AI Reduction Server](tbd) article.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Multi-Genre Natural Language Inference Corpus (MNLI)](https://www.tensorflow.org/datasets/catalog/glue#gluemnli) from the GLUE benchmark. This dataset is loaded from [TensorFlow Datasets](https://www.tensorflow.org/datasets), and used to fine tune a BERT model for sentence prediction.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, you create a custom-trained model from a Python script in a Docker container. You learn how to configure, submit, and monitor a Vertex Training job that uses Reduction Server to optimize network bandwith and latency of the gradient reduction operation in distributed training.  \n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Convert the MNLI dataset to the format required by the TensorFlow NLP Modelling Toolkit.\n",
    "- Create a Vertex AI custom job that uses Reduction Server.\n",
    "- Submit and monitor the job.\n",
    "- Cleanup resources.\n",
    "\n",
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the TensorFlow Official Models and TensorFlow Text libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-models-official==2.5.0 in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: tensorflow-text==2.5.0 in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.0.0)\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.13.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.3.0)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.1.96)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (3.4.2)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.5)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.5.2.54)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.1.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.29.23)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.6.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.5.1)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.5.12)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.20.0)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: gin-config in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.2.0)\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.4.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.19.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.0.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (20.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.18.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.34.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (1.26.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (4.61.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (5.0.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.10)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.36.2)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (4.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.5.0) (0.1.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0) (1.3)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /opt/conda/lib/python3.7/site-packages (from sacrebleu->tf-models-official==2.5.0) (2.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (2.1.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons->tf-models-official==2.5.0) (2.12.1)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (21.2.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.18.2)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade tf-models-official==2.5.0 tensorflow-text==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of the Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.40.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.30.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.18.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.30.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.10)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of the Google Cloud Storage library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.40.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.30.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (49.6.0.post20210108)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (20.9)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2021.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  curious-entropy-222019\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"curious-entropy-222019\"  # @param {type:\"string\"}\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "In this example, your training application uses Cloud Storage for accessing training and validation datasets and for storing checkpoints. \n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://jk-rs-testing\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jk-rs-testing/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jk-rs-testing' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://jk-rs-testing/MNLI_20210710_210507/\n",
      "                                 gs://jk-rs-testing/datasets/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp.data import classifier_data_lib\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "\n",
    "- API_ENDPOINT: The Vertex API service endpoint for job services.\n",
    "- PARENT: The Vertex location root path for job resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = '{}-aiplatform.googleapis.com'.format(REGION)\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vertex client library works as a client/server model. In your Python script, you create a client that sends requests and receives responses from the Vertex server.\n",
    "\n",
    "In this example, you use the Job Service client for submitting and monitoring custom training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "job_client = JobServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets\n",
    "\n",
    "The TensorFlow NLP Modelling Toolkit provides reusable and modularized modeling building blocks. The following function uses utility functions from the toolkit to transform the MNLI dataset into the format expected by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnli_tfrecords(\n",
    "    train_data_output_path,\n",
    "    eval_data_output_path,\n",
    "    metadata_file_path,\n",
    "    vocab_file='gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/vocab.txt',\n",
    "    mnli_type='matched',\n",
    "    max_seq_length=128,\n",
    "    do_lower_case=True):\n",
    "  \"\"\"Generates MNLI training/validation splits in the TFRecord format compatible with the Modelling Toolkit.\"\"\"\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "  processor_text_fn = tokenization.convert_to_unicode\n",
    "\n",
    "  if mnli_type == 'matched':\n",
    "    tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched'\n",
    "  else:\n",
    "    tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_mismatched'\n",
    "\n",
    "  processor = classifier_data_lib.TfdsProcessor(\n",
    "      tfds_params=tfds_params, process_text_fn=processor_text_fn)\n",
    "\n",
    "  metadata = classifier_data_lib.generate_tf_record_from_data_file(\n",
    "      processor,\n",
    "      None,\n",
    "      tokenizer,\n",
    "      train_data_output_path=train_data_output_path,\n",
    "      eval_data_output_path=eval_data_output_path,\n",
    "      max_seq_length=max_seq_length)\n",
    "\n",
    "  with tf.io.gfile.GFile(metadata_file_path, 'w') as writer:\n",
    "    writer.write(json.dumps(metadata, indent=4) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data locations\n",
    "\n",
    "OUTPUT_LOCATION = f'{BUCKET_NAME}/datasets/MNLI'\n",
    "TRAIN_FILE = f'{OUTPUT_LOCATION}/mnli_train.tf_record'\n",
    "EVAL_FILE = f'{OUTPUT_LOCATION}/mnli_valid.tf_record'\n",
    "METADATA_FILE = f'{OUTPUT_LOCATION}/metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/home/jupyter/tensorflow_datasets/glue/mnli/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Writing example 0 of 392702\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-0\n",
      "INFO:absl:tokens: [CLS] meaningful partnerships with stakeholders is crucial . [SEP] in recognition of these tensions , l ##sc has worked dil ##igen ##tly since 1995 to convey the expectations of the state planning initiative and to establish meaningful partnerships with stakeholders aimed at foster ##ing a new sy ##mb ##ios ##is between the federal provider and recipients of legal services funding . [SEP]\n",
      "INFO:absl:input_ids: 101 15902 13797 2007 22859 2003 10232 1012 102 1999 5038 1997 2122 13136 1010 1048 11020 2038 2499 29454 29206 14626 2144 2786 2000 16636 1996 10908 1997 1996 2110 4041 6349 1998 2000 5323 15902 13797 2007 22859 6461 2012 6469 2075 1037 2047 25353 14905 10735 2483 2090 1996 2976 10802 1998 15991 1997 3423 2578 4804 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-1\n",
      "INFO:absl:tokens: [CLS] the clinton followers kept to the higher ground in the discussion . [SEP] the clinton sur ##rogate ##s also held the high ground in the context war . [SEP]\n",
      "INFO:absl:input_ids: 101 1996 7207 8771 2921 2000 1996 3020 2598 1999 1996 6594 1012 102 1996 7207 7505 21799 2015 2036 2218 1996 2152 2598 1999 1996 6123 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-2\n",
      "INFO:absl:tokens: [CLS] women have jobs in all areas of the workforce , they are almost getting the same wages as most men , [SEP] um - hum because women are in every field now i mean i can ' t think of a field that they ' re not involved in [SEP]\n",
      "INFO:absl:input_ids: 101 2308 2031 5841 1999 2035 2752 1997 1996 14877 1010 2027 2024 2471 2893 1996 2168 12678 2004 2087 2273 1010 102 8529 1011 14910 2138 2308 2024 1999 2296 2492 2085 1045 2812 1045 2064 1005 1056 2228 1997 1037 2492 2008 2027 1005 2128 2025 2920 1999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-3\n",
      "INFO:absl:tokens: [CLS] houston is freezing and dry right now . [SEP] houston is really humid now [SEP]\n",
      "INFO:absl:input_ids: 101 5395 2003 12809 1998 4318 2157 2085 1012 102 5395 2003 2428 14178 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-4\n",
      "INFO:absl:tokens: [CLS] but they wouldn ' t be leaving right now . [SEP] but not now . [SEP]\n",
      "INFO:absl:input_ids: 101 2021 2027 2876 1005 1056 2022 2975 2157 2085 1012 102 2021 2025 2085 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:Writing example 10000 of 392702\n",
      "INFO:absl:Writing example 20000 of 392702\n",
      "INFO:absl:Writing example 30000 of 392702\n",
      "INFO:absl:Writing example 40000 of 392702\n",
      "INFO:absl:Writing example 50000 of 392702\n",
      "INFO:absl:Writing example 60000 of 392702\n",
      "INFO:absl:Writing example 70000 of 392702\n",
      "INFO:absl:Writing example 80000 of 392702\n",
      "INFO:absl:Writing example 90000 of 392702\n",
      "INFO:absl:Writing example 100000 of 392702\n",
      "INFO:absl:Writing example 110000 of 392702\n",
      "INFO:absl:Writing example 120000 of 392702\n",
      "INFO:absl:Writing example 130000 of 392702\n",
      "INFO:absl:Writing example 140000 of 392702\n",
      "INFO:absl:Writing example 150000 of 392702\n",
      "INFO:absl:Writing example 160000 of 392702\n",
      "INFO:absl:Writing example 170000 of 392702\n",
      "INFO:absl:Writing example 180000 of 392702\n",
      "INFO:absl:Writing example 190000 of 392702\n",
      "INFO:absl:Writing example 200000 of 392702\n",
      "INFO:absl:Writing example 210000 of 392702\n",
      "INFO:absl:Writing example 220000 of 392702\n",
      "INFO:absl:Writing example 230000 of 392702\n",
      "INFO:absl:Writing example 240000 of 392702\n",
      "INFO:absl:Writing example 250000 of 392702\n",
      "INFO:absl:Writing example 260000 of 392702\n",
      "INFO:absl:Writing example 270000 of 392702\n",
      "INFO:absl:Writing example 280000 of 392702\n",
      "INFO:absl:Writing example 290000 of 392702\n",
      "INFO:absl:Writing example 300000 of 392702\n",
      "INFO:absl:Writing example 310000 of 392702\n",
      "INFO:absl:Writing example 320000 of 392702\n",
      "INFO:absl:Writing example 330000 of 392702\n",
      "INFO:absl:Writing example 340000 of 392702\n",
      "INFO:absl:Writing example 350000 of 392702\n",
      "INFO:absl:Writing example 360000 of 392702\n",
      "INFO:absl:Writing example 370000 of 392702\n",
      "INFO:absl:Writing example 380000 of 392702\n",
      "INFO:absl:Writing example 390000 of 392702\n",
      "INFO:absl:Writing example 0 of 9815\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-0\n",
      "INFO:absl:tokens: [CLS] yeah lots of people for the right life [SEP] uh - huh oh yeah all the people for right uh life or something [SEP]\n",
      "INFO:absl:input_ids: 101 3398 7167 1997 2111 2005 1996 2157 2166 102 7910 1011 9616 2821 3398 2035 1996 2111 2005 2157 7910 2166 2030 2242 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-1\n",
      "INFO:absl:tokens: [CLS] i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant . [SEP] also , i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant as limited quantities of mail move back and forth between basic and works ##har ##ed . [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 1012 102 2036 1010 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 2004 3132 12450 1997 5653 2693 2067 1998 5743 2090 3937 1998 2573 8167 2098 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-2\n",
      "INFO:absl:tokens: [CLS] it cost a lot of money to put forth all that effort . [SEP] taken up by the oh okay oh so you know well that ' s i had wondered sometimes i knew that there was a lot of a lot of effort and a lot of work went into a lot of that and i just wondered if if it lasted and if it took you know like yeah [SEP]\n",
      "INFO:absl:input_ids: 101 2009 3465 1037 2843 1997 2769 2000 2404 5743 2035 2008 3947 1012 102 2579 2039 2011 1996 2821 3100 2821 2061 2017 2113 2092 2008 1005 1055 1045 2018 4999 2823 1045 2354 2008 2045 2001 1037 2843 1997 1037 2843 1997 3947 1998 1037 2843 1997 2147 2253 2046 1037 2843 1997 2008 1998 1045 2074 4999 2065 2065 2009 6354 1998 2065 2009 2165 2017 2113 2066 3398 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-3\n",
      "INFO:absl:tokens: [CLS] i know nothing about the motor oil . [SEP] yeah i know the motor oil [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2113 2498 2055 1996 5013 3514 1012 102 3398 1045 2113 1996 5013 3514 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-4\n",
      "INFO:absl:tokens: [CLS] many hotels in lake ##land have a cost - effective option of dinner , bed , and breakfast with an evening meal in . [SEP] many lake ##land hotels also quote a d , b and b ( dinner , bed , and breakfast ) rate , which includes the evening meal and is often quite cost - effective . [SEP]\n",
      "INFO:absl:input_ids: 101 2116 9275 1999 2697 3122 2031 1037 3465 1011 4621 5724 1997 4596 1010 2793 1010 1998 6350 2007 2019 3944 7954 1999 1012 102 2116 2697 3122 9275 2036 14686 1037 1040 1010 1038 1998 1038 1006 4596 1010 2793 1010 1998 6350 1007 3446 1010 2029 2950 1996 3944 7954 1998 2003 2411 3243 3465 1011 4621 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n"
     ]
    }
   ],
   "source": [
    "generate_mnli_tfrecords(TRAIN_FILE, EVAL_FILE, METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk-rs-testing/datasets/MNLI/\n",
      "gs://jk-rs-testing/datasets/MNLI/metadata.json\n",
      "gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record\n",
      "gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record\n"
     ]
    }
   ],
   "source": [
    "# Verify that the files were successfully created.\n",
    "\n",
    "! gsutil ls {OUTPUT_LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"processor_type\": \"TFDS_glue/mnli\",\n",
      "    \"train_data_size\": 392702,\n",
      "    \"max_seq_length\": 128,\n",
      "    \"task_type\": \"bert_classification\",\n",
      "    \"num_labels\": 3,\n",
      "    \"eval_data_size\": 9815\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Examine the metadata\n",
    "\n",
    "! gsutil cat {METADATA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training container\n",
    "#### Write Dockerfile\n",
    "\n",
    "The first step in containerizing your code is to create a Dockerfile. In the Dockerfile, you'll include all the commands needed to run the image such as installing the necessary libraries and setting up the entry point for the training code.\n",
    "\n",
    "This Dockerfile uses the Deep Learning Container TensorFlow Enterprise 2.5 GPU Docker image. The Deep Learning Containers on Google Cloud come with many common ML and data science frameworks pre-installed. After downloading that image, this Dockerfile installs the TensorFlow Official Models and TensorFlow Text libraries, and the Reduction Server NCCL plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf training_image\n",
    "! mkdir training_image\n",
    "! mkdir training_image/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "WORKDIR /\n",
    "\n",
    "# Install Reduction Server NCCL plugin\n",
    "RUN apt remove -y google-fast-socket \\\n",
    "&&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list \\\n",
    "&&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n",
    "&&  apt update && apt install -y google-reduction-server\n",
    "\n",
    "# Install Official Models and Text libraries\n",
    "RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
    "\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\"]\n",
    "CMD [\"-c\", \"print('TF Model Garden')\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training application code\n",
    "\n",
    "Next, you create a `training_image/trainer` directory with a `train.py` script that contains the code for your training application.\n",
    "\n",
    "The `train.py` training script is based on [the common training driver](https://github.com/tensorflow/models/blob/master/official/nlp/docs/train.md) from the TensorFlow NLP Modelling Toolkit. The common training driver script supports multiple NLP tasks (e.g., pre-training, GLUE and SQuAD fine-tuning) and multiple models (e.g., BERT, ALBERT). \n",
    "\n",
    "A set of configurations to use for a specific NLP task is called an experiment.  The [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) includes a set of pre-defined experiments. When you invoke the script you specificy an experiment type using the `--experiment` command line parameter. The default experiment configuration can be overridden using two methods:\n",
    "- You can specify one or multiple YAML configuration with updated settings using the `--config_file` command line parameter\n",
    "- You can provide updated settings as a list of key/value pairs through the `--params_override` command line parameter\n",
    "\n",
    "When you specify both the `--config_file` and the `--params_override`, the settings in the `--params_override` take precedence.\n",
    "\n",
    "Retrieving the default experiment configuration and merging user provided settings is encapsulated in the `official.core.train_utils.parse_configuration` utility function from the [TensorFlow Model Garden](https://github.com/tensorflow/models).\n",
    "\n",
    "The [common training driver](https://github.com/tensorflow/models/blob/master/official/nlp/train.py)  has been adapted to work seamlessly on a distributed compute environment provisioned when running a Vertex Training job. The [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) uses [Orbit](https://github.com/tensorflow/models/tree/master/orbit) to implement a custom training loop. The custom training loop saves checkpoints, write Tensorboard summaries, and saves a trained model to a storage location specified through the `--model-dir` command line parameter. When using the base common training driver in a distributed setting each worker uses the same base storage location. To avoid conflicts when multiple workers write to the same storage location, we modified the code so that each worker uses a different storage location based on its role in a Vertex compute cluster. This logic is implemented by the `_get_model_dir` utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/train.py\n",
    "\n",
    "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"TFM common training driver.\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import gin\n",
    "from official.common import distribute_utils\n",
    "from official.common import flags as tfm_flags\n",
    "from official.common import registry_imports\n",
    "from official.core import task_factory\n",
    "from official.core import train_lib\n",
    "from official.core import train_utils\n",
    "from official.modeling import performance\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "\n",
    "def _get_model_dir(model_dir):\n",
    "  \"\"\"Defines utility functions for model saving.\n",
    "\n",
    "  In a multi-worker scenario, the chief worker will save to the\n",
    "  desired model directory, while the other workers will save the model to\n",
    "  temporary directories. It’s important that these temporary directories\n",
    "  are unique in order to prevent multiple workers from writing to the same\n",
    "  location. Saving can contain collective ops, so all workers must save and\n",
    "  not just the chief.\n",
    "  \"\"\"\n",
    "\n",
    "  def _is_chief(task_type, task_id):\n",
    "    return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "\n",
    "  tf_config = os.getenv('TF_CONFIG')\n",
    "  if tf_config:\n",
    "    tf_config = json.loads(tf_config)\n",
    "\n",
    "    if not _is_chief(tf_config['task']['type'], tf_config['task']['index']):\n",
    "      model_dir = os.path.join(model_dir,\n",
    "                               'worker-{}').format(tf_config['task']['index'])\n",
    "\n",
    "  logging.info('Setting model_dir to: %s', model_dir)\n",
    "\n",
    "  return model_dir\n",
    "\n",
    "\n",
    "def main(_):\n",
    "\n",
    "  model_dir = _get_model_dir(FLAGS.model_dir)\n",
    "\n",
    "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_params)\n",
    "  params = train_utils.parse_configuration(FLAGS)\n",
    "\n",
    "  if 'train' in FLAGS.mode:\n",
    "    # Pure eval modes do not output yaml files. Otherwise continuous eval job\n",
    "    # may race against the train job for writing the same file.\n",
    "    train_utils.serialize_config(params, model_dir)\n",
    "\n",
    "  # Sets mixed_precision policy. Using 'mixed_float16' or 'mixed_bfloat16'\n",
    "  # can have significant impact on model speeds by utilizing float16 in case of\n",
    "  # GPUs, and bfloat16 in the case of TPUs. loss_scale takes effect only when\n",
    "  # dtype is float16\n",
    "  if params.runtime.mixed_precision_dtype:\n",
    "    performance.set_mixed_precision_policy(params.runtime.mixed_precision_dtype)\n",
    "  distribution_strategy = distribute_utils.get_distribution_strategy(\n",
    "      distribution_strategy=params.runtime.distribution_strategy,\n",
    "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
    "      num_gpus=params.runtime.num_gpus,\n",
    "      tpu_address=params.runtime.tpu,\n",
    "      **params.runtime.model_parallelism())\n",
    "  with distribution_strategy.scope():\n",
    "    task = task_factory.get_task(params.task, logging_dir=model_dir)\n",
    "\n",
    "  train_lib.run_experiment(\n",
    "      distribution_strategy=distribution_strategy,\n",
    "      task=task,\n",
    "      mode=FLAGS.mode,\n",
    "      params=params,\n",
    "      model_dir=model_dir)\n",
    "\n",
    "  train_utils.save_gin_config(FLAGS.mode, model_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tfm_flags.define_flags()\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create base settings for the GLUE/MNLI fine tuning experiment\n",
    "\n",
    "The [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) includes a predefined experiment for a set of text classification tasks - the `bert/sentence_prediction` experiment. The base settings in the `bert/sentence_prediction` experiment need to be updated for the GLUE/MNLI fine tuning task you perform in this example.\n",
    "\n",
    "You do this by creating a YAML configuration file that will be referenced when invoking a training script. As noted, when describing [the common training driver](https://github.com/tensorflow/models/blob/master/official/nlp/docs/train.md) you will be able to fine tune these settings even further for each training run by using the `--params_override` flag.\n",
    "\n",
    "The configuration file has three sections: `task`, `trainer`, `runtime`.\n",
    "\n",
    "The `task` section contains settings specific to your machine learning task, including a URI to a pre-trained BERT model,  training and validation datasets settings, and evaluation metrics. The `trainer` section configures the settings that control the custom training loop, like checkpoint interval or a number of training steps. The `runtime` section includes the settings for a training runtime: a distributed training strategy, GPU configurations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/trainer/glue_mnli_matched.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/glue_mnli_matched.yaml\n",
    "\n",
    "task:\n",
    "  hub_module_url: 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "  model:\n",
    "    num_classes: 3\n",
    "  init_checkpoint: ''\n",
    "  metric_type: 'accuracy'\n",
    "  train_data:\n",
    "    drop_remainder: true\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: true\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "  validation_data:\n",
    "    drop_remainder: false\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: false\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "trainer:\n",
    "  checkpoint_interval: 3000\n",
    "  optimizer_config:\n",
    "    learning_rate:\n",
    "      polynomial:\n",
    "        # 100% of train_steps.\n",
    "        decay_steps: 36813\n",
    "        end_learning_rate: 0.0\n",
    "        initial_learning_rate: 3.0e-05\n",
    "        power: 1.0\n",
    "      type: polynomial\n",
    "    optimizer:\n",
    "      type: adamw\n",
    "    warmup:\n",
    "      polynomial:\n",
    "        power: 1\n",
    "        # ~10% of train_steps.\n",
    "        warmup_steps: 3681\n",
    "      type: polynomial\n",
    "  steps_per_loop: 1000\n",
    "  summary_interval: 1000\n",
    "  # Training data size 392,702 examples, 3 epochs.\n",
    "  train_steps: 36813\n",
    "  validation_interval: 6135\n",
    "  # Eval data size = 9815 examples.\n",
    "  validation_steps: 307\n",
    "  best_checkpoint_export_subdir: 'best_ckpt'\n",
    "  best_checkpoint_eval_metric: 'cls_accuracy'\n",
    "  best_checkpoint_metric_comp: 'higher'\n",
    "runtime:\n",
    "  distribution_strategy: 'multi_worker_mirrored'\n",
    "  all_reduce_alg: 'nccl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the container\n",
    "\n",
    "In the next cells, you build the container and push it to Google Container Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/mnli_finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  9.216kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 0f998c784cd6\n",
      "Step 2/7 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> f5d7eda55d06\n",
      "Step 3/7 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Using cache\n",
      " ---> 9597ef44789a\n",
      "Step 4/7 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Using cache\n",
      " ---> c59ec29f72cf\n",
      "Step 5/7 : COPY trainer /trainer\n",
      " ---> Using cache\n",
      " ---> 2049c3adbf16\n",
      "Step 6/7 : ENTRYPOINT [\"python\"]\n",
      " ---> Using cache\n",
      " ---> 9f9222a65fb4\n",
      "Step 7/7 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Using cache\n",
      " ---> 9bdcb22df83d\n",
      "Successfully built 9bdcb22df83d\n",
      "Successfully tagged gcr.io/curious-entropy-222019/mnli_finetuning:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/curious-entropy-222019/mnli_finetuning]\n",
      "\n",
      "\u001b[1Bfcd5a0a1: Preparing \n",
      "\u001b[1B11bd61a3: Preparing \n",
      "\u001b[1Baf86f7b4: Preparing \n",
      "\u001b[1B40ebdbd6: Preparing \n",
      "\u001b[1B54d2bd94: Preparing \n",
      "\u001b[1Ba056d495: Preparing \n",
      "\u001b[1Bcb8c2687: Preparing \n",
      "\u001b[1B43de6bca: Preparing \n",
      "\u001b[1B87a4088d: Preparing \n",
      "\u001b[1B522d97b4: Preparing \n",
      "\u001b[1B519f0898: Preparing \n",
      "\u001b[1B6aeeabc0: Preparing \n",
      "\u001b[1Bebfdebb3: Preparing \n",
      "\u001b[1B6b863e43: Preparing \n",
      "\u001b[1B43fb2f7a: Preparing \n",
      "\u001b[1Ba9d68143: Preparing \n",
      "\u001b[1B84ac5c5d: Preparing \n",
      "\u001b[1Be1798c0c: Preparing \n",
      "\u001b[1B9de06c8b: Preparing \n",
      "\u001b[1B59f353b4: Preparing \n",
      "\u001b[1B3c11e857: Preparing \n",
      "\u001b[1B5264beff: Preparing \n",
      "\u001b[1B14beba01: Preparing \n",
      "\u001b[1Bf85bc8aa: Preparing \n",
      "\u001b[1Bdd6c9734: Preparing \n",
      "\u001b[1Ba732d388: Preparing \n",
      "\u001b[1B78e3bf48: Preparing \n",
      "\u001b[20B7a4088d: Waiting g \n",
      "\u001b[1B87e0621d: Preparing \n",
      "\u001b[1B7ad6008c: Preparing \n",
      "\u001b[22B22d97b4: Waiting g \n",
      "\u001b[1B872b888e: Preparing \n",
      "\u001b[1B512fd434: Preparing \n",
      "\u001b[19B9d68143: Waiting g \n",
      "\u001b[1B8308da3d: Layer already exists \u001b[34A\u001b[2K\u001b[30A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:9053ebc7539074a50eddc6151609874c8ef7b0782cb05276399535d58f382ac7 size: 7673\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom training job\n",
    "\n",
    "When you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica. A group of replicas with the same configuration is called a worker pool. Vertex Training provides 4 worker pools to cover the different types of machine tasks. To use the Reduction Server, you'll need to use 3 of the 4 available worker pools.\n",
    "\n",
    "* **Worker pool 0** configures the Primary, chief, scheduler, or \"master\".  This worker generally takes on some extra work such as saving checkpoints and writing summary files. There is only ever one chief worker in a cluster, so your worker count for worker pool 0 will always be 1.\n",
    "\n",
    "* **Worker pool 1** is where you configure the rest of the workers for your cluster. \n",
    " \n",
    "* **Worker pool 2** manages Reduction Server reducers. \n",
    "\n",
    "Worker pools 0 and 1 run the custom training container you created in the previous step. Worker pool 2 uses the Reduction Server image provided by Vertex AI.\n",
    "\n",
    "The helper function below creates a custom job specification using the described worker pool topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure worker pools\n",
    "\n",
    "When choosing the number and type of reducers, you should consider the network bandwidth supported by a reducer replica’s machine type. In GCP, a VM’s machine type defines its maximum possible egress bandwidth. For example, the egress bandwidth of the `n1-highcpu-16` machine type is limited at 32 Gbps.\n",
    "\n",
    "Because reducers perform a very limited function, aggregating blocks of gradients, they can run on relatively low-powered and cost effective machines. Even with a large number of gradients this computation does not require accelerated hardware or high CPU or memory resources. However, to avoid network bottlenecks, the total aggregate bandwidth of all replicas in the reducer worker pool must be greater or equal to the total aggregate bandwidth of all replicas in worker pools 0 and 1, which host the GPU workers.\n",
    "\n",
    "Refer to [the article](TBD) for more information about configuring worker pools when using Reduction Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICA_COUNT = 8\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-8g'\n",
    "#WORKER_MACHINE_TYPE = 'n1-standard-8'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "#ACCELERATOR_TYPE = 'NVIDIA_TESLA_V100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 8\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "#PER_REPLICA_BATCH_SIZE = 16\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine tune the  GLUE/MNLI experiment settings\n",
    "\n",
    "The default settings for the GLUE/MNLI fine tuning experiment have been configured in the YAML configuration file created in the previous steps. To override the defaults for a specific training run you can use the `--params_override` flag.\n",
    "\n",
    "The `params_override`  accepts a string with comma separated key/value pairs for each parameter to be overridden.\n",
    "\n",
    "In the following cell you update the following settings:\n",
    "\n",
    "- `trainer.train_step` - The number of training steps. \n",
    "- `trainer.steps_per_loop` - The training script prints out updates about training progress every `steps_per_loop`\n",
    "- `trainer.summary_interval` - The training script logs Tensorboard summaries every `summary_interval`\n",
    "- `trainer.validation_interval` - The training script runs validation every `validation_interval`\n",
    "- `trainer.checkpoint_interval` - The training script creates a checkpoint every `checkpoint_interval`\n",
    "- `task.train_data.global_batch_size` - Global batch size for training data.\n",
    "- `task.validation_data.global_batch_size` - Global batch size for validation data\n",
    "- `task.train_data.input_path` - Location of the training dataset\n",
    "- `task.validation_data.input_path` - Location of the validation dataset\n",
    "- `runtime.num_gpus` -Number of GPUs to use on each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'trainer.train_steps=2000',\n",
    "    'trainer.steps_per_loop=100',\n",
    "    'trainer.summary_interval=100',\n",
    "    'trainer.validation_interval=2000',\n",
    "    'trainer.checkpoint_interval=2000',\n",
    "    'task.train_data.global_batch_size=' + str(REPLICA_COUNT*PER_REPLICA_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(REPLICA_COUNT*PER_REPLICA_BATCH_SIZE*PER_MACHINE_ACCELERATOR_COUNT), \n",
    "    'task.train_data.input_path=' + TRAIN_FILE,\n",
    "    'task.validation_data.input_path=' + EVAL_FILE,\n",
    "    'runtime.num_gpus=' + str(PER_MACHINE_ACCELERATOR_COUNT),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create custom job spec\n",
    "\n",
    "After the experimentation and configuration parameters have been defined, you create the custom job spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{BUCKET_NAME}/{JOB_NAME}/model'\n",
    "\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=bert/sentence_prediction',\n",
    "    '--mode=train',\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=trainer/glue_mnli_matched.yaml',\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210711_172343',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-testing/MNLI_20210711_172343/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.train_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=8'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/curious-entropy-222019/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 8,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-8g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-testing/MNLI_20210711_172343/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.train_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=8'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/curious-entropy-222019/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 8,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-8g'},\n",
      "                                     'replica_count': 7}]}}\n"
     ]
    }
   ],
   "source": [
    "# Examine the spec\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and monitor the job\n",
    "\n",
    "Use the Vertex AI job client to submit and monitor a training job. \n",
    "\n",
    "To submit the job, use the job client service's `create_custom_job` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "name: \"projects/697926852371/locations/us-central1/customJobs/464068673712160768\"\n",
       "display_name: \"MNLI_20210711_172343\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-8g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 8\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/curious-entropy-222019/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-testing/MNLI_20210711_172343/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.train_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=8\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-8g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 8\n",
       "    }\n",
       "    replica_count: 7\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/curious-entropy-222019/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-testing/MNLI_20210711_172343/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.train_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=8\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1626024228\n",
       "  nanos: 392900000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1626024228\n",
       "  nanos: 392900000\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = JobServiceClient(client_options=options)\n",
    "\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "response = client.create_custom_job(\n",
    "    parent=parent, custom_job=custom_job_spec\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the job client service's `get_custom_job` method to retrieve information about a running job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/697926852371/locations/us-central1/customJobs/4892233007324200960\"\n",
       "display_name: \"MNLI_20210711_164022\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-8g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 8\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/curious-entropy-222019/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-testing/MNLI_20210711_164022/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=32,task.validation_data.global_batch_size=32,task.train_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-testing/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=8\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_QUEUED\n",
       "create_time {\n",
       "  seconds: 1626021709\n",
       "  nanos: 863185000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1626021710\n",
       "  nanos: 5218000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1626021710\n",
       "  nanos: 319916000\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_custom_job(name=response.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r {BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary - Upload logs to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORBOARD=projects/895222332033/locations/us-central1/tensorboards/5983067289333792768\n",
      "LOGDIR=gs://jk-rs-testing/MNLI_20210711_164022/model\n",
      "EXPERIMENT=MNLI_20210711_164022\n",
      "./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True\n"
     ]
    }
   ],
   "source": [
    "print('TENSORBOARD={}'.format('projects/895222332033/locations/us-central1/tensorboards/5983067289333792768'))\n",
    "print('LOGDIR={}'.format(MODEL_DIR))\n",
    "print('EXPERIMENT={}'.format(JOB_NAME))\n",
    "print('./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
