{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Distributed training with Reduction Server\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to optimize large distributed training Vertex AI jobs using Reduction Server. \n",
    "\n",
    "The machine learning task in this example is fine tuning a BERT model for sentence prediction using  the Multi-Genre Natural Language Inference Corpus (MNLI) from the GLUE benchmark. \n",
    "\n",
    "The example uses components from [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) and distributed training is implemented using [tf.distribute.MultiWorkerMirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy). \n",
    "\n",
    "For more information about using Reduction Server to optimize distributed training refer to the [Optimizing distributed training with Vertex AI Reduction Server](tbd) article.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "\n",
    "The example uses the *glue/mnli* dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/glue).\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to configure, submit and monitor a Vertex AI custom training job that uses Reduction Server to optimize network bandwith and latency of the gradient reduction operation in the distributed training setting.  \n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Building a custom training container image based on TensorFlow NLP Modelling Toolkit\n",
    "- Converting the glue/mnli dataset to the format required by TensorFlow NLP Modelling Toolkit\n",
    "- Preparing a Vertex AI custom container training job that uses Reduction Server\n",
    "- Submitting and monitoring the job\n",
    "\n",
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install TensorFlow NLP Modelling Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official==2.5.0\n",
      "  Downloading tf_models_official-2.5.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 7.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-text==2.5.0\n",
      "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 32.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (3.4.2)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 7.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 6.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 8.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.2.0)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "\u001b[K     |████████████████████████████████| 352 kB 50.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.20.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.9.0)\n",
      "Collecting tensorflow>=2.5.0\n",
      "  Downloading tensorflow-2.5.0-cp37-cp37m-manylinux2010_x86_64.whl (454.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 454.3 MB 7.6 kB/s  eta 0:00:013.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.4)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 38.2 MB 33.2 MB/s eta 0:00:01�██████████████████████▏      | 30.1 MB 33.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.6.0\n",
      "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "\u001b[K     |████████████████████████████████| 108 kB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.16.0)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
      "\u001b[K     |████████████████████████████████| 211 kB 39.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.8.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.3.0-py3-none-any.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 32.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "\u001b[K     |████████████████████████████████| 679 kB 53.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.4.1)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.1.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.2)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (20.9)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.53.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.18.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.38.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (4.61.1)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (1.26.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.10)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting h5py~=3.1.0\n",
      "  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 39.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard~=2.5\n",
      "  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0 MB 35.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 3.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py~=0.10\n",
      "  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 51.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.36.2)\n",
      "Collecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0dev,>=1.29.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 49.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta~=0.2\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting cached-property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 41.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 43.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.4)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 41.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (4.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.1)\n",
      "Collecting dm-tree~=0.1.1\n",
      "  Downloading dm_tree-0.1.6-cp37-cp37m-manylinux_2_24_x86_64.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 2.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0) (1.3)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (2.1.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.1.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 6.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.18.2)\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.2.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (21.2.0)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 43.6 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: kaggle, py-cpuinfo, termcolor, pycocotools, seqeval, promise\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=22cdbadd625bfbc807b7a79437bd7cfab84876dfd245c287bed40cc8b250f1de\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22245 sha256=bf39e1ed0134f3d83b11f96859f93c24a17f199faa022bbc6595d9c61078d630\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=f6eecf33db99d68b05981dae377523e05f3645146e5504f5b977751034af2680\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=325549 sha256=332ab64ea74ec4836f4434e4dfd533086a968906e29b51fad320dbe7b86338df\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=b8dcef1ac8b46facb3830cef03a68705cff4e55b21a1babd14919b2d72c52ba2\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=2be25b56b08d6fdf1ac64ffc8c016b2214e715461c6f7a0250c6cc2e7a036c57\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
      "Successfully built kaggle py-cpuinfo termcolor pycocotools seqeval promise\n",
      "Installing collected packages: six, typing-extensions, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, grpcio, cached-property, absl-py, typeguard, termcolor, tensorflow-metadata, tensorflow-estimator, tensorboard, promise, portalocker, opt-einsum, keras-preprocessing, keras-nightly, importlib-resources, h5py, google-pasta, gast, flatbuffers, dm-tree, dill, Cython, astunparse, tf-slim, tensorflow-model-optimization, tensorflow-hub, tensorflow-datasets, tensorflow-addons, tensorflow, seqeval, sentencepiece, sacrebleu, pycocotools, py-cpuinfo, opencv-python-headless, oauth2client, kaggle, gin-config, tf-models-official, tensorflow-text\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.0\n",
      "    Uninstalling grpcio-1.38.0:\n",
      "      Successfully uninstalled grpcio-1.38.0\n",
      "Successfully installed Cython-0.29.23 absl-py-0.12.0 astunparse-1.6.3 cached-property-1.5.2 dill-0.3.4 dm-tree-0.1.6 flatbuffers-1.12 gast-0.4.0 gin-config-0.4.0 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 importlib-resources-5.2.0 kaggle-1.5.12 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 oauth2client-4.1.3 opencv-python-headless-4.5.2.54 opt-einsum-3.3.0 portalocker-2.0.0 promise-2.3 py-cpuinfo-8.0.0 pycocotools-2.0.2 sacrebleu-1.5.1 sentencepiece-0.1.96 seqeval-1.2.2 six-1.15.0 tensorboard-2.5.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0 tensorflow-addons-0.13.0 tensorflow-datasets-4.3.0 tensorflow-estimator-2.5.0 tensorflow-hub-0.12.0 tensorflow-metadata-1.1.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.5.0 termcolor-1.1.0 tf-models-official-2.5.0 tf-slim-1.1.0 typeguard-2.12.1 typing-extensions-3.7.4.3 werkzeug-2.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade tf-models-official==2.5.0 tensorflow-text==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.40.0)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.18.1)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.30.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.30.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.5.30)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest GA version of google-cloud-storage library as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /home/jupyter/.local/lib/python3.7/site-packages (1.40.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.30.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.30.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (20.9)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.53.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  jk-mlops-dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06571eb4063b"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append it onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "697568e92bd6"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "In this example, your training application uses Cloud Storage for accessing training and validation datasets and for storing checkpoints. \n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://jk-rs-example\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jk-rs-notebook-test/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jk-rs-notebook-test' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp.data import classifier_data_lib\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
    "    JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex constants\n",
    "\n",
    "Setup up the following constants for Vertex:\n",
    "\n",
    "- API_ENDPOINT: The Vertex API service endpoint for job services.\n",
    "- PARENT: The Vertex location root path for job resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = '{}-aiplatform.googleapis.com'.format(REGION)\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "## Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vertex client library works as a client/server model. On your side (the Python script) you will create a client that sends requests and receives responses from the Vertex server.\n",
    "\n",
    "In this example, you use the Job Service client for submitting and monitoring custom training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "job_client = JobServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets\n",
    "\n",
    "TensorFlow NLP Modelling Toolkit that you use for fine tuning BERT requires datasets in the specific format. The toolkit includes a set of utility functions to help with data conversions. You will use them to convert the *glue/mnli* dataset from TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnli_tfrecords(train_data_output_path, \n",
    "                            eval_data_output_path,\n",
    "                            metadata_file_path,\n",
    "                            vocab_file='gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/vocab.txt', \n",
    "                            mnli_type='matched', \n",
    "                            max_seq_length=128, \n",
    "                            do_lower_case=True):\n",
    "    \"\"\"Generates MNLI training and validation splits in the TFRecord format\n",
    "    compatible with TensorfFlow NLP Modelling Toolkit.\"\"\"\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "    processor_text_fn = tokenization.convert_to_unicode\n",
    "\n",
    "    if mnli_type == 'matched':\n",
    "        tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched'\n",
    "    else: \n",
    "        tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_mismatched'\n",
    "\n",
    "    processor = classifier_data_lib.TfdsProcessor(\n",
    "        tfds_params=tfds_params, process_text_fn=processor_text_fn)\n",
    "\n",
    "    metadata = classifier_data_lib.generate_tf_record_from_data_file(\n",
    "        processor,\n",
    "        None,\n",
    "        tokenizer,\n",
    "        train_data_output_path=train_data_output_path,\n",
    "        eval_data_output_path=eval_data_output_path,\n",
    "        max_seq_length=max_seq_length)\n",
    "\n",
    "    with tf.io.gfile.GFile(metadata_file_path, \"w\") as writer:\n",
    "        writer.write(json.dumps(metadata, indent=4) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_LOCATION = f'{BUCKET_NAME}/datasets/MNLI'\n",
    "TRAIN_FILE = f'{OUTPUT_LOCATION}/mnli_train.tf_record'\n",
    "EVAL_FILE = f'{OUTPUT_LOCATION}/mnli_valid.tf_record'\n",
    "METADATA_FILE = f'{OUTPUT_LOCATION}/metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/home/jupyter/tensorflow_datasets/glue/mnli/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Writing example 0 of 392702\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-0\n",
      "INFO:absl:tokens: [CLS] meaningful partnerships with stakeholders is crucial . [SEP] in recognition of these tensions , l ##sc has worked dil ##igen ##tly since 1995 to convey the expectations of the state planning initiative and to establish meaningful partnerships with stakeholders aimed at foster ##ing a new sy ##mb ##ios ##is between the federal provider and recipients of legal services funding . [SEP]\n",
      "INFO:absl:input_ids: 101 15902 13797 2007 22859 2003 10232 1012 102 1999 5038 1997 2122 13136 1010 1048 11020 2038 2499 29454 29206 14626 2144 2786 2000 16636 1996 10908 1997 1996 2110 4041 6349 1998 2000 5323 15902 13797 2007 22859 6461 2012 6469 2075 1037 2047 25353 14905 10735 2483 2090 1996 2976 10802 1998 15991 1997 3423 2578 4804 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-1\n",
      "INFO:absl:tokens: [CLS] the clinton followers kept to the higher ground in the discussion . [SEP] the clinton sur ##rogate ##s also held the high ground in the context war . [SEP]\n",
      "INFO:absl:input_ids: 101 1996 7207 8771 2921 2000 1996 3020 2598 1999 1996 6594 1012 102 1996 7207 7505 21799 2015 2036 2218 1996 2152 2598 1999 1996 6123 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-2\n",
      "INFO:absl:tokens: [CLS] women have jobs in all areas of the workforce , they are almost getting the same wages as most men , [SEP] um - hum because women are in every field now i mean i can ' t think of a field that they ' re not involved in [SEP]\n",
      "INFO:absl:input_ids: 101 2308 2031 5841 1999 2035 2752 1997 1996 14877 1010 2027 2024 2471 2893 1996 2168 12678 2004 2087 2273 1010 102 8529 1011 14910 2138 2308 2024 1999 2296 2492 2085 1045 2812 1045 2064 1005 1056 2228 1997 1037 2492 2008 2027 1005 2128 2025 2920 1999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-3\n",
      "INFO:absl:tokens: [CLS] houston is freezing and dry right now . [SEP] houston is really humid now [SEP]\n",
      "INFO:absl:input_ids: 101 5395 2003 12809 1998 4318 2157 2085 1012 102 5395 2003 2428 14178 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-4\n",
      "INFO:absl:tokens: [CLS] but they wouldn ' t be leaving right now . [SEP] but not now . [SEP]\n",
      "INFO:absl:input_ids: 101 2021 2027 2876 1005 1056 2022 2975 2157 2085 1012 102 2021 2025 2085 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:Writing example 10000 of 392702\n",
      "INFO:absl:Writing example 20000 of 392702\n",
      "INFO:absl:Writing example 30000 of 392702\n",
      "INFO:absl:Writing example 40000 of 392702\n",
      "INFO:absl:Writing example 50000 of 392702\n",
      "INFO:absl:Writing example 60000 of 392702\n",
      "INFO:absl:Writing example 70000 of 392702\n",
      "INFO:absl:Writing example 80000 of 392702\n",
      "INFO:absl:Writing example 90000 of 392702\n",
      "INFO:absl:Writing example 100000 of 392702\n",
      "INFO:absl:Writing example 110000 of 392702\n",
      "INFO:absl:Writing example 120000 of 392702\n",
      "INFO:absl:Writing example 130000 of 392702\n",
      "INFO:absl:Writing example 140000 of 392702\n",
      "INFO:absl:Writing example 150000 of 392702\n",
      "INFO:absl:Writing example 160000 of 392702\n",
      "INFO:absl:Writing example 170000 of 392702\n",
      "INFO:absl:Writing example 180000 of 392702\n",
      "INFO:absl:Writing example 190000 of 392702\n",
      "INFO:absl:Writing example 200000 of 392702\n",
      "INFO:absl:Writing example 210000 of 392702\n",
      "INFO:absl:Writing example 220000 of 392702\n",
      "INFO:absl:Writing example 230000 of 392702\n",
      "INFO:absl:Writing example 240000 of 392702\n",
      "INFO:absl:Writing example 250000 of 392702\n",
      "INFO:absl:Writing example 260000 of 392702\n",
      "INFO:absl:Writing example 270000 of 392702\n",
      "INFO:absl:Writing example 280000 of 392702\n",
      "INFO:absl:Writing example 290000 of 392702\n",
      "INFO:absl:Writing example 300000 of 392702\n",
      "INFO:absl:Writing example 310000 of 392702\n",
      "INFO:absl:Writing example 320000 of 392702\n",
      "INFO:absl:Writing example 330000 of 392702\n",
      "INFO:absl:Writing example 340000 of 392702\n",
      "INFO:absl:Writing example 350000 of 392702\n",
      "INFO:absl:Writing example 360000 of 392702\n",
      "INFO:absl:Writing example 370000 of 392702\n",
      "INFO:absl:Writing example 380000 of 392702\n",
      "INFO:absl:Writing example 390000 of 392702\n",
      "INFO:absl:Writing example 0 of 9815\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-0\n",
      "INFO:absl:tokens: [CLS] yeah lots of people for the right life [SEP] uh - huh oh yeah all the people for right uh life or something [SEP]\n",
      "INFO:absl:input_ids: 101 3398 7167 1997 2111 2005 1996 2157 2166 102 7910 1011 9616 2821 3398 2035 1996 2111 2005 2157 7910 2166 2030 2242 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-1\n",
      "INFO:absl:tokens: [CLS] i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant . [SEP] also , i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant as limited quantities of mail move back and forth between basic and works ##har ##ed . [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 1012 102 2036 1010 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 2004 3132 12450 1997 5653 2693 2067 1998 5743 2090 3937 1998 2573 8167 2098 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-2\n",
      "INFO:absl:tokens: [CLS] it cost a lot of money to put forth all that effort . [SEP] taken up by the oh okay oh so you know well that ' s i had wondered sometimes i knew that there was a lot of a lot of effort and a lot of work went into a lot of that and i just wondered if if it lasted and if it took you know like yeah [SEP]\n",
      "INFO:absl:input_ids: 101 2009 3465 1037 2843 1997 2769 2000 2404 5743 2035 2008 3947 1012 102 2579 2039 2011 1996 2821 3100 2821 2061 2017 2113 2092 2008 1005 1055 1045 2018 4999 2823 1045 2354 2008 2045 2001 1037 2843 1997 1037 2843 1997 3947 1998 1037 2843 1997 2147 2253 2046 1037 2843 1997 2008 1998 1045 2074 4999 2065 2065 2009 6354 1998 2065 2009 2165 2017 2113 2066 3398 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-3\n",
      "INFO:absl:tokens: [CLS] i know nothing about the motor oil . [SEP] yeah i know the motor oil [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2113 2498 2055 1996 5013 3514 1012 102 3398 1045 2113 1996 5013 3514 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-4\n",
      "INFO:absl:tokens: [CLS] many hotels in lake ##land have a cost - effective option of dinner , bed , and breakfast with an evening meal in . [SEP] many lake ##land hotels also quote a d , b and b ( dinner , bed , and breakfast ) rate , which includes the evening meal and is often quite cost - effective . [SEP]\n",
      "INFO:absl:input_ids: 101 2116 9275 1999 2697 3122 2031 1037 3465 1011 4621 5724 1997 4596 1010 2793 1010 1998 6350 2007 2019 3944 7954 1999 1012 102 2116 2697 3122 9275 2036 14686 1037 1040 1010 1038 1998 1038 1006 4596 1010 2793 1010 1998 6350 1007 3446 1010 2029 2950 1996 3944 7954 1998 2003 2411 3243 3465 1011 4621 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n"
     ]
    }
   ],
   "source": [
    "generate_mnli_tfrecords(TRAIN_FILE, EVAL_FILE, METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk-rs-notebook-test/datasets/MNLI/\n",
      "gs://jk-rs-notebook-test/datasets/MNLI/metadata.json\n",
      "gs://jk-rs-notebook-test/datasets/MNLI/mnli_train.tf_record\n",
      "gs://jk-rs-notebook-test/datasets/MNLI/mnli_valid.tf_record\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls {OUTPUT_LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"processor_type\": \"TFDS_glue/mnli\",\n",
      "    \"train_data_size\": 392702,\n",
      "    \"max_seq_length\": 128,\n",
      "    \"task_type\": \"bert_classification\",\n",
      "    \"num_labels\": 3,\n",
      "    \"eval_data_size\": 9815\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! gsutil cat {METADATA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you can train a custom model using a container image:\n",
    "\n",
    "- **Use a Google Cloud prebuilt container**. If you use a prebuilt container, you will additionally specify a Python package to install into the container image. \n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container needs to contain your code for training a custom model.\n",
    "\n",
    "In this example, you use a custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training script\n",
    "\n",
    "Your training script is based on [the common training driver](https://github.com/tensorflow/models/blob/master/official/nlp/docs/train.md) from TensorFlow NLP Modelling Toolkit. The base driver has been adapted to work seamlessly on a distributed compute environment provisioned when running a Vertex training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf training_image\n",
    "! mkdir training_image\n",
    "! mkdir training_image/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/train.py\n",
    "\n",
    "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"TFM common training driver.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import gin\n",
    "\n",
    "\n",
    "from official.common import distribute_utils\n",
    "# pylint: disable=unused-import\n",
    "from official.common import registry_imports\n",
    "# pylint: enable=unused-import\n",
    "from official.common import flags as tfm_flags\n",
    "from official.core import task_factory\n",
    "from official.core import train_lib\n",
    "from official.core import train_utils\n",
    "from official.modeling import performance\n",
    "\n",
    "from tensorflow.dtypes import float16, bfloat16, float32\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def _get_model_dir(model_dir):\n",
    "  \"\"\"Adjusts model dir for multi-worker training.\n",
    "  \n",
    "     Checkpointing and Saving need to happen on each worker and they need to write \n",
    "     to different paths as they would override each others. This utility function\n",
    "     adjusts the base model dir passed as a flag using Vertex AI cluster topology\n",
    "  \"\"\"\n",
    "  \n",
    "  def _is_chief(task_type, task_id):\n",
    "    return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "  \n",
    "  tf_config = os.getenv('TF_CONFIG')\n",
    "  if tf_config:\n",
    "    tf_config = json.loads(tf_config)\n",
    "   \n",
    "    if not _is_chief(tf_config['task']['type'], tf_config['task']['index']):\n",
    "      model_dir = os.path.join(model_dir, 'worker-{}').format(tf_config['task']['index'])\n",
    "  \n",
    "  logging.info('Setting model_dir to: %s', model_dir)\n",
    "  \n",
    "  return model_dir\n",
    "\n",
    "def main(_):\n",
    "  \n",
    "  model_dir = _get_model_dir(FLAGS.model_dir)\n",
    "\n",
    "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_params)\n",
    "  params = train_utils.parse_configuration(FLAGS)\n",
    "  \n",
    "  if 'train' in FLAGS.mode:\n",
    "    # Pure eval modes do not output yaml files. Otherwise continuous eval job\n",
    "    # may race against the train job for writing the same file.\n",
    "    train_utils.serialize_config(params, model_dir)\n",
    "\n",
    "  # Sets mixed_precision policy. Using 'mixed_float16' or 'mixed_bfloat16'\n",
    "  # can have significant impact on model speeds by utilizing float16 in case of\n",
    "  # GPUs, and bfloat16 in the case of TPUs. loss_scale takes effect only when\n",
    "  # dtype is float16\n",
    "  if params.runtime.mixed_precision_dtype:\n",
    "    # A mitigation for an apparent bug in passing mixed precision\n",
    "    if params.runtime.mixed_precision_dtype == 'mixed_float16':\n",
    "      precision_dtype = float16\n",
    "    elif params.runtime_mixed_precision_dtype == 'mixed_bfloat16':\n",
    "      precision_dtype = bfloat16\n",
    "    else:\n",
    "      precision_dtype = float32 \n",
    "    performance.set_mixed_precision_policy(precision_dtype)\n",
    "  distribution_strategy = distribute_utils.get_distribution_strategy(\n",
    "      distribution_strategy=params.runtime.distribution_strategy,\n",
    "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
    "      num_gpus=params.runtime.num_gpus,\n",
    "      tpu_address=params.runtime.tpu,\n",
    "      **params.runtime.model_parallelism())\n",
    "  with distribution_strategy.scope():\n",
    "    task = task_factory.get_task(params.task, logging_dir=model_dir)\n",
    "\n",
    "  train_lib.run_experiment(\n",
    "      distribution_strategy=distribution_strategy,\n",
    "      task=task,\n",
    "      mode=FLAGS.mode,\n",
    "      params=params,\n",
    "      model_dir=model_dir)\n",
    "\n",
    "  train_utils.save_gin_config(FLAGS.mode, model_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tfm_flags.define_flags()\n",
    "  app.run(main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a base configuration for the MNLI fine tuning task\n",
    "\n",
    "The training script supports overriding the default configuration of a TensorFlow Modelling Toolkit task using either YAML configuration files or command line parameters. The below YAML file sets the base configuration for the MNLI fine tuning task used in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/trainer/glue_mnli_matched.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/glue_mnli_matched.yaml\n",
    "\n",
    "task:\n",
    "  hub_module_url: 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "  model:\n",
    "    num_classes: 3\n",
    "  init_checkpoint: ''\n",
    "  metric_type: 'accuracy'\n",
    "  train_data:\n",
    "    drop_remainder: true\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: true\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "  validation_data:\n",
    "    drop_remainder: false\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: false\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "trainer:\n",
    "  checkpoint_interval: 3000\n",
    "  optimizer_config:\n",
    "    learning_rate:\n",
    "      polynomial:\n",
    "        # 100% of train_steps.\n",
    "        decay_steps: 36813\n",
    "        end_learning_rate: 0.0\n",
    "        initial_learning_rate: 3.0e-05\n",
    "        power: 1.0\n",
    "      type: polynomial\n",
    "    optimizer:\n",
    "      type: adamw\n",
    "    warmup:\n",
    "      polynomial:\n",
    "        power: 1\n",
    "        # ~10% of train_steps.\n",
    "        warmup_steps: 3681\n",
    "      type: polynomial\n",
    "  steps_per_loop: 1000\n",
    "  summary_interval: 1000\n",
    "  # Training data size 392,702 examples, 3 epochs.\n",
    "  train_steps: 36813\n",
    "  validation_interval: 6135\n",
    "  # Eval data size = 9815 examples.\n",
    "  validation_steps: 307\n",
    "  best_checkpoint_export_subdir: 'best_ckpt'\n",
    "  best_checkpoint_eval_metric: 'cls_accuracy'\n",
    "  best_checkpoint_metric_comp: 'higher'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "\n",
    "The custom training container image packages TensorFlow NLP Modelling Toolkit with the training script and the default configuration file created in the previous steps. It also install the Reduction Server NCCL plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "\n",
    "RUN apt remove -y google-fast-socket \\\n",
    "&&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list \\\n",
    "&&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n",
    "&&  apt update && apt install -y google-reduction-server\n",
    "\n",
    "RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY trainer /trainer\n",
    "\n",
    "ENTRYPOINT [\"python\"]\n",
    "CMD [\"-c\", \"print('TF Model Garden')\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the container image and push it to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/mnli_finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  9.728kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 0f998c784cd6\n",
      "Step 2/7 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Using cache\n",
      " ---> 794e327d7cb5\n",
      "Step 3/7 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Using cache\n",
      " ---> 5633b74e153c\n",
      "Step 4/7 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> a6ebf2f1b24f\n",
      "Step 5/7 : COPY trainer /trainer\n",
      " ---> 804d2ba96c94\n",
      "Step 6/7 : ENTRYPOINT [\"python\"]\n",
      " ---> Running in 4bc7a8499d13\n",
      "Removing intermediate container 4bc7a8499d13\n",
      " ---> 4f4d59ce39dc\n",
      "Step 7/7 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Running in a11676beb4ad\n",
      "Removing intermediate container a11676beb4ad\n",
      " ---> d42b6b08cffc\n",
      "Successfully built d42b6b08cffc\n",
      "Successfully tagged gcr.io/jk-mlops-dev/mnli_finetuning:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your custom job specification\n",
    "\n",
    "You can now create a Job Specification for your distributed training job with Reduction Server. \n",
    "\n",
    "If you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica. A group of replicas with the same configuration is called a worker pool. Vertex AI provides 4 worker pools to cover the different types of machine tasks.\n",
    "\n",
    "Worker pool 0 configures the Primary, chief, scheduler, or \"master\".  This worker generally takes on some extra work such as saving checkpoints and writing summary files. There is only ever one chief worker in a cluster, so your worker count for worker pool 0 will always be 1.\n",
    "\n",
    "Worker pool 1 is where you configure the rest of the workers for your cluster. \n",
    " \n",
    "Worker pool 2 manages Reduction Server reducers. \n",
    "\n",
    "Worker pools 0 and 1 run your custom training container you created in the previous step. Worker pool 2 uses the Reduction Server image provided by Vertex AI.\n",
    "\n",
    "The below helper function creates a custom job specification using the described worker pool topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure worker pools\n",
    "\n",
    "Adjust the following constants to reflect the machine types and a number of replicas for your worker pools. \n",
    "\n",
    "When choosing the number and type of reducers, you should consider the network bandwidth supported by a reducer replica’s machine type. In GCP, a VM’s machine type defines its maximum possible egress bandwidth. For example, the egress bandwidth of the n1-highcpu-16 machine type is limited at 32 Gbps.\n",
    "\n",
    "Because reducers perform a very limited function, aggregating blocks of gradients, they can run on relatively low-powered and cost effective machines. Even with a large number of gradients this computation does not require accelerated hardware or high CPU or memory resources. However, to avoid network bottlenecks, the total aggregate bandwidth of all replicas in the reducer worker pool must be greater or equal to the total aggregate bandwidth of all replicas in worker pools 0 and 1, which host the GPU workers.\n",
    "\n",
    "Refer to [the article](TBD) for more information about configuring worker pools when using Reduction Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICA_COUNT = 8\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 10\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the MNLI experiment settings\n",
    "\n",
    "As noted before, the training script supports overriding the default configuration of a TensorFlow Modelling Toolkit task using YAML configuration files and command line parameters. The base configuration for the MNLI fine tuning task is defined in the YAML file packaged into the training container. With each training run you can override selected parameters by using the `params_override` command line argument of the training script.\n",
    "\n",
    "The `params_override` argument accepts a string with comma separated key/value pairs for each parameter to be overwritten.\n",
    "\n",
    "The following parameters are overwritten in the following cell.\n",
    "- `trainer.train_step` - A number of training steps. Recall that there is 392,702 examples in the training data set. \n",
    "- `trainer.steps_per_loop` - The training script prints out updates about training progress every `steps_per_loop`\n",
    "- `trainer.summary_interval` - The training script logs Tensorboard summaries every `summary_interval`\n",
    "- `trainer.validation_interval` - The training script runs validation every `validation_interval`\n",
    "- `trainer.checkpoint_interval` - The training script creates a checkpoint every `checkpoint_interval`\n",
    "- `task.train_data.global_batch_size` - A global batch size for training data. This value should be adjusted based on a GPU type and a number of GPU workers. For example when using NVidia V100 GPUs a batch size of 16 per GPU is a good starting point. With 2 workers, the `global_batch_size` would be 32\n",
    "- `task.validation_data.global_batch_size` - A global batch size for validation data\n",
    "- `task.train_data.input_path` - A location of the training dataset\n",
    "- `task.validation_data.input_path` - A location of the validation dataset\n",
    "- `runtime.num_gpus` - A number of GPUs to use on each worker. This should be adjusted based on the type of a worker machine\n",
    "- `runtime.distribution_strategy` - TensorFlow distribution strategy. \n",
    "- `runtime.all_reduce_alg=nccl` - You ust use NVidia NCCL with Reduction Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'trainer.train_steps=2000',\n",
    "    'trainer.steps_per_loop=100',\n",
    "    'trainer.summary_interval=100',\n",
    "    'trainer.validation_interval=2000',\n",
    "    'trainer.checkpoint_interval=2000',\n",
    "    'task.train_data.global_batch_size=32',\n",
    "    'task.validation_data.global_batch_size=32',\n",
    "    'task.train_data.input_path=' + TRAIN_FILE,\n",
    "    'task.validation_data.input_path=' + EVAL_FILE,\n",
    "    'runtime.num_gpus=' + str(PER_MACHINE_ACCELERATOR_COUNT),\n",
    "    'runtime.distribution_strategy=multi_worker_mirrored',\n",
    "    'runtime.all_reduce_alg=nccl',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a job spec\n",
    "\n",
    "You are now ready to generate the job spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210708_192800',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-notebook-test/MNLI_20210708_192800/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=32,task.validation_data.global_batch_size=32,task.train_data.input_path=gs://jk-rs-notebook-test/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-notebook-test/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-notebook-test/MNLI_20210708_192800/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=32,task.validation_data.global_batch_size=32,task.train_data.input_path=gs://jk-rs-notebook-test/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-notebook-test/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 7},\n",
      "                                    {'container_spec': {'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'},\n",
      "                                     'machine_spec': {'machine_type': 'n1-highcpu-16'},\n",
      "                                     'replica_count': 10}]}}\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{BUCKET_NAME}/{JOB_NAME}/model'\n",
    "\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=bert/sentence_prediction',\n",
    "    '--mode=train',\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=trainer/glue_mnli_matched.yaml',\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
