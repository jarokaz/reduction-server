{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Distributed training with Reduction Server\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\"\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/ai-platform-samples/blob/master/ai-platform-unified/notebooks/notebook_template.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to optimize large distributed training Vertex AI jobs using Reduction Server. \n",
    "\n",
    "The machine learning task in this example is fine tuning a BERT model for sentence prediction using  the Multi-Genre Natural Language Inference Corpus (MNLI) from the GLUE benchmark. \n",
    "\n",
    "The example uses components from [TensorFlow NLP Modelling Toolkit](https://github.com/tensorflow/models/tree/master/official/nlp#tensorflow-nlp-modelling-toolkit) and distributed training is implemented using [tf.distribute.MultiWorkerMirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy). \n",
    "\n",
    "For more information about using Reduction Server to optimize distributed training refer to the [Optimizing distributed training with Vertex AI Reduction Server](tbd) article.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "\n",
    "\n",
    "The example uses the *glue/mnli* dataset from [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/glue).\n",
    "\n",
    "### Objective\n",
    "\n",
    "In this notebook, you will learn how to configure, submit and monitor a Vertex AI custom training job that uses Reduction Server to optimize network bandwith and latency of the gradient reduction operation in the distributed training setting.  \n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Building a custom training container image based on TensorFlow NLP Modelling Toolkit\n",
    "- Converting the glue/mnli dataset to the format required by TensorFlow NLP Modelling Toolkit\n",
    "- Preparing a Vertex AI custom container training job that uses Reduction Server\n",
    "- Submitting and monitoring the job\n",
    "\n",
    "### Costs \n",
    "\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ze4-nDLfK4pw"
   },
   "source": [
    "### Set up your local development environment\n",
    "\n",
    "**If you are using Colab or Google Cloud Notebooks**, your environment already meets\n",
    "all the requirements to run this notebook. You can skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCuSR8GkAgzl"
   },
   "source": [
    "**Otherwise**, make sure your environment meets this notebook's requirements.\n",
    "You need the following:\n",
    "\n",
    "* The Google Cloud SDK\n",
    "* Git\n",
    "* Python 3\n",
    "* virtualenv\n",
    "* Jupyter notebook running in a virtual environment with Python 3\n",
    "\n",
    "The Google Cloud guide to [Setting up a Python development\n",
    "environment](https://cloud.google.com/python/setup) and the [Jupyter\n",
    "installation guide](https://jupyter.org/install) provide detailed instructions\n",
    "for meeting these requirements. The following steps provide a condensed set of\n",
    "instructions:\n",
    "\n",
    "1. [Install and initialize the Cloud SDK.](https://cloud.google.com/sdk/docs/)\n",
    "\n",
    "1. [Install Python 3.](https://cloud.google.com/python/setup#installing_python)\n",
    "\n",
    "1. [Install\n",
    "   virtualenv](https://cloud.google.com/python/setup#installing_and_using_virtualenv)\n",
    "   and create a virtual environment that uses Python 3. Activate the virtual environment.\n",
    "\n",
    "1. To install Jupyter, run `pip3 install jupyter` on the\n",
    "command-line in a terminal shell.\n",
    "\n",
    "1. To launch Jupyter, run `jupyter notebook` on the command-line in a terminal shell.\n",
    "\n",
    "1. Open this notebook in the Jupyter Notebook Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "### Install the required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install TensorFlow NLP Modelling Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "2b4ef9b72d43"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-models-official==2.5.0 in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: tensorflow-text==2.5.0 in /opt/conda/lib/python3.7/site-packages (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.4.1)\n",
      "Requirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.5.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.15.0)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.5.2.54)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.1.96)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.5.12)\n",
      "Requirement already satisfied: pycocotools in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.0.2)\n",
      "Requirement already satisfied: Cython in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.29.23)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.2.0)\n",
      "Requirement already satisfied: gin-config in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-addons in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.13.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.20.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: seqeval in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.2)\n",
      "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.6.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.3.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.10.0)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.0.0)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.1.3)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.2)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.1.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.19.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.16.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2021.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.18.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.34.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (1.26.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (4.61.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (5.0.2)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2.8.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.10)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.36.2)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.7.4.3)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (4.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.5.0) (0.1.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0) (1.3)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /opt/conda/lib/python3.7/site-packages (from sacrebleu->tf-models-official==2.5.0) (2.0.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (2.1.0)\n",
      "Requirement already satisfied: typeguard>=2.7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-addons->tf-models-official==2.5.0) (2.12.1)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (5.2.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.18.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (21.2.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.3.4)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (2.3)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade tf-models-official==2.5.0 tensorflow-text==2.5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest version of Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "wyy5Lbnzg5fi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.30.0)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.40.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.18.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (49.6.0.post20210108)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.30.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.7.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the latest GA version of google-cloud-storage library as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.40.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.1)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.30.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (1.15.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage) (49.6.0.post20210108)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.30.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (20.9)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (1.53.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<2.0.0dev,>=1.21.0->google-cloud-core<2.0dev,>=1.4.1->google-cloud-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.11.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.5)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-storage $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhq5zEbGg0XX"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "After you install the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "EzrelQZ22IZj"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWEdiXsJg0XY"
   },
   "source": [
    "## Before you begin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI API and Compute Engine API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,compute_component). {TODO: Update the APIs needed for your tutorial. Edit the API names, and update the link to append the API IDs, separating each one with a comma. For example, container.googleapis.com,cloudbuild.googleapis.com}\n",
    "\n",
    "1. If you are running this notebook locally, you will need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJYoRfYng0XZ"
   },
   "source": [
    "Otherwise, set your project ID here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "riG_qUokg0XZ"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None:\n",
    "    PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr--iN2kAylZ"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Google Cloud Notebooks**, your environment is already\n",
    "authenticated. Skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR"
   },
   "source": [
    "**If you are using Colab**, run the cell below and follow the instructions\n",
    "when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "1. In the Cloud Console, go to the [**Create service account key**\n",
    "   page](https://console.cloud.google.com/apis/credentials/serviceaccountkey).\n",
    "\n",
    "2. Click **Create service account**.\n",
    "\n",
    "3. In the **Service account name** field, enter a name, and\n",
    "   click **Create**.\n",
    "\n",
    "4. In the **Grant this service account access to project** section, click the **Role** drop-down list. Type \"Vertex AI\"\n",
    "into the filter box, and select\n",
    "   **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "5. Click *Create*. A JSON file that contains your key downloads to your\n",
    "local environment.\n",
    "\n",
    "6. Enter the path to your service account key as the\n",
    "`GOOGLE_APPLICATION_CREDENTIALS` variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPO1eR3CYjk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "\n",
    "In this example, your training application uses Cloud Storage for accessing training and validation datasets and for storing checkpoints. \n",
    "\n",
    "Set the name of your Cloud Storage bucket below. It must be unique across all\n",
    "Cloud Storage buckets.\n",
    "\n",
    "You may also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook. Make sure to [choose a region where Vertex AI services are\n",
    "available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). You may\n",
    "not use a Multi-Regional Storage bucket for training with Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "REGION = \"[your-region]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MzGDU7TWdts_"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://jk-rs-example\"  # @param {type:\"string\"}\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cf221059d072"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EcIXiGsCePi"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NIq7R4HZCfIc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://jk-rs-example/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'jk-rs-example' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucvCsknMCims"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vhOb7YnwClBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://jk-rs-example/MNLI_20210708_222812/\n",
      "                                 gs://jk-rs-example/datasets/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoEqT2Y4DJmf"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pRUOFELefqf1"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "import sys\n",
    "import shutil\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from official.nlp.bert import tokenization\n",
    "from official.nlp.data import classifier_data_lib\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
    "    JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex constants\n",
    "\n",
    "Setup up the following constants for Vertex:\n",
    "\n",
    "- API_ENDPOINT: The Vertex API service endpoint for job services.\n",
    "- PARENT: The Vertex location root path for job resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_ENDPOINT = '{}-aiplatform.googleapis.com'.format(REGION)\n",
    "PARENT = \"projects/\" + PROJECT_ID + \"/locations/\" + REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6ppE7imft-y"
   },
   "source": [
    "## Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vertex client library works as a client/server model. On your side (the Python script) you will create a client that sends requests and receives responses from the Vertex server.\n",
    "\n",
    "In this example, you use the Job Service client for submitting and monitoring custom training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_options = {\"api_endpoint\": API_ENDPOINT}\n",
    "job_client = JobServiceClient(client_options=client_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and validation datasets\n",
    "\n",
    "TensorFlow NLP Modelling Toolkit that you use for fine tuning BERT requires datasets in the specific format. The toolkit includes a set of utility functions to help with data conversions. You will use them to convert the *glue/mnli* dataset from TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mnli_tfrecords(train_data_output_path, \n",
    "                            eval_data_output_path,\n",
    "                            metadata_file_path,\n",
    "                            vocab_file='gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/vocab.txt', \n",
    "                            mnli_type='matched', \n",
    "                            max_seq_length=128, \n",
    "                            do_lower_case=True):\n",
    "    \"\"\"Generates MNLI training and validation splits in the TFRecord format\n",
    "    compatible with TensorfFlow NLP Modelling Toolkit.\"\"\"\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "    processor_text_fn = tokenization.convert_to_unicode\n",
    "\n",
    "    if mnli_type == 'matched':\n",
    "        tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched'\n",
    "    else: \n",
    "        tfds_params = 'dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_mismatched'\n",
    "\n",
    "    processor = classifier_data_lib.TfdsProcessor(\n",
    "        tfds_params=tfds_params, process_text_fn=processor_text_fn)\n",
    "\n",
    "    metadata = classifier_data_lib.generate_tf_record_from_data_file(\n",
    "        processor,\n",
    "        None,\n",
    "        tokenizer,\n",
    "        train_data_output_path=train_data_output_path,\n",
    "        eval_data_output_path=eval_data_output_path,\n",
    "        max_seq_length=max_seq_length)\n",
    "\n",
    "    with tf.io.gfile.GFile(metadata_file_path, \"w\") as writer:\n",
    "        writer.write(json.dumps(metadata, indent=4) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_LOCATION = f'{BUCKET_NAME}/datasets/MNLI'\n",
    "TRAIN_FILE = f'{OUTPUT_LOCATION}/mnli_train.tf_record'\n",
    "EVAL_FILE = f'{OUTPUT_LOCATION}/mnli_valid.tf_record'\n",
    "METADATA_FILE = f'{OUTPUT_LOCATION}/metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Reusing dataset glue (/home/jupyter/tensorflow_datasets/glue/mnli/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset glue for split None, from /home/jupyter/tensorflow_datasets/glue/mnli/1.0.0\n",
      "INFO:absl:Writing example 0 of 392702\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-0\n",
      "INFO:absl:tokens: [CLS] meaningful partnerships with stakeholders is crucial . [SEP] in recognition of these tensions , l ##sc has worked dil ##igen ##tly since 1995 to convey the expectations of the state planning initiative and to establish meaningful partnerships with stakeholders aimed at foster ##ing a new sy ##mb ##ios ##is between the federal provider and recipients of legal services funding . [SEP]\n",
      "INFO:absl:input_ids: 101 15902 13797 2007 22859 2003 10232 1012 102 1999 5038 1997 2122 13136 1010 1048 11020 2038 2499 29454 29206 14626 2144 2786 2000 16636 1996 10908 1997 1996 2110 4041 6349 1998 2000 5323 15902 13797 2007 22859 6461 2012 6469 2075 1037 2047 25353 14905 10735 2483 2090 1996 2976 10802 1998 15991 1997 3423 2578 4804 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-1\n",
      "INFO:absl:tokens: [CLS] the clinton followers kept to the higher ground in the discussion . [SEP] the clinton sur ##rogate ##s also held the high ground in the context war . [SEP]\n",
      "INFO:absl:input_ids: 101 1996 7207 8771 2921 2000 1996 3020 2598 1999 1996 6594 1012 102 1996 7207 7505 21799 2015 2036 2218 1996 2152 2598 1999 1996 6123 2162 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-2\n",
      "INFO:absl:tokens: [CLS] women have jobs in all areas of the workforce , they are almost getting the same wages as most men , [SEP] um - hum because women are in every field now i mean i can ' t think of a field that they ' re not involved in [SEP]\n",
      "INFO:absl:input_ids: 101 2308 2031 5841 1999 2035 2752 1997 1996 14877 1010 2027 2024 2471 2893 1996 2168 12678 2004 2087 2273 1010 102 8529 1011 14910 2138 2308 2024 1999 2296 2492 2085 1045 2812 1045 2064 1005 1056 2228 1997 1037 2492 2008 2027 1005 2128 2025 2920 1999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-3\n",
      "INFO:absl:tokens: [CLS] houston is freezing and dry right now . [SEP] houston is really humid now [SEP]\n",
      "INFO:absl:input_ids: 101 5395 2003 12809 1998 4318 2157 2085 1012 102 5395 2003 2428 14178 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: train-4\n",
      "INFO:absl:tokens: [CLS] but they wouldn ' t be leaving right now . [SEP] but not now . [SEP]\n",
      "INFO:absl:input_ids: 101 2021 2027 2876 1005 1056 2022 2975 2157 2085 1012 102 2021 2025 2085 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:Writing example 10000 of 392702\n",
      "INFO:absl:Writing example 20000 of 392702\n",
      "INFO:absl:Writing example 30000 of 392702\n",
      "INFO:absl:Writing example 40000 of 392702\n",
      "INFO:absl:Writing example 50000 of 392702\n",
      "INFO:absl:Writing example 60000 of 392702\n",
      "INFO:absl:Writing example 70000 of 392702\n",
      "INFO:absl:Writing example 80000 of 392702\n",
      "INFO:absl:Writing example 90000 of 392702\n",
      "INFO:absl:Writing example 100000 of 392702\n",
      "INFO:absl:Writing example 110000 of 392702\n",
      "INFO:absl:Writing example 120000 of 392702\n",
      "INFO:absl:Writing example 130000 of 392702\n",
      "INFO:absl:Writing example 140000 of 392702\n",
      "INFO:absl:Writing example 150000 of 392702\n",
      "INFO:absl:Writing example 160000 of 392702\n",
      "INFO:absl:Writing example 170000 of 392702\n",
      "INFO:absl:Writing example 180000 of 392702\n",
      "INFO:absl:Writing example 190000 of 392702\n",
      "INFO:absl:Writing example 200000 of 392702\n",
      "INFO:absl:Writing example 210000 of 392702\n",
      "INFO:absl:Writing example 220000 of 392702\n",
      "INFO:absl:Writing example 230000 of 392702\n",
      "INFO:absl:Writing example 240000 of 392702\n",
      "INFO:absl:Writing example 250000 of 392702\n",
      "INFO:absl:Writing example 260000 of 392702\n",
      "INFO:absl:Writing example 270000 of 392702\n",
      "INFO:absl:Writing example 280000 of 392702\n",
      "INFO:absl:Writing example 290000 of 392702\n",
      "INFO:absl:Writing example 300000 of 392702\n",
      "INFO:absl:Writing example 310000 of 392702\n",
      "INFO:absl:Writing example 320000 of 392702\n",
      "INFO:absl:Writing example 330000 of 392702\n",
      "INFO:absl:Writing example 340000 of 392702\n",
      "INFO:absl:Writing example 350000 of 392702\n",
      "INFO:absl:Writing example 360000 of 392702\n",
      "INFO:absl:Writing example 370000 of 392702\n",
      "INFO:absl:Writing example 380000 of 392702\n",
      "INFO:absl:Writing example 390000 of 392702\n",
      "INFO:absl:Writing example 0 of 9815\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-0\n",
      "INFO:absl:tokens: [CLS] yeah lots of people for the right life [SEP] uh - huh oh yeah all the people for right uh life or something [SEP]\n",
      "INFO:absl:input_ids: 101 3398 7167 1997 2111 2005 1996 2157 2166 102 7910 1011 9616 2821 3398 2035 1996 2111 2005 2157 7910 2166 2030 2242 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-1\n",
      "INFO:absl:tokens: [CLS] i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant . [SEP] also , i will be assuming that the 6 . 0 ##a cost of the postal service to take the mail from basic to works ##har ##ed condition is constant as limited quantities of mail move back and forth between basic and works ##har ##ed . [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 1012 102 2036 1010 1045 2097 2022 10262 2008 1996 1020 1012 1014 2050 3465 1997 1996 10690 2326 2000 2202 1996 5653 2013 3937 2000 2573 8167 2098 4650 2003 5377 2004 3132 12450 1997 5653 2693 2067 1998 5743 2090 3937 1998 2573 8167 2098 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-2\n",
      "INFO:absl:tokens: [CLS] it cost a lot of money to put forth all that effort . [SEP] taken up by the oh okay oh so you know well that ' s i had wondered sometimes i knew that there was a lot of a lot of effort and a lot of work went into a lot of that and i just wondered if if it lasted and if it took you know like yeah [SEP]\n",
      "INFO:absl:input_ids: 101 2009 3465 1037 2843 1997 2769 2000 2404 5743 2035 2008 3947 1012 102 2579 2039 2011 1996 2821 3100 2821 2061 2017 2113 2092 2008 1005 1055 1045 2018 4999 2823 1045 2354 2008 2045 2001 1037 2843 1997 1037 2843 1997 3947 1998 1037 2843 1997 2147 2253 2046 1037 2843 1997 2008 1998 1045 2074 4999 2065 2065 2009 6354 1998 2065 2009 2165 2017 2113 2066 3398 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 1 (id = 1)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-3\n",
      "INFO:absl:tokens: [CLS] i know nothing about the motor oil . [SEP] yeah i know the motor oil [SEP]\n",
      "INFO:absl:input_ids: 101 1045 2113 2498 2055 1996 5013 3514 1012 102 3398 1045 2113 1996 5013 3514 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 2 (id = 2)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n",
      "INFO:absl:*** Example ***\n",
      "INFO:absl:guid: dev-4\n",
      "INFO:absl:tokens: [CLS] many hotels in lake ##land have a cost - effective option of dinner , bed , and breakfast with an evening meal in . [SEP] many lake ##land hotels also quote a d , b and b ( dinner , bed , and breakfast ) rate , which includes the evening meal and is often quite cost - effective . [SEP]\n",
      "INFO:absl:input_ids: 101 2116 9275 1999 2697 3122 2031 1037 3465 1011 4621 5724 1997 4596 1010 2793 1010 1998 6350 2007 2019 3944 7954 1999 1012 102 2116 2697 3122 9275 2036 14686 1037 1040 1010 1038 1998 1038 1006 4596 1010 2793 1010 1998 6350 1007 3446 1010 2029 2950 1996 3944 7954 1998 2003 2411 3243 3465 1011 4621 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:absl:label: 0 (id = 0)\n",
      "INFO:absl:weight: None\n",
      "INFO:absl:example_id: None\n"
     ]
    }
   ],
   "source": [
    "generate_mnli_tfrecords(TRAIN_FILE, EVAL_FILE, METADATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://jk-rs-example/datasets/MNLI/\n",
      "gs://jk-rs-example/datasets/MNLI/metadata.json\n",
      "gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record\n",
      "gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls {OUTPUT_LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"processor_type\": \"TFDS_glue/mnli\",\n",
      "    \"train_data_size\": 392702,\n",
      "    \"max_seq_length\": 128,\n",
      "    \"task_type\": \"bert_classification\",\n",
      "    \"num_labels\": 3,\n",
      "    \"eval_data_size\": 9815\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! gsutil cat {METADATA_FILE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways you can train a custom model using a container image:\n",
    "\n",
    "- **Use a Google Cloud prebuilt container**. If you use a prebuilt container, you will additionally specify a Python package to install into the container image. \n",
    "\n",
    "- **Use your own custom container image**. If you use your own container, the container needs to contain your code for training a custom model.\n",
    "\n",
    "In this example, you use a custom container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training script\n",
    "\n",
    "Your training script is based on [the common training driver](https://github.com/tensorflow/models/blob/master/official/nlp/docs/train.md) from TensorFlow NLP Modelling Toolkit. The base driver has been adapted to work seamlessly on a distributed compute environment provisioned when running a Vertex training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf training_image\n",
    "! mkdir training_image\n",
    "! mkdir training_image/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing training_image/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/train.py\n",
    "\n",
    "# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"TFM common training driver.\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import gin\n",
    "\n",
    "\n",
    "from official.common import distribute_utils\n",
    "# pylint: disable=unused-import\n",
    "from official.common import registry_imports\n",
    "# pylint: enable=unused-import\n",
    "from official.common import flags as tfm_flags\n",
    "from official.core import task_factory\n",
    "from official.core import train_lib\n",
    "from official.core import train_utils\n",
    "from official.modeling import performance\n",
    "\n",
    "from tensorflow.dtypes import float16, bfloat16, float32\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def _get_model_dir(model_dir):\n",
    "  \"\"\"Adjusts model dir for multi-worker training.\n",
    "  \n",
    "     Checkpointing and Saving need to happen on each worker and they need to write \n",
    "     to different paths as they would override each others. This utility function\n",
    "     adjusts the base model dir passed as a flag using Vertex AI cluster topology\n",
    "  \"\"\"\n",
    "  \n",
    "  def _is_chief(task_type, task_id):\n",
    "    return ((task_type == 'chief' and task_id == 0) or task_type is None)\n",
    "  \n",
    "  tf_config = os.getenv('TF_CONFIG')\n",
    "  if tf_config:\n",
    "    tf_config = json.loads(tf_config)\n",
    "   \n",
    "    if not _is_chief(tf_config['task']['type'], tf_config['task']['index']):\n",
    "      model_dir = os.path.join(model_dir, 'worker-{}').format(tf_config['task']['index'])\n",
    "  \n",
    "  logging.info('Setting model_dir to: %s', model_dir)\n",
    "  \n",
    "  return model_dir\n",
    "\n",
    "def main(_):\n",
    "  \n",
    "  model_dir = _get_model_dir(FLAGS.model_dir)\n",
    "\n",
    "  gin.parse_config_files_and_bindings(FLAGS.gin_file, FLAGS.gin_params)\n",
    "  params = train_utils.parse_configuration(FLAGS)\n",
    "  \n",
    "  if 'train' in FLAGS.mode:\n",
    "    # Pure eval modes do not output yaml files. Otherwise continuous eval job\n",
    "    # may race against the train job for writing the same file.\n",
    "    train_utils.serialize_config(params, model_dir)\n",
    "\n",
    "  # Sets mixed_precision policy. Using 'mixed_float16' or 'mixed_bfloat16'\n",
    "  # can have significant impact on model speeds by utilizing float16 in case of\n",
    "  # GPUs, and bfloat16 in the case of TPUs. loss_scale takes effect only when\n",
    "  # dtype is float16\n",
    "  if params.runtime.mixed_precision_dtype:\n",
    "    performance.set_mixed_precision_policy(params.runtime.mixed_precision_dtype)\n",
    "  distribution_strategy = distribute_utils.get_distribution_strategy(\n",
    "      distribution_strategy=params.runtime.distribution_strategy,\n",
    "      all_reduce_alg=params.runtime.all_reduce_alg,\n",
    "      num_gpus=params.runtime.num_gpus,\n",
    "      tpu_address=params.runtime.tpu,\n",
    "      **params.runtime.model_parallelism())\n",
    "  with distribution_strategy.scope():\n",
    "    task = task_factory.get_task(params.task, logging_dir=model_dir)\n",
    "\n",
    "  train_lib.run_experiment(\n",
    "      distribution_strategy=distribution_strategy,\n",
    "      task=task,\n",
    "      mode=FLAGS.mode,\n",
    "      params=params,\n",
    "      model_dir=model_dir)\n",
    "\n",
    "  train_utils.save_gin_config(FLAGS.mode, model_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tfm_flags.define_flags()\n",
    "  app.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_image/trainer/glue_mnli_matched.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/trainer/glue_mnli_matched.yaml\n",
    "\n",
    "task:\n",
    "  hub_module_url: 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "  model:\n",
    "    num_classes: 3\n",
    "  init_checkpoint: ''\n",
    "  metric_type: 'accuracy'\n",
    "  train_data:\n",
    "    drop_remainder: true\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: true\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "  validation_data:\n",
    "    drop_remainder: false\n",
    "    global_batch_size: 32\n",
    "    input_path: ''\n",
    "    is_training: false\n",
    "    seq_length: 128\n",
    "    label_type: 'int'\n",
    "trainer:\n",
    "  checkpoint_interval: 3000\n",
    "  optimizer_config:\n",
    "    learning_rate:\n",
    "      polynomial:\n",
    "        # 100% of train_steps.\n",
    "        decay_steps: 36813\n",
    "        end_learning_rate: 0.0\n",
    "        initial_learning_rate: 3.0e-05\n",
    "        power: 1.0\n",
    "      type: polynomial\n",
    "    optimizer:\n",
    "      type: adamw\n",
    "    warmup:\n",
    "      polynomial:\n",
    "        power: 1\n",
    "        # ~10% of train_steps.\n",
    "        warmup_steps: 3681\n",
    "      type: polynomial\n",
    "  steps_per_loop: 1000\n",
    "  summary_interval: 1000\n",
    "  # Training data size 392,702 examples, 3 epochs.\n",
    "  train_steps: 36813\n",
    "  validation_interval: 6135\n",
    "  # Eval data size = 9815 examples.\n",
    "  validation_steps: 307\n",
    "  best_checkpoint_export_subdir: 'best_ckpt'\n",
    "  best_checkpoint_eval_metric: 'cls_accuracy'\n",
    "  best_checkpoint_metric_comp: 'higher'\n",
    "runtime:\n",
    "  distribution_strategy: 'multi_worker_mirrored'\n",
    "  all_reduce_alg: 'nccl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Dockerfile\n",
    "\n",
    "The custom training container image packages TensorFlow NLP Modelling Toolkit with the training script and the default configuration file created in the previous steps. It also install the Reduction Server NCCL plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_image/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_image/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
    "\n",
    "RUN apt remove -y google-fast-socket \\\n",
    "&&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list \\\n",
    "&&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - \\\n",
    "&&  apt update && apt install -y google-reduction-server\n",
    "\n",
    "RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "COPY trainer /trainer\n",
    "\n",
    "ENTRYPOINT [\"python\"]\n",
    "CMD [\"-c\", \"print('TF Model Garden')\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the container image and push it to Container Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/mnli_finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  9.216kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 0f998c784cd6\n",
      "Step 2/7 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Using cache\n",
      " ---> a21aae0ed16e\n",
      "Step 3/7 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Using cache\n",
      " ---> 6f47cd244768\n",
      "Step 4/7 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 8d414b878cff\n",
      "Step 5/7 : COPY trainer /trainer\n",
      " ---> 7431fc13425d\n",
      "Step 6/7 : ENTRYPOINT [\"python\"]\n",
      " ---> Running in 64f92b8789b3\n",
      "Removing intermediate container 64f92b8789b3\n",
      " ---> 8e0194e70d8b\n",
      "Step 7/7 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Running in 2645230aaeef\n",
      "Removing intermediate container 2645230aaeef\n",
      " ---> a0e0cb9de981\n",
      "Successfully built a0e0cb9de981\n",
      "Successfully tagged gcr.io/jk-mlops-dev/mnli_finetuning:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} training_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/mnli_finetuning]\n",
      "\n",
      "\u001b[1B6291a417: Preparing \n",
      "\u001b[1Be680029a: Preparing \n",
      "\u001b[1B0cf1801e: Preparing \n",
      "\u001b[1B40ebdbd6: Preparing \n",
      "\u001b[1B54d2bd94: Preparing \n",
      "\u001b[1Ba056d495: Preparing \n",
      "\u001b[1Bcb8c2687: Preparing \n",
      "\u001b[1B43de6bca: Preparing \n",
      "\u001b[1B87a4088d: Preparing \n",
      "\u001b[1B522d97b4: Preparing \n",
      "\u001b[1B519f0898: Preparing \n",
      "\u001b[1B6aeeabc0: Preparing \n",
      "\u001b[1Bebfdebb3: Preparing \n",
      "\u001b[1B6b863e43: Preparing \n",
      "\u001b[1B43fb2f7a: Preparing \n",
      "\u001b[1Ba9d68143: Preparing \n",
      "\u001b[1B84ac5c5d: Preparing \n",
      "\u001b[1Be1798c0c: Preparing \n",
      "\u001b[1B9de06c8b: Preparing \n",
      "\u001b[1B59f353b4: Preparing \n",
      "\u001b[1B3c11e857: Preparing \n",
      "\u001b[1B5264beff: Preparing \n",
      "\u001b[1B14beba01: Preparing \n",
      "\u001b[1Bf85bc8aa: Preparing \n",
      "\u001b[1Bdd6c9734: Preparing \n",
      "\u001b[1Ba732d388: Preparing \n",
      "\u001b[1B78e3bf48: Preparing \n",
      "\u001b[23B056d495: Waiting g \n",
      "\u001b[1B87e0621d: Preparing \n",
      "\u001b[1B7ad6008c: Preparing \n",
      "\u001b[17B3fb2f7a: Waiting g \n",
      "\u001b[1B872b888e: Preparing \n",
      "\u001b[26B3de6bca: Waiting g \n",
      "\u001b[1B31fc0e08: Preparing \n",
      "\u001b[35B291a417: Pushed lready exists 8kB\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[30A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[20A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[1A\u001b[2K\u001b[35A\u001b[2Klatest: digest: sha256:fb566135116923d196628b8ae7de5682c08f2b4cc21bd9c6e48d789a2383c15a size: 7673\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare your custom job specification\n",
    "\n",
    "You can now create a Job Specification for your distributed training job with Reduction Server. \n",
    "\n",
    "If you run a distributed training job with Vertex AI, you specify multiple machines (nodes) in a training cluster. The training service allocates the resources for the machine types you specify. Your running job on a given node is called a replica. A group of replicas with the same configuration is called a worker pool. Vertex AI provides 4 worker pools to cover the different types of machine tasks.\n",
    "\n",
    "Worker pool 0 configures the Primary, chief, scheduler, or \"master\".  This worker generally takes on some extra work such as saving checkpoints and writing summary files. There is only ever one chief worker in a cluster, so your worker count for worker pool 0 will always be 1.\n",
    "\n",
    "Worker pool 1 is where you configure the rest of the workers for your cluster. \n",
    " \n",
    "Worker pool 2 manages Reduction Server reducers. \n",
    "\n",
    "Worker pools 0 and 1 run your custom training container you created in the previous step. Worker pool 2 uses the Reduction Server image provided by Vertex AI.\n",
    "\n",
    "The below helper function creates a custom job specification using the described worker pool topology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure worker pools\n",
    "\n",
    "Adjust the following constants to reflect the machine types and a number of replicas for your worker pools. \n",
    "\n",
    "When choosing the number and type of reducers, you should consider the network bandwidth supported by a reducer replicas machine type. In GCP, a VMs machine type defines its maximum possible egress bandwidth. For example, the egress bandwidth of the n1-highcpu-16 machine type is limited at 32 Gbps.\n",
    "\n",
    "Because reducers perform a very limited function, aggregating blocks of gradients, they can run on relatively low-powered and cost effective machines. Even with a large number of gradients this computation does not require accelerated hardware or high CPU or memory resources. However, to avoid network bottlenecks, the total aggregate bandwidth of all replicas in the reducer worker pool must be greater or equal to the total aggregate bandwidth of all replicas in worker pools 0 and 1, which host the GPU workers.\n",
    "\n",
    "Refer to [the article](TBD) for more information about configuring worker pools when using Reduction Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICA_COUNT = 4\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "PER_MACHINE_ACCELERATOR_COUNT = 1\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 8\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the MNLI experiment settings\n",
    "\n",
    "As noted before, the training script supports overriding the default configuration of a TensorFlow Modelling Toolkit task using YAML configuration files and command line parameters. The base configuration for the MNLI fine tuning task is defined in the YAML file packaged into the training container. With each training run you can override selected parameters by using the `params_override` command line argument of the training script.\n",
    "\n",
    "The `params_override` argument accepts a string with comma separated key/value pairs for each parameter to be overwritten.\n",
    "\n",
    "The following parameters are overwritten in the following cell.\n",
    "- `trainer.train_step` - A number of training steps. Recall that there is 392,702 examples in the training data set. \n",
    "- `trainer.steps_per_loop` - The training script prints out updates about training progress every `steps_per_loop`\n",
    "- `trainer.summary_interval` - The training script logs Tensorboard summaries every `summary_interval`\n",
    "- `trainer.validation_interval` - The training script runs validation every `validation_interval`\n",
    "- `trainer.checkpoint_interval` - The training script creates a checkpoint every `checkpoint_interval`\n",
    "- `task.train_data.global_batch_size` - A global batch size for training data. This value should be adjusted based on a GPU type and a number of GPU workers. For example when using NVidia V100 GPUs a batch size of 16 per GPU is a good starting point. With 2 workers, the `global_batch_size` would be 32\n",
    "- `task.validation_data.global_batch_size` - A global batch size for validation data\n",
    "- `task.train_data.input_path` - A location of the training dataset\n",
    "- `task.validation_data.input_path` - A location of the validation dataset\n",
    "- `runtime.num_gpus` - A number of GPUs to use on each worker. This should be adjusted based on the type of a worker machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'trainer.train_steps=2000',\n",
    "    'trainer.steps_per_loop=100',\n",
    "    'trainer.summary_interval=100',\n",
    "    'trainer.validation_interval=2000',\n",
    "    'trainer.checkpoint_interval=2000',\n",
    "    'task.train_data.global_batch_size=' + str(REPLICA_COUNT*PER_REPLICA_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(REPLICA_COUNT*PER_REPLICA_BATCH_SIZE), \n",
    "    'task.train_data.input_path=' + TRAIN_FILE,\n",
    "    'task.validation_data.input_path=' + EVAL_FILE,\n",
    "    'runtime.num_gpus=' + str(PER_MACHINE_ACCELERATOR_COUNT),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble a job specification\n",
    "\n",
    "You are now ready to assemble a custom job spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210709_020108',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/mnli_finetuning'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 3},\n",
      "                                    {'container_spec': {'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'},\n",
      "                                     'machine_spec': {'machine_type': 'n1-highcpu-16'},\n",
      "                                     'replica_count': 8}]}}\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{BUCKET_NAME}/{JOB_NAME}/model'\n",
    "\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=bert/sentence_prediction',\n",
    "    '--mode=train',\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=trainer/glue_mnli_matched.yaml',\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=PER_MACHINE_ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and monitor the job\n",
    "\n",
    "You will now use the Vertex AI job client to submit and monitor a training job. To submit the job, use the job client service's `create_custom_job` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/895222332033/locations/us-central1/customJobs/6696065395574439936\"\n",
       "display_name: \"MNLI_20210709_020108\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 3\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-highcpu-16\"\n",
       "    }\n",
       "    replica_count: 8\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1625796124\n",
       "  nanos: 166662000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1625796124\n",
       "  nanos: 166662000\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = JobServiceClient(client_options=options)\n",
    "\n",
    "parent = f\"projects/{PROJECT_ID}/locations/{REGION}\"\n",
    "\n",
    "response = client.create_custom_job(\n",
    "    parent=parent, custom_job=custom_job_spec\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get information about a running job\n",
    "\n",
    "You can use the job client service's `get_custom_job` method to retrieve information about a running job.\n",
    "Note that you can also monitor the job using [GCP Console]().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/895222332033/locations/us-central1/customJobs/6696065395574439936\"\n",
       "display_name: \"MNLI_20210709_020108\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 3\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/mnli_finetuning\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-rs-example/MNLI_20210709_020108/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000,task.train_data.global_batch_size=128,task.validation_data.global_batch_size=128,task.train_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record,task.validation_data.input_path=gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record,runtime.num_gpus=1\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"n1-highcpu-16\"\n",
       "    }\n",
       "    replica_count: 8\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1625796124\n",
       "  nanos: 166662000\n",
       "}\n",
       "start_time {\n",
       "  seconds: 1625796124\n",
       "  nanos: 348958000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1625796129\n",
       "  nanos: 744598000\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_custom_job(name=response.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sx_vKniMq9ZX"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Delete model resource\n",
    "! gcloud ai models delete $MODEL_NAME --quiet\n",
    "\n",
    "# Delete Cloud Storage objects that were created\n",
    "! gsutil -m rm -r $JOB_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary tests while debugging mixed precision - To be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'bert/sentence_prediction'\n",
    "CONFIG_FILE = 'trainer/glue_mnli_matched.yaml'\n",
    "MODE = 'train'\n",
    "\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 16\n",
    "ACCELERATOR_COUNT = 2\n",
    "ALL_REDUCE_ALG = 'nccl'\n",
    "STRATEGY = 'mirrored'\n",
    "GLOBAL_BATCH_SIZE = ACCELERATOR_COUNT * PER_REPLICA_BATCH_SIZE\n",
    "\n",
    "TRAINING_STEPS = 200\n",
    "STEPS_PER_LOOP = 50\n",
    "SUMMARY_INTERVAL = 50\n",
    "VALIDATION_INTERVAL = 200\n",
    "CHECKPOINT_INTERVAL = 200\n",
    "\n",
    "MIXED_PRECISION_TYPE = 'float16'\n",
    "\n",
    "LOCAL_DIR = '/tmp'\n",
    "\n",
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'task.train_data.input_path=' + TRAIN_FILE,\n",
    "    'task.validation_data.input_path=' + EVAL_FILE,\n",
    "    'task.train_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(ACCELERATOR_COUNT),\n",
    "#    'runtime.distribution_strategy=' + STRATEGY,\n",
    "#    'runtime.all_reduce_alg=' + ALL_REDUCE_ALG,\n",
    "#    'runtime.mixed_precision_dtype=' + MIXED_PRECISION_TYPE,\n",
    "    'trainer.train_steps=' + str(TRAINING_STEPS),\n",
    "    'trainer.steps_per_loop=' + str(STEPS_PER_LOOP),\n",
    "    'trainer.summary_interval=' + str(SUMMARY_INTERVAL),\n",
    "    'trainer.validation_interval=' + str(VALIDATION_INTERVAL),\n",
    "    'trainer.checkpoint_interval=' + str(CHECKPOINT_INTERVAL),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-09 01:52:45.055095: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "I0709 01:52:49.571075 139962586683200 train.py:59] Setting model_dir to: /tmp/test\n",
      "I0709 01:52:49.590986 139962586683200 train_utils.py:286] Final experiment parameters: {'runtime': {'all_reduce_alg': 'nccl',\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'multi_worker_mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 2,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'hub_module_url': 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
      "          'init_checkpoint': '',\n",
      "          'init_cls_pooler': False,\n",
      "          'metric_type': 'accuracy',\n",
      "          'model': {'encoder': {'bert': {'attention_dropout_rate': 0.1,\n",
      "                                         'dropout_rate': 0.1,\n",
      "                                         'embedding_size': None,\n",
      "                                         'hidden_activation': 'gelu',\n",
      "                                         'hidden_size': 768,\n",
      "                                         'initializer_range': 0.02,\n",
      "                                         'intermediate_size': 3072,\n",
      "                                         'max_position_embeddings': 512,\n",
      "                                         'num_attention_heads': 12,\n",
      "                                         'num_layers': 12,\n",
      "                                         'output_range': None,\n",
      "                                         'return_all_encoder_outputs': False,\n",
      "                                         'type_vocab_size': 2,\n",
      "                                         'vocab_size': 30522},\n",
      "                                'type': 'bert'},\n",
      "                    'num_classes': 3,\n",
      "                    'use_encoder_pooler': False},\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'global_batch_size': 32,\n",
      "                         'include_example_id': False,\n",
      "                         'input_path': 'gs://jk-rs-example/datasets/MNLI/mnli_train.tf_record',\n",
      "                         'is_training': True,\n",
      "                         'label_type': 'int',\n",
      "                         'seed': None,\n",
      "                         'seq_length': 128,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 100,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': False,\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'global_batch_size': 32,\n",
      "                              'include_example_id': False,\n",
      "                              'input_path': 'gs://jk-rs-example/datasets/MNLI/mnli_valid.tf_record',\n",
      "                              'is_training': False,\n",
      "                              'label_type': 'int',\n",
      "                              'seed': None,\n",
      "                              'seq_length': 128,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 100,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': 'cls_accuracy',\n",
      "             'best_checkpoint_export_subdir': 'best_ckpt',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 200,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'polynomial': {'cycle': False,\n",
      "                                                                   'decay_steps': 36813,\n",
      "                                                                   'end_learning_rate': 0.0,\n",
      "                                                                   'initial_learning_rate': 3e-05,\n",
      "                                                                   'name': 'PolynomialDecay',\n",
      "                                                                   'power': 1.0},\n",
      "                                                    'type': 'polynomial'},\n",
      "                                  'optimizer': {'adamw': {'amsgrad': False,\n",
      "                                                          'beta_1': 0.9,\n",
      "                                                          'beta_2': 0.999,\n",
      "                                                          'clipnorm': None,\n",
      "                                                          'clipvalue': None,\n",
      "                                                          'epsilon': 1e-07,\n",
      "                                                          'exclude_from_weight_decay': ['LayerNorm',\n",
      "                                                                                        'layer_norm',\n",
      "                                                                                        'bias'],\n",
      "                                                          'global_clipnorm': None,\n",
      "                                                          'gradient_clip_norm': 1.0,\n",
      "                                                          'include_in_weight_decay': None,\n",
      "                                                          'name': 'AdamWeightDecay',\n",
      "                                                          'weight_decay_rate': 0.01},\n",
      "                                                'type': 'adamw'},\n",
      "                                  'warmup': {'polynomial': {'name': 'polynomial',\n",
      "                                                            'power': 1,\n",
      "                                                            'warmup_steps': 3681},\n",
      "                                             'type': 'polynomial'}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 50,\n",
      "             'summary_interval': 50,\n",
      "             'train_steps': 200,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 200,\n",
      "             'validation_steps': 307}}\n",
      "I0709 01:52:49.591234 139962586683200 train_utils.py:295] Saving experiment configuration to /tmp/test/params.yaml\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/official/common/distribute_utils.py:153: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "W0709 01:52:49.604674 139962586683200 deprecation.py:336] From /opt/conda/lib/python3.7/site-packages/official/common/distribute_utils.py:153: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "use distribute.MultiWorkerMirroredStrategy instead\n",
      "2021-07-09 01:52:49.606199: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-07-09 01:52:49.619870: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.620632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-07-09 01:52:49.620841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.621573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-07-09 01:52:49.621626: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-09 01:52:49.624787: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-09 01:52:49.624870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-09 01:52:49.626303: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-07-09 01:52:49.626763: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-07-09 01:52:49.630006: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-07-09 01:52:49.630840: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-07-09 01:52:49.631141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-09 01:52:49.631324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.632442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.633483: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.634447: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.635169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-07-09 01:52:49.635679: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-07-09 01:52:49.849701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.850545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-07-09 01:52:49.850819: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.851521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-07-09 01:52:49.851679: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.852455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.853306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.854139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:49.854903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-07-09 01:52:49.854977: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-07-09 01:52:50.590241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-07-09 01:52:50.590318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2021-07-09 01:52:50.590331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2021-07-09 01:52:50.590339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2021-07-09 01:52:50.590704: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.591838: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.592852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.593815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.594780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.595470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "2021-07-09 01:52:50.596442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-07-09 01:52:50.597211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13837 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.NCCL\n",
      "I0709 01:52:50.846944 139962586683200 collective_all_reduce_strategy.py:408] Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0', '/device:GPU:1'), communication = CommunicationImplementation.NCCL\n",
      "I0709 01:52:50.848467 139962586683200 train_utils.py:96] Created the best checkpoint exporter. data_dir: /tmp/test, export_subdir: best_ckpt, metric_name: cls_accuracy\n",
      "I0709 01:52:50.848661 139962586683200 train_utils.py:214] Running default trainer.\n",
      "I0709 01:52:50.871978 139962586683200 resolver.py:106] Using /tmp/tfhub_modules to cache modules.\n",
      "I0709 01:52:50.873061 139962586683200 resolver.py:416] Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'.\n",
      "I0709 01:53:04.730916 139962586683200 resolver.py:154] Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4, Total size: 1.27GB\n",
      "I0709 01:53:04.731355 139962586683200 resolver.py:431] Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'.\n",
      "I0709 01:53:25.584743 139962586683200 optimization.py:139] gradient_clip_norm=1.000000\n",
      "I0709 01:53:26.202215 139962586683200 controller.py:362] restoring or initializing model...\n",
      "restoring or initializing model...\n",
      "I0709 01:53:26.202471 139962586683200 controller.py:368] initialized model.\n",
      "initialized model.\n",
      "I0709 01:53:26.202568 139962586683200 train_lib.py:96] Starts to execute mode: train\n",
      "I0709 01:53:26.203410 139962586683200 controller.py:211] train | step:      0 | training until step 200...\n",
      "train | step:      0 | training until step 200...\n",
      "2021-07-09 01:53:26.218177: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-09 01:53:26.218965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
      "INFO:tensorflow:Collective all_reduce tensors: 394 all_reduces, num_devices = 2, group_size = 2, implementation = NCCL, num_packs = 1\n",
      "I0709 01:53:44.408415 139962586683200 cross_device_ops.py:1155] Collective all_reduce tensors: 394 all_reduces, num_devices = 2, group_size = 2, implementation = NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce IndexedSlices: 394 all_reduces, num_devices =2, group_size = 2, implementation = NCCL\n",
      "I0709 01:53:45.332747 139962586683200 cross_device_ops.py:1170] Collective all_reduce IndexedSlices: 394 all_reduces, num_devices =2, group_size = 2, implementation = NCCL\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"PartitionedCall:1\", shape=(None,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0), values=Tensor(\"clip_by_global_norm/clip_by_global_norm/_0:0\", shape=(None, 1024), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0), dense_shape=Tensor(\"PartitionedCall:2\", shape=(2,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:0))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"replica_1/PartitionedCall:1\", shape=(None,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1), values=Tensor(\"replica_1/clip_by_global_norm/replica_1/clip_by_global_norm/_0:0\", shape=(None, 1024), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1), dense_shape=Tensor(\"replica_1/PartitionedCall:2\", shape=(2,), dtype=int32, device=/job:localhost/replica:0/task:0/device:GPU:1))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"shape. This may consume a large amount of memory.\" % value)\n",
      "INFO:tensorflow:Collective all_reduce tensors: 394 all_reduces, num_devices = 2, group_size = 2, implementation = NCCL, num_packs = 1\n",
      "I0709 01:53:54.680030 139962586683200 cross_device_ops.py:1155] Collective all_reduce tensors: 394 all_reduces, num_devices = 2, group_size = 2, implementation = NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce IndexedSlices: 394 all_reduces, num_devices =2, group_size = 2, implementation = NCCL\n",
      "I0709 01:53:55.596093 139962586683200 cross_device_ops.py:1170] Collective all_reduce IndexedSlices: 394 all_reduces, num_devices =2, group_size = 2, implementation = NCCL\n",
      "2021-07-09 01:54:55.560438: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-09 01:54:56.204567: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "I0709 01:56:14.275024 139962586683200 controller.py:425] train | step:     50 | steps/sec:    0.3 | output: \n",
      "    {'cls_accuracy': 0.35875,\n",
      "     'learning_rate': 3.6675146e-07,\n",
      "     'training_loss': 1.1041107}\n",
      "train | step:     50 | steps/sec:    0.3 | output: \n",
      "    {'cls_accuracy': 0.35875,\n",
      "     'learning_rate': 3.6675146e-07,\n",
      "     'training_loss': 1.1041107}\n",
      "I0709 01:56:26.041630 139962586683200 controller.py:454] saved checkpoint to /tmp/test/ckpt-50.\n",
      "saved checkpoint to /tmp/test/ckpt-50.\n",
      "I0709 01:57:47.491600 139962586683200 controller.py:425] train | step:    100 | steps/sec:    0.5 | output: \n",
      "    {'cls_accuracy': 0.315,\n",
      "     'learning_rate': 7.3350293e-07,\n",
      "     'training_loss': 1.1140316}\n",
      "train | step:    100 | steps/sec:    0.5 | output: \n",
      "    {'cls_accuracy': 0.315,\n",
      "     'learning_rate': 7.3350293e-07,\n",
      "     'training_loss': 1.1140316}\n",
      "I0709 01:59:08.904117 139962586683200 controller.py:425] train | step:    150 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.330625,\n",
      "     'learning_rate': 1.1002544e-06,\n",
      "     'training_loss': 1.1032954}\n",
      "train | step:    150 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.330625,\n",
      "     'learning_rate': 1.1002544e-06,\n",
      "     'training_loss': 1.1032954}\n",
      "I0709 02:00:31.087746 139962586683200 controller.py:425] train | step:    200 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.36,\n",
      "     'learning_rate': 1.4670059e-06,\n",
      "     'training_loss': 1.0930978}\n",
      "train | step:    200 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.36,\n",
      "     'learning_rate': 1.4670059e-06,\n",
      "     'training_loss': 1.0930978}\n",
      "I0709 02:00:43.378129 139962586683200 controller.py:454] saved checkpoint to /tmp/test/ckpt-200.\n",
      "saved checkpoint to /tmp/test/ckpt-200.\n",
      "I0709 02:00:43.387368 139962586683200 train_lib.py:123] Number of trainable params in model: 336.194564 Millions.\n",
      "I0709 02:00:43.388063 139962586683200 train_utils.py:304] Saving gin configurations to /tmp/test/operative_config.train.gin\n"
     ]
    }
   ],
   "source": [
    "! docker run -it --rm --gpus all {TRAIN_IMAGE} trainer/train.py \\\n",
    "--experiment={EXPERIMENT} \\\n",
    "--mode={MODE} \\\n",
    "--model_dir={LOCAL_DIR}/test \\\n",
    "--config_file={CONFIG_FILE}\\\n",
    "--params_override={PARAMS_OVERRIDE}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
