{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Vertex Reduction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
    "    JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "GCS_BUCKET = f'gs://jk-staging-{REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = !gsutil ls {GCS_BUCKET}\n",
    "if objects:\n",
    "    if 'BucketNotFoundException' in objects[0]:\n",
    "        print('Creating a new bucket')\n",
    "        !gsutil mb -l {REGION} {GCS_BUCKET} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'{GCS_BUCKET}/vertex'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/model_garden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} model_garden_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TFRecords files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'{GCS_BUCKET}/datasets'\n",
    "BERT_DIR = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16'\n",
    "TASK = 'MNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-8'\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAIN_IMAGE,\n",
    "            'command': ['python', 'dataprep/create_finetuning_data.py'],\n",
    "            'args': [\n",
    "                '--fine_tuning_task_type=classification',\n",
    "                '--tfds_params=dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched',\n",
    "                '--max_seq_length=128',\n",
    "                f'--vocab_file={BERT_DIR}/vocab.txt',\n",
    "                f'--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data',\n",
    "                f'--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record',\n",
    "                f'--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-central1/customJobs/859171580083699712\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-central1/customJobs/859171580083699712')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/859171580083699712?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/859171580083699712 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/895222332033/locations/us-central1/customJobs/859171580083699712\n"
     ]
    }
   ],
   "source": [
    "job_name = \"PREPARE_DATA_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a fine tuning job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the MNLI fine tuning experiment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'bert/sentence_prediction'\n",
    "CONFIG_FILE = 'trainer/glue_mnli_matched.yaml'\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_TRAIN_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record'\n",
    "MNLI_VALID_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "REPLICA_COUNT = 8\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "GLOBAL_BATCH_SIZE = REPLICA_COUNT * PER_REPLICA_BATCH_SIZE\n",
    "\n",
    "ACCELERATOR_COUNT = 1\n",
    "ALL_REDUCE_ALG = 'nccl'\n",
    "STRATEGY = 'multi_worker_mirrored'\n",
    "\n",
    "TRAINING_STEPS = 2000\n",
    "STEPS_PER_LOOP = 100\n",
    "SUMMARY_INTERVAL = 100\n",
    "VALIDATION_INTERVAL = 2000\n",
    "CHECKPOINT_INTERVAL = 2000\n",
    "\n",
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.train_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(ACCELERATOR_COUNT),\n",
    "    'runtime.distribution_strategy=' + STRATEGY,\n",
    "    'runtime.all_reduce_alg=' + ALL_REDUCE_ALG,\n",
    " #   'runtime.mixed_precision_dtype=' + mixed_precision_dtype,\n",
    "    'trainer.train_steps=' + str(TRAINING_STEPS),\n",
    "    'trainer.steps_per_loop=' + str(STEPS_PER_LOOP),\n",
    "    'trainer.summary_interval=' + str(SUMMARY_INTERVAL),\n",
    "    'trainer.validation_interval=' + str(VALIDATION_INTERVAL),\n",
    "    'trainer.checkpoint_interval=' + str(CHECKPOINT_INTERVAL),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex training custom job spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{GCS_BUCKET}/{JOB_NAME}/model'\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "ACCLERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=' + EXPERIMENT,\n",
    "    '--mode=' + MODE,\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=' + CONFIG_FILE,\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210621_221941',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210621_221941/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210621_221941/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 7}]}}\n"
     ]
    }
   ],
   "source": [
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = JobServiceClient(client_options=options)\n",
    "\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "\n",
    "response = client.create_custom_job(\n",
    "    parent=parent, custom_job=custom_job_spec\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_custom_job(name=response.name).state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload logs to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TENSORBOARD={}'.format('projects/895222332033/locations/us-central1/tensorboards/5983067289333792768'))\n",
    "print('LOGDIR={}'.format(model_dir))\n",
    "print('EXPERIMENT={}'.format(job_name))\n",
    "print('./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the container image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_TRAIN_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record'\n",
    "MNLI_VALID_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "output_dir = '/tmp'\n",
    "num_gpus = 2 \n",
    "strategy = 'mirrored'\n",
    "#strategy = 'multi_worker_mirrored'\n",
    "\n",
    "params_override = [\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(num_gpus),\n",
    "    'runtime.distribution_strategy=' + strategy,\n",
    "    'runtime.mixed_precision_dtype=' + 'tf.float16',\n",
    "]\n",
    "\n",
    "params = ','.join(params_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all {TRAIN_IMAGE} trainer/train.py \\\n",
    "--experiment=bert/sentence_prediction \\\n",
    "--mode=train_and_eval \\\n",
    "--model_dir={output_dir}/test \\\n",
    "--config_file=trainer/glue_mnli_matched.yaml \\\n",
    "--params_override={params}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud.aiplatform.gapic import \\\n",
    "#    JobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_BUCKET = f'gs://jk-vertex-{REGION}'\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = job_name\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "--volume {MNLI_LOCAL_FOLDER}/MNLI:/data/MNLI \\\n",
    "--env OUTPUT_DIR={OUTPUT_DIR} \\\n",
    "--env TASK=MNLI \\\n",
    "--env BERT_DIR={BERT_DIR} \\\n",
    "{TRAIN_IMAGE} \\\n",
    "'python models/official/nlp/data/create_finetuning_data.py \\\n",
    " --input_data_dir=/data/MNLI \\\n",
    " --vocab_file=${BERT_DIR}/vocab.txt \\\n",
    " --train_data_output_path=${OUTPUT_DIR}/${TASK}/${TASK}_train.tf_record \\\n",
    " --eval_data_output_path=${OUTPUT_DIR}/${TASK}/${TASK}_eval.tf_record \\\n",
    " --meta_data_file_path=${OUTPUT_DIR}/${TASK}/${TASK}_meta_data \\\n",
    " --fine_tuning_task_type=classification --max_seq_length=128 \\\n",
    " --classification_task_name=${TASK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "{TRAIN_IMAGE} \\\n",
    "dataprep/create_finetuning_data.py \\\n",
    "--fine_tuning_task_type=classification \\\n",
    "--tfds_params=\"dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched\" \\\n",
    "--max_seq_length=128 \\\n",
    "--vocab_file={BERT_DIR}/vocab.txt \\\n",
    "--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data \\\n",
    "--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record \\\n",
    "--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "{TRAIN_IMAGE} \\\n",
    "dataprep/create_finetuning_data.py \\\n",
    "--fine_tuning_task_type=classification \\\n",
    "--tfds_params=\"dataset=glue/mnli,text_key=hypothesis,text_b_key=premise\" \\\n",
    "--max_seq_length=128 \\\n",
    "--vocab_file={BERT_DIR}/vocab.txt \\\n",
    "--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data \\\n",
    "--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record \\\n",
    "--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "file_extension": ".py",
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
