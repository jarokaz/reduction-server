{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Vertex Reduction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
    "    JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "GCS_BUCKET = f'gs://jk-staging-{REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = !gsutil ls {GCS_BUCKET}\n",
    "if objects:\n",
    "    if 'BucketNotFoundException' in objects[0]:\n",
    "        print('Creating a new bucket')\n",
    "        !gsutil mb -l {REGION} {GCS_BUCKET} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'{GCS_BUCKET}/vertex'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/model_garden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 27.7 KiB before compression.\n",
      "Uploading tarball of [model_garden_image] to [gs://jk-mlops-dev_cloudbuild/source/1624314140.348313-aca78fcba9064d28ac28ad8cd52e534e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/locations/global/builds/da1dfe86-1c10-4170-b76a-852c5f39cbe4].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/da1dfe86-1c10-4170-b76a-852c5f39cbe4?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"da1dfe86-1c10-4170-b76a-852c5f39cbe4\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1624314140.348313-aca78fcba9064d28ac28ad8cd52e534e.tgz#1624314140684650\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1624314140.348313-aca78fcba9064d28ac28ad8cd52e534e.tgz#1624314140684650...\n",
      "/ [1 files][  6.0 KiB/  6.0 KiB]                                                \n",
      "Operation completed over 1 objects/6.0 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   38.4kB\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-5\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "e1948de7822d: Pulling fs layer\n",
      "9c8e4ac0274d: Pulling fs layer\n",
      "29dcb86d08de: Pulling fs layer\n",
      "70d8019dd5e8: Pulling fs layer\n",
      "5ccc73e465ef: Pulling fs layer\n",
      "e9d8630baacd: Pulling fs layer\n",
      "82ad86c72f57: Pulling fs layer\n",
      "a6746fce16a1: Pulling fs layer\n",
      "7a67c64e20d2: Pulling fs layer\n",
      "334312f3cce5: Pulling fs layer\n",
      "d1c06a369ea4: Pulling fs layer\n",
      "61aa2ea143ea: Pulling fs layer\n",
      "87555fa6733c: Pulling fs layer\n",
      "dcfc4adad992: Pulling fs layer\n",
      "dcb1b56a4779: Pulling fs layer\n",
      "95837e5e0af5: Pulling fs layer\n",
      "9fe1d9bf08cb: Pulling fs layer\n",
      "1cc20834967b: Pulling fs layer\n",
      "f492fb29e345: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "4072293fd93e: Pulling fs layer\n",
      "27d78ef0086e: Pulling fs layer\n",
      "c733e8557bb1: Pulling fs layer\n",
      "df0cdf11e45e: Pulling fs layer\n",
      "3327ae24303f: Pulling fs layer\n",
      "3efe73011a96: Pulling fs layer\n",
      "e1948de7822d: Waiting\n",
      "9c8e4ac0274d: Waiting\n",
      "29dcb86d08de: Waiting\n",
      "70d8019dd5e8: Waiting\n",
      "5ccc73e465ef: Waiting\n",
      "e9d8630baacd: Waiting\n",
      "82ad86c72f57: Waiting\n",
      "a6746fce16a1: Waiting\n",
      "7a67c64e20d2: Waiting\n",
      "334312f3cce5: Waiting\n",
      "d1c06a369ea4: Waiting\n",
      "61aa2ea143ea: Waiting\n",
      "87555fa6733c: Waiting\n",
      "dcfc4adad992: Waiting\n",
      "dcb1b56a4779: Waiting\n",
      "95837e5e0af5: Waiting\n",
      "9fe1d9bf08cb: Waiting\n",
      "1cc20834967b: Waiting\n",
      "f492fb29e345: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "4072293fd93e: Waiting\n",
      "27d78ef0086e: Waiting\n",
      "c733e8557bb1: Waiting\n",
      "df0cdf11e45e: Waiting\n",
      "3327ae24303f: Waiting\n",
      "3efe73011a96: Waiting\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "e1948de7822d: Verifying Checksum\n",
      "e1948de7822d: Download complete\n",
      "9c8e4ac0274d: Verifying Checksum\n",
      "9c8e4ac0274d: Download complete\n",
      "70d8019dd5e8: Verifying Checksum\n",
      "70d8019dd5e8: Download complete\n",
      "29dcb86d08de: Verifying Checksum\n",
      "29dcb86d08de: Download complete\n",
      "e9d8630baacd: Verifying Checksum\n",
      "e9d8630baacd: Download complete\n",
      "4bbfd2c87b75: Verifying Checksum\n",
      "4bbfd2c87b75: Download complete\n",
      "a6746fce16a1: Verifying Checksum\n",
      "a6746fce16a1: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "e1948de7822d: Pull complete\n",
      "82ad86c72f57: Verifying Checksum\n",
      "82ad86c72f57: Download complete\n",
      "334312f3cce5: Verifying Checksum\n",
      "334312f3cce5: Download complete\n",
      "9c8e4ac0274d: Pull complete\n",
      "5ccc73e465ef: Verifying Checksum\n",
      "5ccc73e465ef: Download complete\n",
      "7a67c64e20d2: Verifying Checksum\n",
      "7a67c64e20d2: Download complete\n",
      "87555fa6733c: Verifying Checksum\n",
      "87555fa6733c: Download complete\n",
      "d1c06a369ea4: Verifying Checksum\n",
      "d1c06a369ea4: Download complete\n",
      "29dcb86d08de: Pull complete\n",
      "dcfc4adad992: Verifying Checksum\n",
      "dcfc4adad992: Download complete\n",
      "61aa2ea143ea: Verifying Checksum\n",
      "61aa2ea143ea: Download complete\n",
      "95837e5e0af5: Verifying Checksum\n",
      "95837e5e0af5: Download complete\n",
      "9fe1d9bf08cb: Verifying Checksum\n",
      "9fe1d9bf08cb: Download complete\n",
      "f492fb29e345: Download complete\n",
      "1cc20834967b: Verifying Checksum\n",
      "1cc20834967b: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "94e3f0ba3fb3: Verifying Checksum\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Verifying Checksum\n",
      "00f212e657e4: Download complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "dcb1b56a4779: Verifying Checksum\n",
      "dcb1b56a4779: Download complete\n",
      "4072293fd93e: Verifying Checksum\n",
      "4072293fd93e: Download complete\n",
      "70d8019dd5e8: Pull complete\n",
      "c733e8557bb1: Download complete\n",
      "3327ae24303f: Verifying Checksum\n",
      "3327ae24303f: Download complete\n",
      "3efe73011a96: Verifying Checksum\n",
      "3efe73011a96: Download complete\n",
      "df0cdf11e45e: Verifying Checksum\n",
      "df0cdf11e45e: Download complete\n",
      "27d78ef0086e: Verifying Checksum\n",
      "27d78ef0086e: Download complete\n",
      "5ccc73e465ef: Pull complete\n",
      "e9d8630baacd: Pull complete\n",
      "82ad86c72f57: Pull complete\n",
      "a6746fce16a1: Pull complete\n",
      "7a67c64e20d2: Pull complete\n",
      "334312f3cce5: Pull complete\n",
      "d1c06a369ea4: Pull complete\n",
      "61aa2ea143ea: Pull complete\n",
      "87555fa6733c: Pull complete\n",
      "dcfc4adad992: Pull complete\n",
      "dcb1b56a4779: Pull complete\n",
      "95837e5e0af5: Pull complete\n",
      "9fe1d9bf08cb: Pull complete\n",
      "1cc20834967b: Pull complete\n",
      "f492fb29e345: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "4072293fd93e: Pull complete\n",
      "27d78ef0086e: Pull complete\n",
      "c733e8557bb1: Pull complete\n",
      "df0cdf11e45e: Pull complete\n",
      "3327ae24303f: Pull complete\n",
      "3efe73011a96: Pull complete\n",
      "Digest: sha256:7fa2b006819f3a484ea0a9c006b25f7321f0769a01b24440a56abca80953a75b\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-5:latest\n",
      " ---> 0f998c784cd6\n",
      "Step 2/8 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Running in 010b21a626f7\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be REMOVED:\n",
      "  google-fast-socket\n",
      "0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "(Reading database ... 80586 files and directories currently installed.)\n",
      "Removing google-fast-socket (0.0.3) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "deb https://packages.cloud.google.com/apt google-fast-socket main\n",
      "\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5385 B]\n",
      "Get:3 https://packages.cloud.google.com/apt google-fast-socket InRelease [5405 B]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6780 B]\n",
      "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:7 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [339 B]\n",
      "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
      "Get:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
      "Get:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:13 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
      "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1415 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [450 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2185 kB]\n",
      "Get:17 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [431 B]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:20 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [190 kB]\n",
      "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
      "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [599 kB]\n",
      "Get:22 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [73.8 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2185 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2619 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.5 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [481 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]\n",
      "Fetched 23.7 MB in 5s (5212 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "15 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  google-reduction-server\n",
      "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 707 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://packages.cloud.google.com/apt google-fast-socket/main amd64 google-reduction-server amd64 2.1.5 [707 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 707 kB in 1s (1354 kB/s)\n",
      "Selecting previously unselected package google-reduction-server.\n",
      "(Reading database ... 80583 files and directories currently installed.)\n",
      "Preparing to unpack .../google-reduction-server_2.1.5_amd64.deb ...\n",
      "Unpacking google-reduction-server (2.1.5) ...\n",
      "Setting up google-reduction-server (2.1.5) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "Removing intermediate container 010b21a626f7\n",
      " ---> 1988a9daff8a\n",
      "Step 3/8 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Running in 34562c4871b6\n",
      "Collecting tf-models-official==2.5.0\n",
      "  Downloading tf_models_official-2.5.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting tensorflow-text==2.5.0\n",
      "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.16.0)\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.1.3)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.2.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.4.1)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.19.5)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.3.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.8.0)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (3.4.2)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.20.0)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.4)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.9.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.1.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.2)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (20.9)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2021.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.18.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.38.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (4.61.1)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (1.26.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.0.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.13.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.6.3)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.36.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0.dev2021032900)\n",
      "Collecting grpcio<2.0dev,>=1.29.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.0)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (4.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.5.0) (0.1.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.4.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0) (1.3)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0) (0.24.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (2.1.0)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.18.2)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (2.3)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (5.1.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.3.4)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (1.0.0)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=21c7e3885cdd202b89c0c2f7f942324ecb4a8abd69359b05d2854f0ebaea2be4\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22245 sha256=e94ca66bf6358557d3e8721c1bf55f8ede9bd0712a06c10bd7c1e23ded66d7fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272450 sha256=42a8c7b440f9e324137473a0c8952ad4ca45696f09c90862157b70619fdd924f\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=669667c60b7ba47788df2b10c9d15c6b8102583bf9f2c2a3abf25ab41b8afb0a\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval\n",
      "Installing collected packages: six, typing-extensions, grpcio, absl-py, typeguard, portalocker, Cython, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pycocotools, py-cpuinfo, opencv-python-headless, kaggle, gin-config, tf-models-official, tensorflow-text\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.0\n",
      "    Uninstalling grpcio-1.38.0:\n",
      "      Successfully uninstalled grpcio-1.38.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "Successfully installed Cython-0.29.23 absl-py-0.12.0 gin-config-0.4.0 grpcio-1.34.1 kaggle-1.5.12 opencv-python-headless-4.5.2.54 portalocker-2.0.0 py-cpuinfo-8.0.0 pycocotools-2.0.2 sacrebleu-1.5.1 sentencepiece-0.1.96 seqeval-1.2.2 six-1.15.0 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.5.0 tf-models-official-2.5.0 tf-slim-1.1.0 typeguard-2.12.1 typing-extensions-3.7.4.3\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tfx-bsl 1.0.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.9.0 which is incompatible.\n",
      "tfx-bsl 1.0.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "tensorflow-transform 1.0.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "apache-beam 2.30.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.30.0 requires pyarrow<4.0.0,>=0.15.1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 34562c4871b6\n",
      " ---> a8268955920e\n",
      "Step 4/8 : WORKDIR /\n",
      " ---> Running in d60ed5f15aa3\n",
      "Removing intermediate container d60ed5f15aa3\n",
      " ---> 14abbfcf4927\n",
      "Step 5/8 : COPY trainer /trainer\n",
      " ---> 5d2147261770\n",
      "Step 6/8 : COPY dataprep /dataprep\n",
      " ---> a2a3e4cdd511\n",
      "Step 7/8 : ENTRYPOINT [\"python\"]\n",
      " ---> Running in 5118fe112299\n",
      "Removing intermediate container 5118fe112299\n",
      " ---> 4b349120108c\n",
      "Step 8/8 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Running in ce93577e55f9\n",
      "Removing intermediate container ce93577e55f9\n",
      " ---> 33b406961c70\n",
      "Successfully built 33b406961c70\n",
      "Successfully tagged gcr.io/jk-mlops-dev/model_garden:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/model_garden\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/model_garden]\n",
      "8d979ccaf880: Preparing\n",
      "1e69d3224aa6: Preparing\n",
      "abff78c3f3a7: Preparing\n",
      "eba90c14f092: Preparing\n",
      "03dc40ebdbd6: Preparing\n",
      "f5d954d2bd94: Preparing\n",
      "db34a056d495: Preparing\n",
      "f9b1cb8c2687: Preparing\n",
      "e68443de6bca: Preparing\n",
      "88bb87a4088d: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "61546b863e43: Preparing\n",
      "d2b843fb2f7a: Preparing\n",
      "36c9a9d68143: Preparing\n",
      "730e84ac5c5d: Preparing\n",
      "a25ae1798c0c: Preparing\n",
      "37a19de06c8b: Preparing\n",
      "27c459f353b4: Preparing\n",
      "25d03c11e857: Preparing\n",
      "d5585264beff: Preparing\n",
      "e39414beba01: Preparing\n",
      "ff6af85bc8aa: Preparing\n",
      "98cedd6c9734: Preparing\n",
      "574aa732d388: Preparing\n",
      "686978e3bf48: Preparing\n",
      "80088b120579: Preparing\n",
      "79a187e0621d: Preparing\n",
      "72657ad6008c: Preparing\n",
      "22d4dd8ed907: Preparing\n",
      "d7e7872b888e: Preparing\n",
      "5f08512fd434: Preparing\n",
      "c7bb31fc0e08: Preparing\n",
      "50858308da3d: Preparing\n",
      "37a19de06c8b: Waiting\n",
      "27c459f353b4: Waiting\n",
      "f5d954d2bd94: Waiting\n",
      "25d03c11e857: Waiting\n",
      "db34a056d495: Waiting\n",
      "f9b1cb8c2687: Waiting\n",
      "d5585264beff: Waiting\n",
      "e68443de6bca: Waiting\n",
      "88bb87a4088d: Waiting\n",
      "e39414beba01: Waiting\n",
      "29bf522d97b4: Waiting\n",
      "ff6af85bc8aa: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "98cedd6c9734: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "61546b863e43: Waiting\n",
      "574aa732d388: Waiting\n",
      "d2b843fb2f7a: Waiting\n",
      "686978e3bf48: Waiting\n",
      "36c9a9d68143: Waiting\n",
      "80088b120579: Waiting\n",
      "730e84ac5c5d: Waiting\n",
      "a25ae1798c0c: Waiting\n",
      "79a187e0621d: Waiting\n",
      "72657ad6008c: Waiting\n",
      "22d4dd8ed907: Waiting\n",
      "d7e7872b888e: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "50858308da3d: Waiting\n",
      "03dc40ebdbd6: Layer already exists\n",
      "f5d954d2bd94: Layer already exists\n",
      "db34a056d495: Layer already exists\n",
      "f9b1cb8c2687: Layer already exists\n",
      "e68443de6bca: Layer already exists\n",
      "8d979ccaf880: Pushed\n",
      "1e69d3224aa6: Pushed\n",
      "88bb87a4088d: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "d2b843fb2f7a: Layer already exists\n",
      "61546b863e43: Layer already exists\n",
      "730e84ac5c5d: Layer already exists\n",
      "a25ae1798c0c: Layer already exists\n",
      "36c9a9d68143: Layer already exists\n",
      "37a19de06c8b: Layer already exists\n",
      "25d03c11e857: Layer already exists\n",
      "27c459f353b4: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "ff6af85bc8aa: Layer already exists\n",
      "d5585264beff: Layer already exists\n",
      "574aa732d388: Layer already exists\n",
      "98cedd6c9734: Layer already exists\n",
      "686978e3bf48: Layer already exists\n",
      "79a187e0621d: Layer already exists\n",
      "80088b120579: Layer already exists\n",
      "72657ad6008c: Layer already exists\n",
      "5f08512fd434: Layer already exists\n",
      "22d4dd8ed907: Layer already exists\n",
      "d7e7872b888e: Layer already exists\n",
      "50858308da3d: Layer already exists\n",
      "c7bb31fc0e08: Layer already exists\n",
      "eba90c14f092: Pushed\n",
      "abff78c3f3a7: Pushed\n",
      "latest: digest: sha256:3899d62e0c36c8ea66bf61713e38ac33c66add3a7792fb693ffb5ffcee6ac40e size: 7881\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                      STATUS\n",
      "da1dfe86-1c10-4170-b76a-852c5f39cbe4  2021-06-21T22:22:20+00:00  8M24S     gs://jk-mlops-dev_cloudbuild/source/1624314140.348313-aca78fcba9064d28ac28ad8cd52e534e.tgz  gcr.io/jk-mlops-dev/model_garden (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} model_garden_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'{GCS_BUCKET}/datasets'\n",
    "BERT_DIR = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16'\n",
    "TASK = 'MNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-8'\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAIN_IMAGE,\n",
    "            'command': ['python', 'dataprep/create_finetuning_data.py'],\n",
    "            'args': [\n",
    "                '--fine_tuning_task_type=classification',\n",
    "                '--tfds_params=dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched',\n",
    "                '--max_seq_length=128',\n",
    "                f'--vocab_file={BERT_DIR}/vocab.txt',\n",
    "                f'--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data',\n",
    "                f'--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record',\n",
    "                f'--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-central1/customJobs/2139882724117184512\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-central1/customJobs/2139882724117184512')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/2139882724117184512?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/2139882724117184512 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/895222332033/locations/us-central1/customJobs/2139882724117184512\n"
     ]
    }
   ],
   "source": [
    "job_name = \"PREPARE_DATA_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and run MNLI fine tuning job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure MNLI experiment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'bert/sentence_prediction'\n",
    "CONFIG_FILE = 'trainer/glue_mnli_matched.yaml'\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_TRAIN_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record'\n",
    "MNLI_VALID_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "REPLICA_COUNT = 8\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "GLOBAL_BATCH_SIZE = REPLICA_COUNT * PER_REPLICA_BATCH_SIZE\n",
    "\n",
    "ACCELERATOR_COUNT = 1\n",
    "ALL_REDUCE_ALG = 'nccl'\n",
    "STRATEGY = 'multi_worker_mirrored'\n",
    "\n",
    "TRAINING_STEPS = 2000\n",
    "STEPS_PER_LOOP = 100\n",
    "SUMMARY_INTERVAL = 100\n",
    "VALIDATION_INTERVAL = 2000\n",
    "CHECKPOINT_INTERVAL = 2000\n",
    "\n",
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.train_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(ACCELERATOR_COUNT),\n",
    "    'runtime.distribution_strategy=' + STRATEGY,\n",
    "    'runtime.all_reduce_alg=' + ALL_REDUCE_ALG,\n",
    " #   'runtime.mixed_precision_dtype=' + mixed_precision_dtype,\n",
    "    'trainer.train_steps=' + str(TRAINING_STEPS),\n",
    "    'trainer.steps_per_loop=' + str(STEPS_PER_LOOP),\n",
    "    'trainer.summary_interval=' + str(SUMMARY_INTERVAL),\n",
    "    'trainer.validation_interval=' + str(VALIDATION_INTERVAL),\n",
    "    'trainer.checkpoint_interval=' + str(CHECKPOINT_INTERVAL),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex training custom job spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{GCS_BUCKET}/{JOB_NAME}/model'\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=' + EXPERIMENT,\n",
    "    '--mode=' + MODE,\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=' + CONFIG_FILE,\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 0\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210621_230150',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210621_230150/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210621_230150/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 1,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 7}]}}\n"
     ]
    }
   ],
   "source": [
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"projects/895222332033/locations/us-central1/customJobs/349701872237412352\"\n",
       "display_name: \"MNLI_20210621_230150\"\n",
       "job_spec {\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 1\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/model_garden\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-staging-us-central1/MNLI_20210621_230150/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000\"\n",
       "    }\n",
       "  }\n",
       "  worker_pool_specs {\n",
       "    machine_spec {\n",
       "      machine_type: \"a2-highgpu-1g\"\n",
       "      accelerator_type: NVIDIA_TESLA_A100\n",
       "      accelerator_count: 1\n",
       "    }\n",
       "    replica_count: 7\n",
       "    disk_spec {\n",
       "      boot_disk_type: \"pd-ssd\"\n",
       "      boot_disk_size_gb: 100\n",
       "    }\n",
       "    container_spec {\n",
       "      image_uri: \"gcr.io/jk-mlops-dev/model_garden\"\n",
       "      command: \"python\"\n",
       "      command: \"trainer/train.py\"\n",
       "      args: \"--experiment=bert/sentence_prediction\"\n",
       "      args: \"--mode=train\"\n",
       "      args: \"--model_dir=gs://jk-staging-us-central1/MNLI_20210621_230150/model\"\n",
       "      args: \"--config_file=trainer/glue_mnli_matched.yaml\"\n",
       "      args: \"--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=256,task.validation_data.global_batch_size=256,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=1,runtime.distribution_strategy=multi_worker_mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=2000,trainer.steps_per_loop=100,trainer.summary_interval=100,trainer.validation_interval=2000,trainer.checkpoint_interval=2000\"\n",
       "    }\n",
       "  }\n",
       "}\n",
       "state: JOB_STATE_PENDING\n",
       "create_time {\n",
       "  seconds: 1624316520\n",
       "  nanos: 649814000\n",
       "}\n",
       "update_time {\n",
       "  seconds: 1624316520\n",
       "  nanos: 649814000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = JobServiceClient(client_options=options)\n",
    "\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "\n",
    "response = client.create_custom_job(\n",
    "    parent=parent, custom_job=custom_job_spec\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JobState.JOB_STATE_QUEUED: 1>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_custom_job(name=response.name).state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload logs to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TENSORBOARD={}'.format('projects/895222332033/locations/us-central1/tensorboards/5983067289333792768'))\n",
    "print('LOGDIR={}'.format(model_dir))\n",
    "print('EXPERIMENT={}'.format(job_name))\n",
    "print('./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the container image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_TRAIN_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record'\n",
    "MNLI_VALID_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "output_dir = '/tmp'\n",
    "num_gpus = 2 \n",
    "strategy = 'mirrored'\n",
    "#strategy = 'multi_worker_mirrored'\n",
    "\n",
    "params_override = [\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(num_gpus),\n",
    "    'runtime.distribution_strategy=' + strategy,\n",
    "    'runtime.mixed_precision_dtype=' + 'tf.float16',\n",
    "]\n",
    "\n",
    "params = ','.join(params_override)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all {TRAIN_IMAGE} trainer/train.py \\\n",
    "--experiment=bert/sentence_prediction \\\n",
    "--mode=train_and_eval \\\n",
    "--model_dir={output_dir}/test \\\n",
    "--config_file=trainer/glue_mnli_matched.yaml \\\n",
    "--params_override={params}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.cloud.aiplatform.gapic import \\\n",
    "#    JobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STAGING_BUCKET = f'gs://jk-vertex-{REGION}'\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")\n",
    "REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_name = job_name\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parking lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "--volume {MNLI_LOCAL_FOLDER}/MNLI:/data/MNLI \\\n",
    "--env OUTPUT_DIR={OUTPUT_DIR} \\\n",
    "--env TASK=MNLI \\\n",
    "--env BERT_DIR={BERT_DIR} \\\n",
    "{TRAIN_IMAGE} \\\n",
    "'python models/official/nlp/data/create_finetuning_data.py \\\n",
    " --input_data_dir=/data/MNLI \\\n",
    " --vocab_file=${BERT_DIR}/vocab.txt \\\n",
    " --train_data_output_path=${OUTPUT_DIR}/${TASK}/${TASK}_train.tf_record \\\n",
    " --eval_data_output_path=${OUTPUT_DIR}/${TASK}/${TASK}_eval.tf_record \\\n",
    " --meta_data_file_path=${OUTPUT_DIR}/${TASK}/${TASK}_meta_data \\\n",
    " --fine_tuning_task_type=classification --max_seq_length=128 \\\n",
    " --classification_task_name=${TASK}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "{TRAIN_IMAGE} \\\n",
    "dataprep/create_finetuning_data.py \\\n",
    "--fine_tuning_task_type=classification \\\n",
    "--tfds_params=\"dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched\" \\\n",
    "--max_seq_length=128 \\\n",
    "--vocab_file={BERT_DIR}/vocab.txt \\\n",
    "--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data \\\n",
    "--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record \\\n",
    "--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all \\\n",
    "{TRAIN_IMAGE} \\\n",
    "dataprep/create_finetuning_data.py \\\n",
    "--fine_tuning_task_type=classification \\\n",
    "--tfds_params=\"dataset=glue/mnli,text_key=hypothesis,text_b_key=premise\" \\\n",
    "--max_seq_length=128 \\\n",
    "--vocab_file={BERT_DIR}/vocab.txt \\\n",
    "--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data \\\n",
    "--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record \\\n",
    "--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "file_extension": ".py",
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
