{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Vertex Reduction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform_v1beta1 import types\n",
    "from google.cloud.aiplatform_v1beta1.services.job_service import \\\n",
    "    JobServiceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GCP settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'jk-mlops-dev'\n",
    "REGION = 'us-central1'\n",
    "API_ENDPOINT = f'{REGION}-aiplatform.googleapis.com'\n",
    "GCS_BUCKET = f'gs://jk-staging-{REGION}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = !gsutil ls {GCS_BUCKET}\n",
    "if objects:\n",
    "    if 'BucketNotFoundException' in objects[0]:\n",
    "        print('Creating a new bucket')\n",
    "        !gsutil mb -l {REGION} {GCS_BUCKET} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT,\n",
    "    location=REGION,\n",
    "    staging_bucket=f'{GCS_BUCKET}/vertex'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = f'gcr.io/{PROJECT}/model_garden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 9 file(s) totalling 27.7 KiB before compression.\n",
      "Uploading tarball of [model_garden_image] to [gs://jk-mlops-dev_cloudbuild/source/1624546720.217471-bc8f902bedcb42f48c805a20c1b42465.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/locations/global/builds/40831f61-9b95-4a4b-976e-645965bfb90e].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/40831f61-9b95-4a4b-976e-645965bfb90e?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"40831f61-9b95-4a4b-976e-645965bfb90e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1624546720.217471-bc8f902bedcb42f48c805a20c1b42465.tgz#1624546720586901\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1624546720.217471-bc8f902bedcb42f48c805a20c1b42465.tgz#1624546720586901...\n",
      "/ [1 files][  6.0 KiB/  6.0 KiB]                                                \n",
      "Operation completed over 1 objects/6.0 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon   38.4kB\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      "latest: Pulling from deeplearning-platform-release/tf2-gpu.2-5\n",
      "4bbfd2c87b75: Pulling fs layer\n",
      "d2e110be24e1: Pulling fs layer\n",
      "889a7173dcfe: Pulling fs layer\n",
      "e1948de7822d: Pulling fs layer\n",
      "9c8e4ac0274d: Pulling fs layer\n",
      "29dcb86d08de: Pulling fs layer\n",
      "70d8019dd5e8: Pulling fs layer\n",
      "5ccc73e465ef: Pulling fs layer\n",
      "e9d8630baacd: Pulling fs layer\n",
      "82ad86c72f57: Pulling fs layer\n",
      "a6746fce16a1: Pulling fs layer\n",
      "7a67c64e20d2: Pulling fs layer\n",
      "334312f3cce5: Pulling fs layer\n",
      "d1c06a369ea4: Pulling fs layer\n",
      "61aa2ea143ea: Pulling fs layer\n",
      "87555fa6733c: Pulling fs layer\n",
      "dcfc4adad992: Pulling fs layer\n",
      "dcb1b56a4779: Pulling fs layer\n",
      "95837e5e0af5: Pulling fs layer\n",
      "9fe1d9bf08cb: Pulling fs layer\n",
      "1cc20834967b: Pulling fs layer\n",
      "f492fb29e345: Pulling fs layer\n",
      "b1c9ec7e36f6: Pulling fs layer\n",
      "94e3f0ba3fb3: Pulling fs layer\n",
      "00f212e657e4: Pulling fs layer\n",
      "7d32fad557e1: Pulling fs layer\n",
      "4072293fd93e: Pulling fs layer\n",
      "27d78ef0086e: Pulling fs layer\n",
      "c733e8557bb1: Pulling fs layer\n",
      "df0cdf11e45e: Pulling fs layer\n",
      "3327ae24303f: Pulling fs layer\n",
      "3efe73011a96: Pulling fs layer\n",
      "e1948de7822d: Waiting\n",
      "9c8e4ac0274d: Waiting\n",
      "29dcb86d08de: Waiting\n",
      "70d8019dd5e8: Waiting\n",
      "5ccc73e465ef: Waiting\n",
      "e9d8630baacd: Waiting\n",
      "82ad86c72f57: Waiting\n",
      "a6746fce16a1: Waiting\n",
      "7a67c64e20d2: Waiting\n",
      "334312f3cce5: Waiting\n",
      "d1c06a369ea4: Waiting\n",
      "61aa2ea143ea: Waiting\n",
      "87555fa6733c: Waiting\n",
      "dcfc4adad992: Waiting\n",
      "dcb1b56a4779: Waiting\n",
      "95837e5e0af5: Waiting\n",
      "9fe1d9bf08cb: Waiting\n",
      "1cc20834967b: Waiting\n",
      "f492fb29e345: Waiting\n",
      "b1c9ec7e36f6: Waiting\n",
      "94e3f0ba3fb3: Waiting\n",
      "00f212e657e4: Waiting\n",
      "7d32fad557e1: Waiting\n",
      "4072293fd93e: Waiting\n",
      "27d78ef0086e: Waiting\n",
      "c733e8557bb1: Waiting\n",
      "df0cdf11e45e: Waiting\n",
      "3327ae24303f: Waiting\n",
      "3efe73011a96: Waiting\n",
      "889a7173dcfe: Verifying Checksum\n",
      "889a7173dcfe: Download complete\n",
      "d2e110be24e1: Verifying Checksum\n",
      "d2e110be24e1: Download complete\n",
      "e1948de7822d: Verifying Checksum\n",
      "e1948de7822d: Download complete\n",
      "9c8e4ac0274d: Verifying Checksum\n",
      "9c8e4ac0274d: Download complete\n",
      "29dcb86d08de: Verifying Checksum\n",
      "29dcb86d08de: Download complete\n",
      "4bbfd2c87b75: Download complete\n",
      "70d8019dd5e8: Verifying Checksum\n",
      "70d8019dd5e8: Download complete\n",
      "e9d8630baacd: Download complete\n",
      "a6746fce16a1: Verifying Checksum\n",
      "a6746fce16a1: Download complete\n",
      "4bbfd2c87b75: Pull complete\n",
      "d2e110be24e1: Pull complete\n",
      "889a7173dcfe: Pull complete\n",
      "e1948de7822d: Pull complete\n",
      "9c8e4ac0274d: Pull complete\n",
      "29dcb86d08de: Pull complete\n",
      "70d8019dd5e8: Pull complete\n",
      "82ad86c72f57: Download complete\n",
      "334312f3cce5: Verifying Checksum\n",
      "334312f3cce5: Download complete\n",
      "7a67c64e20d2: Verifying Checksum\n",
      "7a67c64e20d2: Download complete\n",
      "61aa2ea143ea: Verifying Checksum\n",
      "61aa2ea143ea: Download complete\n",
      "87555fa6733c: Verifying Checksum\n",
      "87555fa6733c: Download complete\n",
      "dcfc4adad992: Verifying Checksum\n",
      "dcfc4adad992: Download complete\n",
      "d1c06a369ea4: Verifying Checksum\n",
      "d1c06a369ea4: Download complete\n",
      "95837e5e0af5: Verifying Checksum\n",
      "95837e5e0af5: Download complete\n",
      "9fe1d9bf08cb: Verifying Checksum\n",
      "9fe1d9bf08cb: Download complete\n",
      "1cc20834967b: Verifying Checksum\n",
      "1cc20834967b: Download complete\n",
      "f492fb29e345: Verifying Checksum\n",
      "f492fb29e345: Download complete\n",
      "b1c9ec7e36f6: Verifying Checksum\n",
      "b1c9ec7e36f6: Download complete\n",
      "94e3f0ba3fb3: Verifying Checksum\n",
      "94e3f0ba3fb3: Download complete\n",
      "00f212e657e4: Verifying Checksum\n",
      "00f212e657e4: Download complete\n",
      "7d32fad557e1: Verifying Checksum\n",
      "7d32fad557e1: Download complete\n",
      "dcb1b56a4779: Download complete\n",
      "4072293fd93e: Download complete\n",
      "c733e8557bb1: Verifying Checksum\n",
      "c733e8557bb1: Download complete\n",
      "5ccc73e465ef: Verifying Checksum\n",
      "5ccc73e465ef: Download complete\n",
      "3327ae24303f: Verifying Checksum\n",
      "3327ae24303f: Download complete\n",
      "3efe73011a96: Verifying Checksum\n",
      "3efe73011a96: Download complete\n",
      "df0cdf11e45e: Verifying Checksum\n",
      "df0cdf11e45e: Download complete\n",
      "27d78ef0086e: Verifying Checksum\n",
      "27d78ef0086e: Download complete\n",
      "5ccc73e465ef: Pull complete\n",
      "e9d8630baacd: Pull complete\n",
      "82ad86c72f57: Pull complete\n",
      "a6746fce16a1: Pull complete\n",
      "7a67c64e20d2: Pull complete\n",
      "334312f3cce5: Pull complete\n",
      "d1c06a369ea4: Pull complete\n",
      "61aa2ea143ea: Pull complete\n",
      "87555fa6733c: Pull complete\n",
      "dcfc4adad992: Pull complete\n",
      "dcb1b56a4779: Pull complete\n",
      "95837e5e0af5: Pull complete\n",
      "9fe1d9bf08cb: Pull complete\n",
      "1cc20834967b: Pull complete\n",
      "f492fb29e345: Pull complete\n",
      "b1c9ec7e36f6: Pull complete\n",
      "94e3f0ba3fb3: Pull complete\n",
      "00f212e657e4: Pull complete\n",
      "7d32fad557e1: Pull complete\n",
      "4072293fd93e: Pull complete\n",
      "27d78ef0086e: Pull complete\n",
      "c733e8557bb1: Pull complete\n",
      "df0cdf11e45e: Pull complete\n",
      "3327ae24303f: Pull complete\n",
      "3efe73011a96: Pull complete\n",
      "Digest: sha256:7fa2b006819f3a484ea0a9c006b25f7321f0769a01b24440a56abca80953a75b\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/tf2-gpu.2-5:latest\n",
      " ---> 0f998c784cd6\n",
      "Step 2/8 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Running in 09d4104f296e\n",
      "\u001b[91m\n",
      "\u001b[0m\u001b[91mWARNING: \u001b[0m\u001b[91mapt\u001b[0m\u001b[91m \u001b[0m\u001b[91mdoes not have a stable CLI interface. \u001b[0m\u001b[91mUse with caution in scripts.\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91m\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be REMOVED:\n",
      "  google-fast-socket\n",
      "0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "(Reading database ... 80586 files and directories currently installed.)\n",
      "Removing google-fast-socket (0.0.3) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "deb https://packages.cloud.google.com/apt google-fast-socket main\n",
      "\u001b[91mWarning: apt-key output should not be parsed (stdout is not a terminal)\n",
      "\u001b[0mOK\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:3 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5385 B]\n",
      "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
      "Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
      "Get:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:10 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6780 B]\n",
      "Get:11 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2219 kB]\n",
      "Get:13 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [339 B]\n",
      "Get:14 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [473 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1418 kB]\n",
      "Get:16 https://packages.cloud.google.com/apt google-fast-socket InRelease [5405 B]\n",
      "Ign:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
      "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [599 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:19 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [73.8 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:21 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [191 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:23 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [431 B]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.5 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2652 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [505 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2188 kB]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]\n",
      "Fetched 23.8 MB in 3s (6896 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "17 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "\u001b[0m\u001b[91mWARNING: \u001b[0m\u001b[91mapt\u001b[0m\u001b[91m \u001b[0m\u001b[91mdoes not have a stable CLI interface. \u001b[0m\u001b[91mUse with caution in scripts.\u001b[0m\u001b[91m\n",
      "\u001b[0m\u001b[91m\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  google-reduction-server\n",
      "0 upgraded, 1 newly installed, 0 to remove and 17 not upgraded.\n",
      "Need to get 707 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://packages.cloud.google.com/apt google-fast-socket/main amd64 google-reduction-server amd64 2.1.5 [707 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 707 kB in 0s (3337 kB/s)\n",
      "Selecting previously unselected package google-reduction-server.\n",
      "(Reading database ... 80583 files and directories currently installed.)\n",
      "Preparing to unpack .../google-reduction-server_2.1.5_amd64.deb ...\n",
      "Unpacking google-reduction-server (2.1.5) ...\n",
      "Setting up google-reduction-server (2.1.5) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "Removing intermediate container 09d4104f296e\n",
      " ---> 1b86759ec11d\n",
      "Step 3/8 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Running in e97678fde236\n",
      "Collecting tf-models-official==2.5.0\n",
      "  Downloading tf_models_official-2.5.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting tensorflow-text==2.5.0\n",
      "  Downloading tensorflow_text-2.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "Requirement already satisfied: oauth2client in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.1.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.6.3)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.9.0)\n",
      "Requirement already satisfied: google-cloud-bigquery>=0.31.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.20.0)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.19.5)\n",
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.2.54-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (4.3.0)\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.2.tar.gz (23 kB)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (8.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.4.1)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (0.12.0)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.2.4)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "Collecting tf-slim>=1.1.0\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "Requirement already satisfied: tensorflow>=2.5.0 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (5.8.0)\n",
      "Collecting Cython\n",
      "  Using cached Cython-0.29.23-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from tf-models-official==2.5.0) (3.4.2)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.1.0)\n",
      "Requirement already satisfied: google-auth<2dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.30.2)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.0.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.53.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2021.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (20.9)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.2.7)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.18.1)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.7.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (1.38.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery>=0.31.0->tf-models-official==2.5.0) (2.20)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.4.7)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (2.8.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (4.61.1)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle>=1.3.9->tf-models-official==2.5.0) (1.26.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2dev,>=1.16.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official==2.5.0) (4.0.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0.dev2021032900)\n",
      "Collecting grpcio<2.0dev,>=1.29.0\n",
      "  Downloading grpcio-1.34.1-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12.1)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.13.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.36.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.12)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.2)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.6.3)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (0.2.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (2.5.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow>=2.5.0->tf-models-official==2.5.0) (1.1.0)\n",
      "Collecting six\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (0.4.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (4.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.1.1)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official==2.5.0) (0.1.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.5.0->tf-models-official==2.5.0) (3.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->tf-models-official==2.5.0) (0.10.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official==2.5.0) (1.3)\n",
      "Collecting portalocker==2.0.0\n",
      "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.7/site-packages (from seqeval->tf-models-official==2.5.0) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official==2.5.0) (1.0.1)\n",
      "Collecting typeguard>=2.7\n",
      "  Downloading typeguard-2.12.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (21.2.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (1.0.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (5.1.4)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (2.3)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.18.2)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->tf-models-official==2.5.0) (0.3.4)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Building wheels for collected packages: kaggle, py-cpuinfo, pycocotools, seqeval\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73053 sha256=4e1647535ee4345f847788615e72a8f281c169c74da91b70c2dbb74d18a68f64\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "  Building wheel for py-cpuinfo (setup.py): started\n",
      "  Building wheel for py-cpuinfo (setup.py): finished with status 'done'\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22245 sha256=2eff9255c4b0c6aeb6952754a13576b1d8e2101114dead0edaa8abab1d7614e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for pycocotools (setup.py): started\n",
      "  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl size=272444 sha256=7aecf328cc579e941b6a11e9948a25fd5f6e6384919bf79f0740b25c09709a63\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/cf/1b/e95c99c5f9d1648be3f500ca55e7ce55f24818b0f48336adaf\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=2b24478bf48a458930cd68f1ce10249d4c075ad9eb4c6914208870cbbf9e7a53\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "Successfully built kaggle py-cpuinfo pycocotools seqeval\n",
      "Installing collected packages: six, typing-extensions, grpcio, absl-py, typeguard, portalocker, Cython, tf-slim, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pycocotools, py-cpuinfo, opencv-python-headless, kaggle, gin-config, tf-models-official, tensorflow-text\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.0\n",
      "    Uninstalling grpcio-1.38.0:\n",
      "      Successfully uninstalled grpcio-1.38.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.13.0\n",
      "    Uninstalling absl-py-0.13.0:\n",
      "      Successfully uninstalled absl-py-0.13.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.18.0 requires tensorflow-io-gcs-filesystem==0.18.0, which is not installed.\n",
      "tfx-bsl 1.0.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.9.0 which is incompatible.\n",
      "tfx-bsl 1.0.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "tensorflow-transform 1.0.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "apache-beam 2.30.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
      "apache-beam 2.30.0 requires pyarrow<4.0.0,>=0.15.1, but you have pyarrow 4.0.1 which is incompatible.\n",
      "WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mSuccessfully installed Cython-0.29.23 absl-py-0.12.0 gin-config-0.4.0 grpcio-1.34.1 kaggle-1.5.12 opencv-python-headless-4.5.2.54 portalocker-2.0.0 py-cpuinfo-8.0.0 pycocotools-2.0.2 sacrebleu-1.5.1 sentencepiece-0.1.96 seqeval-1.2.2 six-1.15.0 tensorflow-addons-0.13.0 tensorflow-model-optimization-0.6.0 tensorflow-text-2.5.0 tf-models-official-2.5.0 tf-slim-1.1.0 typeguard-2.12.1 typing-extensions-3.7.4.3\n",
      "Removing intermediate container e97678fde236\n",
      " ---> d4576e43a12a\n",
      "Step 4/8 : WORKDIR /\n",
      " ---> Running in a1c9d648fd0e\n",
      "Removing intermediate container a1c9d648fd0e\n",
      " ---> 2926bedbed39\n",
      "Step 5/8 : COPY trainer /trainer\n",
      " ---> efce99b0a1b2\n",
      "Step 6/8 : COPY dataprep /dataprep\n",
      " ---> 029004f66fcb\n",
      "Step 7/8 : ENTRYPOINT [\"python\"]\n",
      " ---> Running in 58716717bfe0\n",
      "Removing intermediate container 58716717bfe0\n",
      " ---> b9693385a333\n",
      "Step 8/8 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Running in 268d0ef419e2\n",
      "Removing intermediate container 268d0ef419e2\n",
      " ---> e34f92e8e1f9\n",
      "Successfully built e34f92e8e1f9\n",
      "Successfully tagged gcr.io/jk-mlops-dev/model_garden:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/model_garden\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/model_garden]\n",
      "3f30cc6dcee2: Preparing\n",
      "f9d2d0b95692: Preparing\n",
      "4a1df7ea4de6: Preparing\n",
      "43e4f3cd7ec2: Preparing\n",
      "03dc40ebdbd6: Preparing\n",
      "f5d954d2bd94: Preparing\n",
      "db34a056d495: Preparing\n",
      "f9b1cb8c2687: Preparing\n",
      "e68443de6bca: Preparing\n",
      "88bb87a4088d: Preparing\n",
      "29bf522d97b4: Preparing\n",
      "d96c519f0898: Preparing\n",
      "ff0a6aeeabc0: Preparing\n",
      "8a7cebfdebb3: Preparing\n",
      "61546b863e43: Preparing\n",
      "d2b843fb2f7a: Preparing\n",
      "36c9a9d68143: Preparing\n",
      "730e84ac5c5d: Preparing\n",
      "a25ae1798c0c: Preparing\n",
      "37a19de06c8b: Preparing\n",
      "27c459f353b4: Preparing\n",
      "25d03c11e857: Preparing\n",
      "d5585264beff: Preparing\n",
      "e39414beba01: Preparing\n",
      "ff6af85bc8aa: Preparing\n",
      "98cedd6c9734: Preparing\n",
      "574aa732d388: Preparing\n",
      "686978e3bf48: Preparing\n",
      "80088b120579: Preparing\n",
      "79a187e0621d: Preparing\n",
      "72657ad6008c: Preparing\n",
      "22d4dd8ed907: Preparing\n",
      "d7e7872b888e: Preparing\n",
      "5f08512fd434: Preparing\n",
      "c7bb31fc0e08: Preparing\n",
      "50858308da3d: Preparing\n",
      "61546b863e43: Waiting\n",
      "d2b843fb2f7a: Waiting\n",
      "36c9a9d68143: Waiting\n",
      "730e84ac5c5d: Waiting\n",
      "a25ae1798c0c: Waiting\n",
      "37a19de06c8b: Waiting\n",
      "27c459f353b4: Waiting\n",
      "25d03c11e857: Waiting\n",
      "d5585264beff: Waiting\n",
      "e39414beba01: Waiting\n",
      "ff6af85bc8aa: Waiting\n",
      "98cedd6c9734: Waiting\n",
      "574aa732d388: Waiting\n",
      "686978e3bf48: Waiting\n",
      "80088b120579: Waiting\n",
      "79a187e0621d: Waiting\n",
      "72657ad6008c: Waiting\n",
      "22d4dd8ed907: Waiting\n",
      "d7e7872b888e: Waiting\n",
      "5f08512fd434: Waiting\n",
      "c7bb31fc0e08: Waiting\n",
      "50858308da3d: Waiting\n",
      "f9b1cb8c2687: Waiting\n",
      "e68443de6bca: Waiting\n",
      "88bb87a4088d: Waiting\n",
      "f5d954d2bd94: Waiting\n",
      "29bf522d97b4: Waiting\n",
      "db34a056d495: Waiting\n",
      "d96c519f0898: Waiting\n",
      "ff0a6aeeabc0: Waiting\n",
      "8a7cebfdebb3: Waiting\n",
      "03dc40ebdbd6: Layer already exists\n",
      "f5d954d2bd94: Layer already exists\n",
      "db34a056d495: Layer already exists\n",
      "f9b1cb8c2687: Layer already exists\n",
      "3f30cc6dcee2: Pushed\n",
      "f9d2d0b95692: Pushed\n",
      "e68443de6bca: Layer already exists\n",
      "29bf522d97b4: Layer already exists\n",
      "88bb87a4088d: Layer already exists\n",
      "d96c519f0898: Layer already exists\n",
      "ff0a6aeeabc0: Layer already exists\n",
      "8a7cebfdebb3: Layer already exists\n",
      "61546b863e43: Layer already exists\n",
      "730e84ac5c5d: Layer already exists\n",
      "36c9a9d68143: Layer already exists\n",
      "d2b843fb2f7a: Layer already exists\n",
      "a25ae1798c0c: Layer already exists\n",
      "37a19de06c8b: Layer already exists\n",
      "27c459f353b4: Layer already exists\n",
      "25d03c11e857: Layer already exists\n",
      "d5585264beff: Layer already exists\n",
      "e39414beba01: Layer already exists\n",
      "98cedd6c9734: Layer already exists\n",
      "ff6af85bc8aa: Layer already exists\n",
      "80088b120579: Layer already exists\n",
      "574aa732d388: Layer already exists\n",
      "686978e3bf48: Layer already exists\n",
      "72657ad6008c: Layer already exists\n",
      "22d4dd8ed907: Layer already exists\n",
      "79a187e0621d: Layer already exists\n",
      "c7bb31fc0e08: Layer already exists\n",
      "d7e7872b888e: Layer already exists\n",
      "5f08512fd434: Layer already exists\n",
      "50858308da3d: Layer already exists\n",
      "43e4f3cd7ec2: Pushed\n",
      "4a1df7ea4de6: Pushed\n",
      "latest: digest: sha256:d2c9f0891f21cb07a0e703db8119742e99da0ab61c288b622185f9be3f968dbe size: 7881\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                      STATUS\n",
      "40831f61-9b95-4a4b-976e-645965bfb90e  2021-06-24T14:58:40+00:00  8M32S     gs://jk-mlops-dev_cloudbuild/source/1624546720.217471-bc8f902bedcb42f48c805a20c1b42465.tgz  gcr.io/jk-mlops-dev/model_garden (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag {TRAIN_IMAGE} model_garden_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f'{GCS_BUCKET}/datasets'\n",
    "BERT_DIR = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16'\n",
    "TASK = 'MNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pool_specs = [\n",
    "    {\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-8'\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': TRAIN_IMAGE,\n",
    "            'command': ['python', 'dataprep/create_finetuning_data.py'],\n",
    "            'args': [\n",
    "                '--fine_tuning_task_type=classification',\n",
    "                '--tfds_params=dataset=glue/mnli,text_key=hypothesis,text_b_key=premise,train_split=train,dev_split=validation_matched',\n",
    "                '--max_seq_length=128',\n",
    "                f'--vocab_file={BERT_DIR}/vocab.txt',\n",
    "                f'--meta_data_file_path={OUTPUT_DIR}/{TASK}/{TASK}_meta_data',\n",
    "                f'--train_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record',\n",
    "                f'--eval_data_output_path={OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record',\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-central1/customJobs/8483343536756883456\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-central1/customJobs/8483343536756883456')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8483343536756883456?project=895222332033\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_QUEUED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-central1/customJobs/8483343536756883456 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob run completed. Resource name: projects/895222332033/locations/us-central1/customJobs/8483343536756883456\n"
     ]
    }
   ],
   "source": [
    "job_name = \"PREPARE_DATA_{}\".format(time.strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=job_name,\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n",
    "\n",
    "job.run(sync=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and run MNLI fine tuning job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_job_spec(\n",
    "    job_name,\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    reduction_server_count=0,\n",
    "    reduction_server_machine_type='n1-highcpu-16',\n",
    "    reduction_server_image_uri='us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'\n",
    "):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    if reduction_server_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': reduction_server_count,\n",
    "            'machine_spec': {\n",
    "                'machine_type': reduction_server_machine_type,\n",
    "            },\n",
    "            'container_spec': {\n",
    "                'image_uri': reduction_server_image_uri\n",
    "            }\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "        \n",
    "    custom_job_spec = {\n",
    "        'display_name': job_name,\n",
    "        'job_spec': {\n",
    "            'worker_pool_specs': worker_pool_specs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return custom_job_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure MNLI experiment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'bert/sentence_prediction'\n",
    "CONFIG_FILE = 'trainer/glue_mnli_matched.yaml'\n",
    "MODE = 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mixed_precision_dtype' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-f549b93e685f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;34m'runtime.distribution_strategy='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSTRATEGY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m'runtime.all_reduce_alg='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mALL_REDUCE_ALG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;34m'runtime.mixed_precision_dtype='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmixed_precision_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;34m'trainer.train_steps='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINING_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m'trainer.steps_per_loop='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTEPS_PER_LOOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mixed_precision_dtype' is not defined"
     ]
    }
   ],
   "source": [
    "MNLI_TRAIN_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record'\n",
    "MNLI_VALID_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "REPLICA_COUNT = 8\n",
    "PER_REPLICA_BATCH_SIZE = 32\n",
    "GLOBAL_BATCH_SIZE = REPLICA_COUNT * PER_REPLICA_BATCH_SIZE\n",
    "\n",
    "ACCELERATOR_COUNT = 1\n",
    "ALL_REDUCE_ALG = 'nccl'\n",
    "STRATEGY = 'multi_worker_mirrored'\n",
    "\n",
    "TRAINING_STEPS = 2000\n",
    "STEPS_PER_LOOP = 100\n",
    "SUMMARY_INTERVAL = 100\n",
    "VALIDATION_INTERVAL = 2000\n",
    "CHECKPOINT_INTERVAL = 2000\n",
    "\n",
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.train_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(ACCELERATOR_COUNT),\n",
    "    'runtime.distribution_strategy=' + STRATEGY,\n",
    "    'runtime.all_reduce_alg=' + ALL_REDUCE_ALG,\n",
    "    'trainer.train_steps=' + str(TRAINING_STEPS),\n",
    "    'trainer.steps_per_loop=' + str(STEPS_PER_LOOP),\n",
    "    'trainer.summary_interval=' + str(SUMMARY_INTERVAL),\n",
    "    'trainer.validation_interval=' + str(VALIDATION_INTERVAL),\n",
    "    'trainer.checkpoint_interval=' + str(CHECKPOINT_INTERVAL),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vertex training custom job spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = 'MNLI_{}'.format(time.strftime('%Y%m%d_%H%M%S'))\n",
    "MODEL_DIR = f'{GCS_BUCKET}/{JOB_NAME}/model'\n",
    "WORKER_MACHINE_TYPE = 'a2-highgpu-1g'\n",
    "ACCELERATOR_TYPE = 'NVIDIA_TESLA_A100'\n",
    "WORKER_CMD = ['python', 'trainer/train.py']\n",
    "WORKER_ARGS = [\n",
    "    '--experiment=' + EXPERIMENT,\n",
    "    '--mode=' + MODE,\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--config_file=' + CONFIG_FILE,\n",
    "    '--params_override=' + PARAMS_OVERRIDE,\n",
    "]\n",
    "\n",
    "REDUCTION_SERVER_COUNT = 10\n",
    "REDUCTION_SERVER_MACHINE_TYPE = 'n1-highcpu-16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'display_name': 'MNLI_20210625_145933',\n",
      " 'job_spec': {'worker_pool_specs': [{'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210625_145933/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=32,task.validation_data.global_batch_size=32,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=2,runtime.distribution_strategy=mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=200,trainer.steps_per_loop=50,trainer.summary_interval=50,trainer.validation_interval=200,trainer.checkpoint_interval=200'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 2,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 1},\n",
      "                                    {'container_spec': {'args': ['--experiment=bert/sentence_prediction',\n",
      "                                                                 '--mode=train',\n",
      "                                                                 '--model_dir=gs://jk-staging-us-central1/MNLI_20210625_145933/model',\n",
      "                                                                 '--config_file=trainer/glue_mnli_matched.yaml',\n",
      "                                                                 '--params_override=task.train_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record,task.validation_data.input_path=gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record,task.train_data.global_batch_size=32,task.validation_data.global_batch_size=32,task.hub_module_url=https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4,runtime.num_gpus=2,runtime.distribution_strategy=mirrored,runtime.all_reduce_alg=nccl,trainer.train_steps=200,trainer.steps_per_loop=50,trainer.summary_interval=50,trainer.validation_interval=200,trainer.checkpoint_interval=200'],\n",
      "                                                        'command': ['python',\n",
      "                                                                    'trainer/train.py'],\n",
      "                                                        'image_uri': 'gcr.io/jk-mlops-dev/model_garden'},\n",
      "                                     'machine_spec': {'accelerator_count': 2,\n",
      "                                                      'accelerator_type': 'NVIDIA_TESLA_A100',\n",
      "                                                      'machine_type': 'a2-highgpu-1g'},\n",
      "                                     'replica_count': 7},\n",
      "                                    {'container_spec': {'image_uri': 'us-docker.pkg.dev/vertex-ai-restricted/training/reductionserver:latest'},\n",
      "                                     'machine_spec': {'machine_type': 'n1-highcpu-16'},\n",
      "                                     'replica_count': 10}]}}\n"
     ]
    }
   ],
   "source": [
    "custom_job_spec = prepare_custom_job_spec(\n",
    "    job_name=JOB_NAME,\n",
    "    image_uri=TRAIN_IMAGE,\n",
    "    args=WORKER_ARGS,\n",
    "    cmd=WORKER_CMD,\n",
    "    replica_count=REPLICA_COUNT,\n",
    "    machine_type=WORKER_MACHINE_TYPE,\n",
    "    accelerator_count=ACCELERATOR_COUNT,\n",
    "    accelerator_type=ACCELERATOR_TYPE,\n",
    "    reduction_server_count=REDUCTION_SERVER_COUNT,\n",
    "    reduction_server_machine_type=REDUCTION_SERVER_MACHINE_TYPE,\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(custom_job_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit and monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgument",
     "evalue": "400 The accelerator count \"2\" is not allowed. The allowed numbers of accelerator \"NVIDIA_TESLA_A100\" to use with machine type \"a2-highgpu-1g\" are: [1].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    922\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.INVALID_ARGUMENT\n\tdetails = \"The accelerator count \"2\" is not allowed. The allowed numbers of accelerator \"NVIDIA_TESLA_A100\" to use with machine type \"a2-highgpu-1g\" are: [1].\"\n\tdebug_error_string = \"{\"created\":\"@1624633175.737827484\",\"description\":\"Error received from peer ipv4:74.125.20.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"The accelerator count \"2\" is not allowed. The allowed numbers of accelerator \"NVIDIA_TESLA_A100\" to use with machine type \"a2-highgpu-1g\" are: [1].\",\"grpc_status\":3}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-ff6b93b709b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m response = client.create_custom_job(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_job\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_job_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform_v1beta1/services/job_service/client.py\u001b[0m in \u001b[0;36mcreate_custom_job\u001b[0;34m(self, request, parent, custom_job, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgument\u001b[0m: 400 The accelerator count \"2\" is not allowed. The allowed numbers of accelerator \"NVIDIA_TESLA_A100\" to use with machine type \"a2-highgpu-1g\" are: [1]."
     ]
    }
   ],
   "source": [
    "options = dict(api_endpoint=API_ENDPOINT)\n",
    "client = JobServiceClient(client_options=options)\n",
    "\n",
    "parent = f\"projects/{PROJECT}/locations/{REGION}\"\n",
    "\n",
    "response = client.create_custom_job(\n",
    "    parent=parent, custom_job=custom_job_spec\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<JobState.JOB_STATE_SUCCEEDED: 4>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_custom_job(name=response.name).state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary - Upload logs to Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORBOARD=projects/895222332033/locations/us-central1/tensorboards/5983067289333792768\n",
      "LOGDIR=gs://jk-staging-us-central1/MNLI_20210624_234931/model\n",
      "EXPERIMENT=MNLI_20210624_234931\n",
      "./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True\n"
     ]
    }
   ],
   "source": [
    "print('TENSORBOARD={}'.format('projects/895222332033/locations/us-central1/tensorboards/5983067289333792768'))\n",
    "print('LOGDIR={}'.format(MODEL_DIR))\n",
    "print('EXPERIMENT={}'.format(JOB_NAME))\n",
    "print('./tb-gcp-uploader --tensorboard_resource_name $TENSORBOARD   --logdir=$LOGDIR   --experiment_name=$EXPERIMENT --one_shot=True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary - Test the container image locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  38.91kB\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> 950969e5619c\n",
      "Step 2/8 : RUN apt remove -y google-fast-socket &&  echo \"deb https://packages.cloud.google.com/apt google-fast-socket main\" | tee /etc/apt/sources.list.d/google-fast-socket.list &&  curl -s -L https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&  apt update && apt install -y google-reduction-server\n",
      " ---> Using cache\n",
      " ---> 8964668e6bca\n",
      "Step 3/8 : RUN pip install tf-models-official==2.5.0 tensorflow-text==2.5.0\n",
      " ---> Using cache\n",
      " ---> 04fc51107496\n",
      "Step 4/8 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> 8eaa3f9eef6c\n",
      "Step 5/8 : COPY trainer /trainer\n",
      " ---> Using cache\n",
      " ---> 39b9d75adec1\n",
      "Step 6/8 : COPY dataprep /dataprep\n",
      " ---> Using cache\n",
      " ---> 04964b034c96\n",
      "Step 7/8 : ENTRYPOINT [\"python\"]\n",
      " ---> Using cache\n",
      " ---> a7e532e60f9a\n",
      "Step 8/8 : CMD [\"-c\", \"print('TF Model Garden')\"]\n",
      " ---> Using cache\n",
      " ---> 6659694cd2d3\n",
      "Successfully built 6659694cd2d3\n",
      "Successfully tagged gcr.io/jk-mlops-dev/model_garden:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {TRAIN_IMAGE} model_garden_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 'bert/sentence_prediction'\n",
    "CONFIG_FILE = 'trainer/glue_mnli_matched.yaml'\n",
    "MODE = 'train'\n",
    "\n",
    "MNLI_TRAIN_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_train.tf_record'\n",
    "MNLI_VALID_SPLIT = f'{OUTPUT_DIR}/{TASK}/{TASK}_eval.tf_record'\n",
    "BERT_HUB_URL = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'\n",
    "\n",
    "PER_REPLICA_BATCH_SIZE = 16\n",
    "ACCELERATOR_COUNT = 2\n",
    "ALL_REDUCE_ALG = 'nccl'\n",
    "STRATEGY = 'mirrored'\n",
    "GLOBAL_BATCH_SIZE = ACCELERATOR_COUNT * PER_REPLICA_BATCH_SIZE\n",
    "\n",
    "TRAINING_STEPS = 200\n",
    "STEPS_PER_LOOP = 50\n",
    "SUMMARY_INTERVAL = 50\n",
    "VALIDATION_INTERVAL = 200\n",
    "CHECKPOINT_INTERVAL = 200\n",
    "\n",
    "MIXED_PRECISION_TYPE = 'mixed_float16'\n",
    "\n",
    "LOCAL_DIR = '/tmp'\n",
    "\n",
    "PARAMS_OVERRIDE = ','.join([\n",
    "    'task.train_data.input_path=' + MNLI_TRAIN_SPLIT,\n",
    "    'task.validation_data.input_path=' + MNLI_VALID_SPLIT,\n",
    "    'task.train_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.validation_data.global_batch_size=' + str(GLOBAL_BATCH_SIZE),\n",
    "    'task.hub_module_url=' + BERT_HUB_URL,\n",
    "    'runtime.num_gpus=' + str(ACCELERATOR_COUNT),\n",
    "    'runtime.distribution_strategy=' + STRATEGY,\n",
    "    'runtime.all_reduce_alg=' + ALL_REDUCE_ALG,\n",
    "#    'runtime.mixed_precision_dtype=' + MIXED_PRECISION_TYPE,\n",
    "    'trainer.train_steps=' + str(TRAINING_STEPS),\n",
    "    'trainer.steps_per_loop=' + str(STEPS_PER_LOOP),\n",
    "    'trainer.summary_interval=' + str(SUMMARY_INTERVAL),\n",
    "    'trainer.validation_interval=' + str(VALIDATION_INTERVAL),\n",
    "    'trainer.checkpoint_interval=' + str(CHECKPOINT_INTERVAL),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-25 02:19:56.981187: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "None\n",
      "I0625 02:20:04.301980 139903761540928 train.py:59] Setting model_dir to: /tmp/test\n",
      "I0625 02:20:04.317953 139903761540928 train_utils.py:286] Final experiment parameters: {'runtime': {'all_reduce_alg': 'nccl',\n",
      "             'batchnorm_spatial_persistent': False,\n",
      "             'dataset_num_private_threads': None,\n",
      "             'default_shard_dim': -1,\n",
      "             'distribution_strategy': 'mirrored',\n",
      "             'enable_xla': False,\n",
      "             'gpu_thread_mode': None,\n",
      "             'loss_scale': None,\n",
      "             'mixed_precision_dtype': None,\n",
      "             'num_cores_per_replica': 1,\n",
      "             'num_gpus': 2,\n",
      "             'num_packs': 1,\n",
      "             'per_gpu_thread_count': 0,\n",
      "             'run_eagerly': False,\n",
      "             'task_index': -1,\n",
      "             'tpu': None,\n",
      "             'tpu_enable_xla_dynamic_padder': None,\n",
      "             'worker_hosts': None},\n",
      " 'task': {'hub_module_url': 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4',\n",
      "          'init_checkpoint': '',\n",
      "          'init_cls_pooler': False,\n",
      "          'metric_type': 'accuracy',\n",
      "          'model': {'encoder': {'bert': {'attention_dropout_rate': 0.1,\n",
      "                                         'dropout_rate': 0.1,\n",
      "                                         'embedding_size': None,\n",
      "                                         'hidden_activation': 'gelu',\n",
      "                                         'hidden_size': 768,\n",
      "                                         'initializer_range': 0.02,\n",
      "                                         'intermediate_size': 3072,\n",
      "                                         'max_position_embeddings': 512,\n",
      "                                         'num_attention_heads': 12,\n",
      "                                         'num_layers': 12,\n",
      "                                         'output_range': None,\n",
      "                                         'return_all_encoder_outputs': False,\n",
      "                                         'type_vocab_size': 2,\n",
      "                                         'vocab_size': 30522},\n",
      "                                'type': 'bert'},\n",
      "                    'num_classes': 3,\n",
      "                    'use_encoder_pooler': False},\n",
      "          'train_data': {'block_length': 1,\n",
      "                         'cache': False,\n",
      "                         'cycle_length': None,\n",
      "                         'deterministic': None,\n",
      "                         'drop_remainder': True,\n",
      "                         'enable_tf_data_service': False,\n",
      "                         'global_batch_size': 32,\n",
      "                         'include_example_id': False,\n",
      "                         'input_path': 'gs://jk-staging-us-central1/datasets/MNLI/MNLI_train.tf_record',\n",
      "                         'is_training': True,\n",
      "                         'label_type': 'int',\n",
      "                         'seed': None,\n",
      "                         'seq_length': 128,\n",
      "                         'sharding': True,\n",
      "                         'shuffle_buffer_size': 100,\n",
      "                         'tf_data_service_address': None,\n",
      "                         'tf_data_service_job_name': None,\n",
      "                         'tfds_as_supervised': False,\n",
      "                         'tfds_data_dir': '',\n",
      "                         'tfds_name': '',\n",
      "                         'tfds_skip_decoding_feature': '',\n",
      "                         'tfds_split': ''},\n",
      "          'validation_data': {'block_length': 1,\n",
      "                              'cache': False,\n",
      "                              'cycle_length': None,\n",
      "                              'deterministic': None,\n",
      "                              'drop_remainder': False,\n",
      "                              'enable_tf_data_service': False,\n",
      "                              'global_batch_size': 32,\n",
      "                              'include_example_id': False,\n",
      "                              'input_path': 'gs://jk-staging-us-central1/datasets/MNLI/MNLI_eval.tf_record',\n",
      "                              'is_training': False,\n",
      "                              'label_type': 'int',\n",
      "                              'seed': None,\n",
      "                              'seq_length': 128,\n",
      "                              'sharding': True,\n",
      "                              'shuffle_buffer_size': 100,\n",
      "                              'tf_data_service_address': None,\n",
      "                              'tf_data_service_job_name': None,\n",
      "                              'tfds_as_supervised': False,\n",
      "                              'tfds_data_dir': '',\n",
      "                              'tfds_name': '',\n",
      "                              'tfds_skip_decoding_feature': '',\n",
      "                              'tfds_split': ''}},\n",
      " 'trainer': {'allow_tpu_summary': False,\n",
      "             'best_checkpoint_eval_metric': 'cls_accuracy',\n",
      "             'best_checkpoint_export_subdir': 'best_ckpt',\n",
      "             'best_checkpoint_metric_comp': 'higher',\n",
      "             'checkpoint_interval': 200,\n",
      "             'continuous_eval_timeout': 3600,\n",
      "             'eval_tf_function': True,\n",
      "             'eval_tf_while_loop': False,\n",
      "             'loss_upper_bound': 1000000.0,\n",
      "             'max_to_keep': 5,\n",
      "             'optimizer_config': {'ema': None,\n",
      "                                  'learning_rate': {'polynomial': {'cycle': False,\n",
      "                                                                   'decay_steps': 36813,\n",
      "                                                                   'end_learning_rate': 0.0,\n",
      "                                                                   'initial_learning_rate': 3e-05,\n",
      "                                                                   'name': 'PolynomialDecay',\n",
      "                                                                   'power': 1.0},\n",
      "                                                    'type': 'polynomial'},\n",
      "                                  'optimizer': {'adamw': {'amsgrad': False,\n",
      "                                                          'beta_1': 0.9,\n",
      "                                                          'beta_2': 0.999,\n",
      "                                                          'clipnorm': None,\n",
      "                                                          'clipvalue': None,\n",
      "                                                          'epsilon': 1e-07,\n",
      "                                                          'exclude_from_weight_decay': ['LayerNorm',\n",
      "                                                                                        'layer_norm',\n",
      "                                                                                        'bias'],\n",
      "                                                          'global_clipnorm': None,\n",
      "                                                          'gradient_clip_norm': 1.0,\n",
      "                                                          'include_in_weight_decay': None,\n",
      "                                                          'name': 'AdamWeightDecay',\n",
      "                                                          'weight_decay_rate': 0.01},\n",
      "                                                'type': 'adamw'},\n",
      "                                  'warmup': {'polynomial': {'name': 'polynomial',\n",
      "                                                            'power': 1,\n",
      "                                                            'warmup_steps': 3681},\n",
      "                                             'type': 'polynomial'}},\n",
      "             'recovery_begin_steps': 0,\n",
      "             'recovery_max_trials': 0,\n",
      "             'steps_per_loop': 50,\n",
      "             'summary_interval': 50,\n",
      "             'train_steps': 200,\n",
      "             'train_tf_function': True,\n",
      "             'train_tf_while_loop': True,\n",
      "             'validation_interval': 200,\n",
      "             'validation_steps': 307}}\n",
      "I0625 02:20:04.318142 139903761540928 train_utils.py:295] Saving experiment configuration to /tmp/test/params.yaml\n",
      "2021-06-25 02:20:04.332216: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-06-25 02:20:05.651717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.652525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-25 02:20:05.652730: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.653456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-25 02:20:05.653513: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-25 02:20:05.759471: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-25 02:20:05.759602: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-06-25 02:20:05.774135: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-06-25 02:20:05.804488: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-06-25 02:20:05.870613: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-06-25 02:20:05.883587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-06-25 02:20:05.905674: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-06-25 02:20:05.905990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.906909: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.907674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.908482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:05.909252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-06-25 02:20:05.909747: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-06-25 02:20:06.089954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.090783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-25 02:20:06.091030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.091861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-06-25 02:20:06.092030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.092948: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.093862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.094760: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.095528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-06-25 02:20:06.095602: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-06-25 02:20:06.834662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-06-25 02:20:06.834736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2021-06-25 02:20:06.834749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y \n",
      "2021-06-25 02:20:06.834756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N \n",
      "2021-06-25 02:20:06.835164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.836107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.836901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.837696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.838515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.839390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "2021-06-25 02:20:06.840481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-06-25 02:20:06.841187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13837 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "W0625 02:20:06.843993 139903761540928 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "I0625 02:20:07.095574 139903761540928 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "I0625 02:20:07.097350 139903761540928 train_utils.py:96] Created the best checkpoint exporter. data_dir: /tmp/test, export_subdir: best_ckpt, metric_name: cls_accuracy\n",
      "I0625 02:20:07.097542 139903761540928 train_utils.py:214] Running default trainer.\n",
      "I0625 02:20:07.115030 139903761540928 resolver.py:106] Using /tmp/tfhub_modules to cache modules.\n",
      "I0625 02:20:07.115745 139903761540928 resolver.py:416] Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'.\n",
      "I0625 02:20:19.897211 139903761540928 resolver.py:154] Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4, Total size: 1.27GB\n",
      "I0625 02:20:19.897747 139903761540928 resolver.py:431] Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/4'.\n",
      "I0625 02:20:41.485239 139903761540928 optimization.py:139] gradient_clip_norm=1.000000\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.490797 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.492575 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.495743 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.496734 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.500442 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.501346 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.503623 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.504540 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.507349 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.508295 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0625 02:20:41.743649 139903761540928 controller.py:362] restoring or initializing model...\n",
      "restoring or initializing model...\n",
      "I0625 02:20:41.743882 139903761540928 controller.py:368] initialized model.\n",
      "initialized model.\n",
      "I0625 02:20:41.743973 139903761540928 train_lib.py:96] Starts to execute mode: train\n",
      "I0625 02:20:41.744770 139903761540928 controller.py:211] train | step:      0 | training until step 200...\n",
      "train | step:      0 | training until step 200...\n",
      "2021-06-25 02:20:41.757897: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-06-25 02:20:41.758605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
      "INFO:tensorflow:batch_all_reduce: 394 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0625 02:21:00.017198 139903761540928 cross_device_ops.py:903] batch_all_reduce: 394 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "W0625 02:21:01.859345 139903761540928 cross_device_ops.py:934] Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "I0625 02:21:01.859604 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:batch_all_reduce: 394 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0625 02:21:10.201644 139903761540928 cross_device_ops.py:903] batch_all_reduce: 394 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "W0625 02:21:12.087925 139903761540928 cross_device_ops.py:934] Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "I0625 02:21:12.088190 139903761540928 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "2021-06-25 02:22:07.203101: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-06-25 02:22:08.335060: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "I0625 02:23:37.195528 139903761540928 controller.py:425] train | step:     50 | steps/sec:    0.3 | output: \n",
      "    {'cls_accuracy': 0.338125,\n",
      "     'learning_rate': 3.6675146e-07,\n",
      "     'training_loss': 1.1269222}\n",
      "train | step:     50 | steps/sec:    0.3 | output: \n",
      "    {'cls_accuracy': 0.338125,\n",
      "     'learning_rate': 3.6675146e-07,\n",
      "     'training_loss': 1.1269222}\n",
      "I0625 02:23:47.681207 139903761540928 controller.py:454] saved checkpoint to /tmp/test/ckpt-50.\n",
      "saved checkpoint to /tmp/test/ckpt-50.\n",
      "I0625 02:25:16.483514 139903761540928 controller.py:425] train | step:    100 | steps/sec:    0.5 | output: \n",
      "    {'cls_accuracy': 0.329375,\n",
      "     'learning_rate': 7.3350293e-07,\n",
      "     'training_loss': 1.1200315}\n",
      "train | step:    100 | steps/sec:    0.5 | output: \n",
      "    {'cls_accuracy': 0.329375,\n",
      "     'learning_rate': 7.3350293e-07,\n",
      "     'training_loss': 1.1200315}\n",
      "I0625 02:26:44.691775 139903761540928 controller.py:425] train | step:    150 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.350625,\n",
      "     'learning_rate': 1.1002544e-06,\n",
      "     'training_loss': 1.1019136}\n",
      "train | step:    150 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.350625,\n",
      "     'learning_rate': 1.1002544e-06,\n",
      "     'training_loss': 1.1019136}\n",
      "I0625 02:28:12.955405 139903761540928 controller.py:425] train | step:    200 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.369375,\n",
      "     'learning_rate': 1.4670059e-06,\n",
      "     'training_loss': 1.0935988}\n",
      "train | step:    200 | steps/sec:    0.6 | output: \n",
      "    {'cls_accuracy': 0.369375,\n",
      "     'learning_rate': 1.4670059e-06,\n",
      "     'training_loss': 1.0935988}\n",
      "I0625 02:28:23.109853 139903761540928 controller.py:454] saved checkpoint to /tmp/test/ckpt-200.\n",
      "saved checkpoint to /tmp/test/ckpt-200.\n",
      "I0625 02:28:23.118923 139903761540928 train_lib.py:123] Number of trainable params in model: 336.194564 Millions.\n",
      "I0625 02:28:23.119448 139903761540928 train_utils.py:304] Saving gin configurations to /tmp/test/operative_config.train.gin\n"
     ]
    }
   ],
   "source": [
    "! docker run -it --rm --gpus all {TRAIN_IMAGE} trainer/train.py \\\n",
    "--experiment={EXPERIMENT} \\\n",
    "--mode={MODE} \\\n",
    "--model_dir={LOCAL_DIR}/test \\\n",
    "--config_file={CONFIG_FILE}\\\n",
    "--params_override={PARAMS_OVERRIDE}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "file_extension": ".py",
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
