{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed training with Vertex Reduction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'jk-mlops-dev'\n",
    "REGION = 'us-west1'\n",
    "STAGING_BUCKET = 'gs://jk-vertex-staging'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_IMAGE = 'gcr.io/deeplearning-platform-release/tf2-gpu.2-5'\n",
    "MODEL_GARDEN_VERSION = 'v2.5.0'\n",
    "TRAIN_IMAGE = f'gcr.io/{PROJECT_ID}/tf_nlp_toolkit'\n",
    "\n",
    "dockerfile = f'''\n",
    "FROM {BASE_IMAGE}\n",
    "WORKDIR /source\n",
    "RUN git clone -b {MODEL_GARDEN_VERSION}  --single-branch https://github.com/tensorflow/models.git \n",
    "RUN pip install -r models/official/requirements.txt \n",
    "ENV PYTHONPATH=/source/models\n",
    "\n",
    "#ENTRYPOINT [\"/bin/bash\", \"-c\"]\n",
    "#CMD [\"echo TensorFlow Model Garden image\"]\n",
    "ENTRYPOINT [\"python\"]\n",
    "CMD [\"-c\", \"print('Hello')\"]\n",
    "'''\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  129.5kB\n",
      "Step 1/7 : FROM gcr.io/deeplearning-platform-release/tf2-gpu.2-5\n",
      " ---> b963122c3c2c\n",
      "Step 2/7 : WORKDIR /source\n",
      " ---> Using cache\n",
      " ---> ae3fb96fd36d\n",
      "Step 3/7 : RUN git clone -b v2.5.0  --single-branch https://github.com/tensorflow/models.git\n",
      " ---> Using cache\n",
      " ---> 9a6a3b235d6c\n",
      "Step 4/7 : RUN pip install -r models/official/requirements.txt\n",
      " ---> Using cache\n",
      " ---> 4f5f993b1f1d\n",
      "Step 5/7 : ENV PYTHONPATH=/source/models\n",
      " ---> Using cache\n",
      " ---> c34b5beadd9f\n",
      "Step 6/7 : ENTRYPOINT [\"python\"]\n",
      " ---> Using cache\n",
      " ---> 3f09894d6299\n",
      "Step 7/7 : CMD [\"-c\", \"print('Hello')\"]\n",
      " ---> Using cache\n",
      " ---> eaa66349bebe\n",
      "Successfully built eaa66349bebe\n",
      "Successfully tagged gcr.io/jk-mlops-dev/tf_nlp_toolkit:latest\n"
     ]
    }
   ],
   "source": [
    "! docker build -t {TRAIN_IMAGE} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the container locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNLI_TRAIN_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record'\n",
    "MNLI_VALID_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record'\n",
    "MNLI_METADATA = 'gs://jk-vertex-demos/datasets/MNLI/metadata.json'\n",
    "BERT_CHECKPOINT = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16'\n",
    "MODEL_DIR = 'gs://jk-vertex-demos/testing/r1'\n",
    "\n",
    "task = 'MNLI'\n",
    "mode = 'train_and_eval'\n",
    "global_batch_size = 32\n",
    "steps_per_loop = 10\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "distribution_strategy = 'mirrored'\n",
    "num_gpus = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-26 05:14:31.521712: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "/opt/conda/lib/python3.7/site-packages/absl/flags/_validators.py:356: UserWarning: Flag --model_dir has a non-None default value; therefore, mark_flag_as_required will pass even if flag is not specified in the command line!\n",
      "  'command line!' % flag_name)\n",
      "2021-05-26 05:14:35.748749: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2021-05-26 05:14:37.111185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.112128: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-05-26 05:14:37.112313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.113061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-05-26 05:14:37.113123: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-26 05:14:37.117429: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-05-26 05:14:37.117528: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-05-26 05:14:37.120810: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2021-05-26 05:14:37.121393: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2021-05-26 05:14:37.125975: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-05-26 05:14:37.127287: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-05-26 05:14:37.127616: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-05-26 05:14:37.127778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.128637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.129449: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.130262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.130987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-05-26 05:14:37.131530: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-05-26 05:14:37.313361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.314184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-05-26 05:14:37.314387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.315157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:00:05.0 name: Tesla T4 computeCapability: 7.5\n",
      "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
      "2021-05-26 05:14:37.315291: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.316202: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.317029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.318040: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:37.318875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2021-05-26 05:14:37.318956: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-05-26 05:14:38.086408: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-05-26 05:14:38.086494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2021-05-26 05:14:38.086506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2021-05-26 05:14:38.086538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2021-05-26 05:14:38.087034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.088057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.088935: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.089701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.090518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.091320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "2021-05-26 05:14:38.091904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-05-26 05:14:38.092630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 13837 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)\n",
      "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "W0526 05:14:38.097457 139758392870720 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "I0526 05:14:38.347502 139758392870720 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "I0526 05:14:38.519683 139758392870720 run_classifier.py:181] Training using TF 2.x Keras compile/fit API with distribution strategy.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "W0526 05:14:38.988477 139758392870720 deprecation.py:534] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "I0526 05:14:43.683250 139758392870720 optimization.py:86] using Adamw optimizer\n",
      "I0526 05:14:43.683541 139758392870720 optimization.py:139] gradient_clip_norm=1.000000\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.526365 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.528237 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.531747 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.532806 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.565741 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.566754 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.569566 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:17.570662 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "2021-05-26 05:16:17.577723: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2021-05-26 05:16:17.577774: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "2021-05-26 05:16:17.577842: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 2 GPUs\n",
      "2021-05-26 05:16:17.579648: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.0\n",
      "2021-05-26 05:16:17.782006: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2021-05-26 05:16:17.782258: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\n",
      "2021-05-26 05:16:18.356084: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-05-26 05:16:18.356761: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
      "Epoch 1/3\n",
      "INFO:tensorflow:batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0526 05:16:28.689391 139758392870720 cross_device_ops.py:903] batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "W0526 05:16:30.694908 139758392870720 cross_device_ops.py:934] Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "I0526 05:16:30.695224 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:32.674055 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "I0526 05:16:32.676818 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0526 05:16:42.472164 139758392870720 cross_device_ops.py:903] batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "W0526 05:16:44.655799 139758392870720 cross_device_ops.py:934] Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "I0526 05:16:44.656095 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0526 05:16:59.688625 139758392870720 cross_device_ops.py:903] batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "W0526 05:17:01.555576 139758392870720 cross_device_ops.py:934] Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "I0526 05:17:01.555850 139758392870720 cross_device_ops.py:621] Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "I0526 05:17:13.540375 139758392870720 cross_device_ops.py:903] batch_all_reduce: 392 all-reduces with algorithm = nccl, num_packs = 1\n",
      "2021-05-26 05:17:39.799611: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-05-26 05:17:40.408642: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "   10/12271 [..............................] - ETA: 33:27:38 - loss: 1.1408 - accuracy: 0.38122021-05-26 05:17:58.842713: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.\n",
      "2021-05-26 05:17:58.842779: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.\n",
      "   20/12271 [..............................] - ETA: 19:57:36 - loss: 1.1443 - accuracy: 0.37342021-05-26 05:18:15.982362: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-05-26 05:18:16.123615: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed\n",
      "2021-05-26 05:18:17.069279: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 128321 callback api events and 128378 activity events. \n",
      "2021-05-26 05:18:20.211713: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.\n",
      "2021-05-26 05:18:24.810145: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20\n",
      "2021-05-26 05:18:27.286811: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.trace.json.gz\n",
      "2021-05-26 05:18:29.984797: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20\n",
      "2021-05-26 05:18:30.151171: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.memory_profile.json.gz\n",
      "2021-05-26 05:18:31.397739: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20Dumped tool data for xplane.pb to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.xplane.pb\n",
      "Dumped tool data for overview_page.pb to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to gs://jk-vertex-demos/testing/r1/summaries/train/plugins/profile/2021_05_26_05_18_20/6da54df5ec25.kernel_stats.pb\n",
      "\n",
      "  100/12271 [..............................] - ETA: 8:50:45 - loss: 1.1483 - accuracy: 0.3562I0526 05:20:40.094313 139758392870720 keras_utils.py:148] TimeHistory: 261.65 seconds, 12.23 examples/second between steps 0 and 100\n"
     ]
    }
   ],
   "source": [
    "!docker run -it --rm --gpus all {TRAIN_IMAGE} models/official/nlp/bert/run_classifier.py \\\n",
    "--mode={mode} \\\n",
    "--model_dir={MODEL_DIR} \\\n",
    "--input_meta_data_path={MNLI_METADATA} \\\n",
    "--train_data_path={MNLI_TRAIN_SPLIT} \\\n",
    "--eval_data_path={MNLI_VALID_SPLIT} \\\n",
    "--bert_config_file={BERT_CHECKPOINT}/bert_config.json \\\n",
    "--init_checkpoint={BERT_CHECKPOINT}/bert_model.ckpt \\\n",
    "--train_batch_size={global_batch_size} \\\n",
    "--eval_batch_size={global_batch_size} \\\n",
    "--steps_per_loop={steps_per_loop} \\\n",
    "--learning_rate={learning_rate} \\\n",
    "--num_train_epochs={num_train_epochs} \\\n",
    "--distribution_strategy={distribution_strategy} \\\n",
    "--num_gpus={num_gpus}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push the container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/tf_nlp_toolkit]\n",
      "\n",
      "\u001b[1Be22ef9a1: Preparing \n",
      "\u001b[1B77ad8be7: Preparing \n",
      "\u001b[1Ba2fa6014: Preparing \n",
      "\u001b[1B961a296c: Preparing \n",
      "\u001b[1B53abc6c2: Preparing \n",
      "\u001b[1B3723ef37: Preparing \n",
      "\u001b[1B0089a9c0: Preparing \n",
      "\u001b[1B3e41a2c0: Preparing \n",
      "\u001b[1B25162004: Preparing \n",
      "\u001b[1B99d982dd: Preparing \n",
      "\u001b[1B6603d114: Preparing \n",
      "\u001b[1Bc97a79f1: Preparing \n",
      "\u001b[1Be02b8502: Preparing \n",
      "\u001b[1Bd34a65ac: Preparing \n",
      "\u001b[1Bce22e436: Preparing \n",
      "\u001b[1B7e013d33: Preparing \n",
      "\u001b[1Baff4f6ee: Preparing \n",
      "\u001b[1Be4ccb381: Preparing \n",
      "\u001b[1B90ceec1e: Preparing \n",
      "\u001b[1B0ab30137: Preparing \n",
      "\u001b[1Bed8ae595: Preparing \n",
      "\u001b[1B855df562: Preparing \n",
      "\u001b[1Bdb3c5655: Preparing \n",
      "\u001b[1B0a9a6a11: Preparing \n",
      "\u001b[1B7e8b38e6: Preparing \n",
      "\u001b[1B8f196cf4: Preparing \n",
      "\u001b[1B01dbc7de: Preparing \n",
      "\u001b[1B31d2d72b: Preparing \n",
      "\u001b[1Ba966f459: Preparing \n",
      "\u001b[1Bb9e63cdf: Preparing \n",
      "\u001b[1B49f5bf51: Preparing \n",
      "\u001b[1Baa2fa9fe: Preparing \n",
      "\u001b[1B325cc380: Preparing \n",
      "\u001b[1Bdd81f9fa: Preparing \n",
      "\u001b[1B09cad0ba: Layer already exists \u001b[31A\u001b[2K\u001b[29A\u001b[2K\u001b[25A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[16A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:b7c25898469025a174a4d8c671833737948c0aa78f74e254f2c541e4f22166e4 size: 7672\n"
     ]
    }
   ],
   "source": [
    "! docker push {TRAIN_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Vertex Training jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=STAGING_BUCKET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_pool_specs(\n",
    "    image_uri,\n",
    "    args,\n",
    "    cmd, \n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4',\n",
    "    accelerator_count=0,\n",
    "    accelerator_type='ACCELERATOR_TYPE_UNSPECIFIED'):\n",
    "\n",
    "    if accelerator_count > 0:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type,\n",
    "            'accelerator_type': accelerator_type,\n",
    "            'accelerator_count': accelerator_count,\n",
    "        }\n",
    "    else:\n",
    "        machine_spec = {\n",
    "            'machine_type': machine_type\n",
    "        }\n",
    "    \n",
    "    container_spec = {\n",
    "        'image_uri': image_uri,\n",
    "        'args': args,\n",
    "        'command': cmd,\n",
    "    }\n",
    "    \n",
    "    chief_spec = {\n",
    "        'replica_count': 1,\n",
    "        'machine_spec': machine_spec,\n",
    "        'container_spec': container_spec\n",
    "    }\n",
    "\n",
    "    worker_pool_specs = [chief_spec]\n",
    "    if replica_count > 1:\n",
    "        workers_spec = {\n",
    "            'replica_count': replica_count - 1,\n",
    "            'machine_spec': machine_spec,\n",
    "            'container_spec': container_spec\n",
    "        }\n",
    "        worker_pool_specs.append(workers_spec)\n",
    "    \n",
    "    return worker_pool_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'container_spec': {'args': ['--mode=train_and_eval',\n",
      "                              '--model_dir=gs://jk-vertex-demos/testing/r1',\n",
      "                              '--input_meta_data_path=gs://jk-vertex-demos/datasets/MNLI/metadata.json',\n",
      "                              '--train_data_path=gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record',\n",
      "                              '--eval_data_path=gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record',\n",
      "                              '--bert_config_file=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/bert_config.json',\n",
      "                              '--init_checkpoint=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/bert_model.ckpt',\n",
      "                              '--train_batch_size=32',\n",
      "                              '--eval_batch_size=32',\n",
      "                              '--steps_per_loop=10',\n",
      "                              '--learning_rate=2e-05',\n",
      "                              '--num_train_epochs=3',\n",
      "                              '--distribution_strategy=multi_worker_mirrored',\n",
      "                              '--num_gpus=1'],\n",
      "                     'command': ['python',\n",
      "                                 'models/official/nlp/bert/run_classifier.py'],\n",
      "                     'image_uri': 'gcr.io/jk-mlops-dev/tf_nlp_toolkit'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_V100',\n",
      "                   'machine_type': 'n1-standard-8'},\n",
      "  'replica_count': 1},\n",
      " {'container_spec': {'args': ['--mode=train_and_eval',\n",
      "                              '--model_dir=gs://jk-vertex-demos/testing/r1',\n",
      "                              '--input_meta_data_path=gs://jk-vertex-demos/datasets/MNLI/metadata.json',\n",
      "                              '--train_data_path=gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record',\n",
      "                              '--eval_data_path=gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record',\n",
      "                              '--bert_config_file=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/bert_config.json',\n",
      "                              '--init_checkpoint=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16/bert_model.ckpt',\n",
      "                              '--train_batch_size=32',\n",
      "                              '--eval_batch_size=32',\n",
      "                              '--steps_per_loop=10',\n",
      "                              '--learning_rate=2e-05',\n",
      "                              '--num_train_epochs=3',\n",
      "                              '--distribution_strategy=multi_worker_mirrored',\n",
      "                              '--num_gpus=1'],\n",
      "                     'command': ['python',\n",
      "                                 'models/official/nlp/bert/run_classifier.py'],\n",
      "                     'image_uri': 'gcr.io/jk-mlops-dev/tf_nlp_toolkit'},\n",
      "  'machine_spec': {'accelerator_count': 1,\n",
      "                   'accelerator_type': 'NVIDIA_TESLA_V100',\n",
      "                   'machine_type': 'n1-standard-8'},\n",
      "  'replica_count': 1}]\n"
     ]
    }
   ],
   "source": [
    "MNLI_TRAIN_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_train.tf_record'\n",
    "MNLI_VALID_SPLIT = 'gs://jk-vertex-demos/datasets/MNLI/mnli_valid.tf_record'\n",
    "MNLI_METADATA = 'gs://jk-vertex-demos/datasets/MNLI/metadata.json'\n",
    "BERT_CHECKPOINT = 'gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16'\n",
    "MODEL_DIR = 'gs://jk-vertex-demos/testing/r1'\n",
    "\n",
    "task = 'MNLI'\n",
    "mode = 'train_and_eval'\n",
    "global_batch_size = 32\n",
    "steps_per_loop = 10\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "\n",
    "distribution_strategy = 'mirrored'\n",
    "distribution_strategy = 'multi_worker_mirrored'\n",
    "num_gpus = 1\n",
    "\n",
    "replica_count = 2\n",
    "machine_type = 'n1-standard-8'\n",
    "accelerator_count = 1\n",
    "accelerator_type = 'NVIDIA_TESLA_V100'\n",
    "\n",
    "image_uri = TRAIN_IMAGE\n",
    "\n",
    "cmd = [\n",
    "    \"python\", \"models/official/nlp/bert/run_classifier.py\"\n",
    "]\n",
    "\n",
    "args = [\n",
    "    '--mode=' + mode,\n",
    "    '--model_dir=' + MODEL_DIR,\n",
    "    '--input_meta_data_path=' + MNLI_METADATA,\n",
    "    '--train_data_path=' + MNLI_TRAIN_SPLIT,\n",
    "    '--eval_data_path=' + MNLI_VALID_SPLIT,\n",
    "    '--bert_config_file=' + BERT_CHECKPOINT + '/bert_config.json',\n",
    "    '--init_checkpoint=' + BERT_CHECKPOINT + '/bert_model.ckpt',\n",
    "    '--train_batch_size=' + str(global_batch_size),\n",
    "    '--eval_batch_size=' + str(global_batch_size),\n",
    "    '--steps_per_loop=' + str(steps_per_loop),\n",
    "    '--learning_rate=' + str(learning_rate),\n",
    "    '--num_train_epochs=' + str(num_train_epochs),\n",
    "    '--distribution_strategy=' + distribution_strategy,\n",
    "    '--num_gpus=' + str(num_gpus),\n",
    "]\n",
    "\n",
    "worker_pool_specs = prepare_worker_pool_specs(\n",
    "    image_uri=image_uri,\n",
    "    args=args,\n",
    "    cmd=cmd,\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    accelerator_type=accelerator_type\n",
    ")\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "print(pp.pformat(worker_pool_specs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:Creating CustomJob\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob created. Resource name: projects/895222332033/locations/us-west1/customJobs/8383380337606000640\n",
      "INFO:google.cloud.aiplatform.jobs:To use this CustomJob in another session:\n",
      "INFO:google.cloud.aiplatform.jobs:custom_job = aiplatform.CustomJob.get('projects/895222332033/locations/us-west1/customJobs/8383380337606000640')\n",
      "INFO:google.cloud.aiplatform.jobs:View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/8383380337606000640?project=895222332033\n"
     ]
    }
   ],
   "source": [
    "display_name = 'custom-test'\n",
    "\n",
    "job = aiplatform.CustomJob(\n",
    "    display_name=display_name,\n",
    "    worker_pool_specs=worker_pool_specs,\n",
    ")\n",
    "\n",
    "job.run(sync=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/895222332033/locations/us-west1/customJobs/8383380337606000640'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.jobs:CustomJob projects/895222332033/locations/us-west1/customJobs/8383380337606000640 current state:\n",
      "JobState.JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m70",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m70"
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
